nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.0531158447265625e-06
user  100 time:  174.45938754081726
user  200 time:  361.22906160354614
user  300 time:  548.6578204631805
user  400 time:  737.9476692676544
user  500 time:  927.2540574073792
user  600 time:  1118.1470303535461
user  700 time:  1308.4540355205536
user  800 time:  1493.3411543369293
user  900 time:  1682.8543093204498
user  1000 time:  1871.5365686416626
user  1100 time:  2060.94793176651
user  1200 time:  2250.051433801651
user  1300 time:  2437.847718000412
user  1400 time:  2628.9816715717316
start training item-item instance self attention module...
user  0 time:  8.344650268554688e-06
user  100 time:  112.47074317932129
user  200 time:  210.44772124290466
user  300 time:  315.3759174346924
user  400 time:  413.0926728248596
user  500 time:  519.7994499206543
user  600 time:  615.7601125240326
user  700 time:  730.7713887691498
user  800 time:  834.9412186145782
user  900 time:  940.7226221561432
user  1000 time:  1047.4988422393799
user  1100 time:  1142.9640173912048
user  1200 time:  1234.896300792694
user  1300 time:  1349.8430576324463
user  1400 time:  1449.9537410736084
start updating user and item embedding...
user_name:1450
user  0 time:  1.33514404296875e-05
user  100 time:  23.03487753868103
user  200 time:  46.068328619003296
user  300 time:  69.3742527961731
user  400 time:  92.36858558654785
user  500 time:  115.8202874660492
user  600 time:  138.9042866230011
user  700 time:  162.627347946167
user  800 time:  186.48804426193237
user  900 time:  210.04021787643433
user  1000 time:  233.6694254875183
user  1100 time:  257.14682507514954
user  1200 time:  276.80320286750793
user  1300 time:  295.780752658844
user  1400 time:  314.49163126945496
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 161.5438865676988, train time: 43.80200791358948
epoch: 1, training loss: 86.52576048848277, train time: 41.64916372299194
epoch: 2, training loss: 70.73998189961276, train time: 40.86424660682678
epoch: 3, training loss: 62.30587182110685, train time: 38.468058824539185
epoch: 4, training loss: 57.22041104996606, train time: 41.362919330596924
epoch: 5, training loss: 51.930592151697056, train time: 37.63549518585205
epoch: 6, training loss: 49.428963850261425, train time: 36.5151641368866
epoch: 7, training loss: 45.30855609754872, train time: 36.291616916656494
epoch: 8, training loss: 43.03970737506461, train time: 40.31810760498047
epoch: 9, training loss: 41.16688922188041, train time: 42.72462224960327
epoch: 10, training loss: 37.84722717599652, train time: 40.40954613685608
epoch: 11, training loss: 36.787930337304715, train time: 45.16881084442139
epoch: 12, training loss: 34.922357820061734, train time: 43.61348915100098
epoch: 13, training loss: 33.25021303060021, train time: 36.90715527534485
epoch: 14, training loss: 31.407154772449758, train time: 39.731847286224365
epoch: 15, training loss: 31.365993689638344, train time: 47.77039051055908
epoch: 16, training loss: 28.055575604777914, train time: 53.98333239555359
epoch: 17, training loss: 27.347448599019117, train time: 53.65012264251709
epoch: 18, training loss: 26.929554604730583, train time: 53.626243352890015
epoch: 19, training loss: 25.63974749803765, train time: 53.68584394454956
epoch: 20, training loss: 24.890762104914756, train time: 53.73672103881836
epoch: 21, training loss: 24.160760103849498, train time: 53.78589940071106
epoch: 22, training loss: 23.148954562861945, train time: 53.83235812187195
epoch: 23, training loss: 22.50809435964129, train time: 53.6077778339386
epoch: 24, training loss: 21.781247848133717, train time: 53.7943012714386
epoch: 25, training loss: 22.52310075482319, train time: 53.67675518989563
epoch: 26, training loss: 21.11458980790485, train time: 53.90212392807007
epoch: 27, training loss: 21.32889139393319, train time: 54.09995102882385
epoch: 28, training loss: 19.265255887186413, train time: 54.36658072471619
epoch: 29, training loss: 19.714283874470766, train time: 54.227274894714355
epoch: 30, training loss: 19.05046919248207, train time: 54.06563401222229
epoch: 31, training loss: 19.104588535038374, train time: 53.15125346183777
epoch: 32, training loss: 19.326864656432463, train time: 52.890597343444824
epoch: 33, training loss: 18.624410670241105, train time: 52.78100919723511
epoch: 34, training loss: 18.01195476983321, train time: 52.74743914604187
epoch: 35, training loss: 18.92934531254582, train time: 53.2909049987793
epoch: 36, training loss: 17.496242023671584, train time: 52.93251824378967
epoch: 37, training loss: 17.543020582568715, train time: 55.663121700286865
epoch: 38, training loss: 17.44749598795579, train time: 70.88458776473999
epoch: 39, training loss: 16.713805272174795, train time: 71.06537342071533
epoch: 40, training loss: 16.877903305948394, train time: 70.9049379825592
epoch: 41, training loss: 16.111661805169888, train time: 70.9849271774292
epoch: 42, training loss: 17.188501297945095, train time: 71.04300880432129
epoch: 43, training loss: 15.57756493081024, train time: 70.48654961585999
epoch: 44, training loss: 16.732671175382393, train time: 70.81788063049316
epoch: 45, training loss: 16.2747867789069, train time: 71.05943608283997
epoch: 46, training loss: 15.965904626134261, train time: 71.14612317085266
epoch: 47, training loss: 16.243501586694265, train time: 70.85027980804443
epoch: 48, training loss: 15.87889102279479, train time: 70.92960500717163
epoch: 49, training loss: 15.587661762906464, train time: 70.63301181793213
epo:49|HR@1:0.6987 | HR@5:0.8225 | HR@10:0.8487 | HR@20:0.8843 | HR@50:0.9464 | NDCG@1:0.3739 | NDCG@5:0.4653 | NDCG@10:0.5051| NDCG@20:0.5536| NDCG@50:0.6500| best_HR@1:0.6987 | best_HR@5:0.8225 | best_HR@10:0.8487 | best_HR@20:0.8843 | best_HR@50:0.9464 | best_NDCG@1:0.3739 | best_NDCG@5:0.4653 | best_NDCG@10:0.5051 | best_NDCG@20:0.5536 | best_NDCG@50:0.6500 | train_time:70.63 | test_time:530.96
epoch: 50, training loss: 15.086959741131068, train time: 71.42234349250793
epoch: 51, training loss: 16.23332052738749, train time: 70.91344356536865
epoch: 52, training loss: 14.53112658254463, train time: 70.90091180801392
epoch: 53, training loss: 14.446616835843997, train time: 70.92007231712341
epoch: 54, training loss: 14.62406957523774, train time: 70.7720365524292
epoch: 55, training loss: 14.587418278546693, train time: 71.21571803092957
epoch: 56, training loss: 14.352812289329336, train time: 71.03441834449768
epoch: 57, training loss: 14.256520463025026, train time: 57.50290894508362
epoch: 58, training loss: 14.701716225736618, train time: 57.542057037353516
epoch: 59, training loss: 15.302939389028893, train time: 55.450849533081055
epoch: 60, training loss: 13.13482297202222, train time: 55.06402039527893
epoch: 61, training loss: 13.827979819130633, train time: 58.460959672927856
epoch: 62, training loss: 13.838733445383923, train time: 55.68313384056091
epoch: 63, training loss: 14.606785355976058, train time: 57.79109859466553
epoch: 64, training loss: 14.167358758607293, train time: 60.18003964424133
epoch: 65, training loss: 13.160762080533459, train time: 55.25993824005127
epoch: 66, training loss: 12.86136891677944, train time: 61.36887526512146
epoch: 67, training loss: 12.936419595462098, train time: 70.83253812789917
epoch: 68, training loss: 13.405973502368852, train time: 70.73250198364258
epoch: 69, training loss: 12.95245424786674, train time: 70.72944355010986
epoch: 70, training loss: 12.768575309190624, train time: 70.89109945297241
epoch: 71, training loss: 12.47630017738993, train time: 70.70893931388855
epoch: 72, training loss: 12.877397759008545, train time: 70.59049820899963
epoch: 73, training loss: 13.584672351856625, train time: 70.74089217185974
epoch: 74, training loss: 13.366464236536444, train time: 70.8389630317688
epoch: 75, training loss: 12.996330158839669, train time: 70.83177900314331
epoch: 76, training loss: 12.110850736065231, train time: 70.81564402580261
epoch: 77, training loss: 12.21126035097302, train time: 70.91655230522156
epoch: 78, training loss: 12.69810128231461, train time: 65.40159368515015
epoch: 79, training loss: 12.173447148917091, train time: 56.5857195854187
epoch: 80, training loss: 12.57937610132899, train time: 54.84986162185669
epoch: 81, training loss: 12.84823915655943, train time: 54.760013580322266
epoch: 82, training loss: 12.750111378853546, train time: 59.17088723182678
epoch: 83, training loss: 11.482608709488886, train time: 57.60246682167053
epoch: 84, training loss: 11.396282147069371, train time: 56.392335176467896
epoch: 85, training loss: 12.375428994301387, train time: 55.84652614593506
epoch: 86, training loss: 11.900365454414782, train time: 63.088943004608154
epoch: 87, training loss: 12.375316397685992, train time: 57.18750882148743
epoch: 88, training loss: 11.36358029129002, train time: 70.12046360969543
epoch: 89, training loss: 12.108290934586421, train time: 71.11980390548706
epoch: 90, training loss: 11.476693083362761, train time: 70.92481899261475
epoch: 91, training loss: 10.59089676354722, train time: 70.88357043266296
epoch: 92, training loss: 11.705499399666564, train time: 70.52137160301208
epoch: 93, training loss: 12.698185165801306, train time: 70.86694526672363
epoch: 94, training loss: 11.864465488439919, train time: 70.81444668769836
epoch: 95, training loss: 11.20500645180033, train time: 71.1200864315033
epoch: 96, training loss: 11.1409003258197, train time: 70.77966666221619
epoch: 97, training loss: 12.623923457299782, train time: 70.8794457912445
epoch: 98, training loss: 10.601330934148507, train time: 71.50915813446045
epoch: 99, training loss: 10.552983275839722, train time: 71.08835220336914
epo:99|HR@1:0.6752 | HR@5:0.8036 | HR@10:0.8310 | HR@20:0.8634 | HR@50:0.9284 | NDCG@1:0.3997 | NDCG@5:0.4855 | NDCG@10:0.5232| NDCG@20:0.5694| NDCG@50:0.6619| best_HR@1:0.6987 | best_HR@5:0.8225 | best_HR@10:0.8487 | best_HR@20:0.8843 | best_HR@50:0.9464 | best_NDCG@1:0.3997 | best_NDCG@5:0.4855 | best_NDCG@10:0.5232 | best_NDCG@20:0.5694 | best_NDCG@50:0.6619 | train_time:71.09 | test_time:527.99
training finish
