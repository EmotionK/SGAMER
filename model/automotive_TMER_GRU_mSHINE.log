nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.5299530029296875e-06
user  100 time:  8.287058353424072
user  200 time:  10.115923881530762
user  300 time:  16.42951512336731
user  400 time:  25.916813135147095
user  500 time:  30.010132312774658
user  600 time:  34.96599078178406
user  700 time:  38.774561643600464
user  800 time:  43.45661807060242
user  900 time:  52.1362030506134
user  1000 time:  59.923555850982666
user  1100 time:  64.14969635009766
user  1200 time:  64.72390651702881
user  1300 time:  68.06345582008362
user  1400 time:  74.66253542900085
user  1500 time:  80.79585552215576
user  1600 time:  88.21906733512878
user  1700 time:  94.87257719039917
user  1800 time:  102.10240507125854
user  1900 time:  108.12485313415527
start training item-item instance self attention module...
user  0 time:  3.5762786865234375e-06
user  100 time:  178.2861807346344
user  200 time:  346.59174036979675
user  300 time:  534.2734577655792
user  400 time:  703.9715161323547
user  500 time:  885.5081222057343
user  600 time:  1076.0960702896118
user  700 time:  1255.433671951294
user  800 time:  1438.1519632339478
user  900 time:  1632.9983639717102
user  1000 time:  1829.1766743659973
user  1100 time:  2009.3861119747162
user  1200 time:  2192.9136502742767
user  1300 time:  2372.5364694595337
user  1400 time:  2566.6400458812714
user  1500 time:  2758.5746035575867
user  1600 time:  2943.707644701004
user  1700 time:  3126.634558200836
user  1800 time:  3303.731505393982
user  1900 time:  3498.6241137981415
start updating user and item embedding...
user_name:2000
user  0 time:  1.0251998901367188e-05
user  100 time:  19.443994283676147
user  200 time:  39.00721788406372
user  300 time:  58.415452003479004
user  400 time:  77.59460735321045
user  500 time:  97.06928539276123
user  600 time:  116.60915684700012
user  700 time:  136.0982632637024
user  800 time:  155.5986659526825
user  900 time:  175.55831575393677
user  1000 time:  195.10074186325073
user  1100 time:  214.7334702014923
user  1200 time:  234.06436443328857
user  1300 time:  253.61987829208374
user  1400 time:  273.5223853588104
user  1500 time:  293.25974464416504
user  1600 time:  312.8971543312073
user  1700 time:  332.4379713535309
user  1800 time:  351.6588509082794
user  1900 time:  371.4873893260956
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 452.2654282246949, train time: 598.5941758155823
epoch: 1, training loss: 428.6964859785512, train time: 600.9184603691101
epoch: 2, training loss: 380.70103444717824, train time: 596.6548020839691
epoch: 3, training loss: 366.5226253629662, train time: 596.9589550495148
epoch: 4, training loss: 358.434756828472, train time: 596.1074619293213
epoch: 5, training loss: 353.42579855257645, train time: 595.7748055458069
epoch: 6, training loss: 347.8177992843557, train time: 593.7606592178345
epoch: 7, training loss: 343.70931138750166, train time: 594.1265449523926
epoch: 8, training loss: 339.60904823360033, train time: 595.0556130409241
epoch: 9, training loss: 336.3014446145389, train time: 592.5715789794922
epoch: 10, training loss: 333.9197574935388, train time: 593.7697689533234
epoch: 11, training loss: 331.0734380685026, train time: 593.6638479232788
epoch: 12, training loss: 328.1140171063598, train time: 590.341362953186
epoch: 13, training loss: 326.5400540358387, train time: 590.544016122818
epoch: 14, training loss: 323.95427164738066, train time: 591.9113066196442
epoch: 15, training loss: 322.6638838488143, train time: 577.0431389808655
epoch: 16, training loss: 320.5948051594896, train time: 579.2706501483917
epoch: 17, training loss: 317.98381806444377, train time: 591.1468777656555
epoch: 18, training loss: 316.24498826800846, train time: 587.4405109882355
epoch: 19, training loss: 314.30473814043216, train time: 587.6055657863617
epoch: 20, training loss: 312.5733142015524, train time: 590.0111093521118
epoch: 21, training loss: 309.9731623274274, train time: 586.1450493335724
epoch: 22, training loss: 308.3301922342507, train time: 591.436865568161
epoch: 23, training loss: 305.5608828124823, train time: 589.8248414993286
epoch: 24, training loss: 302.96903797099367, train time: 592.7497162818909
epoch: 25, training loss: 301.12145025189966, train time: 589.4660060405731
epoch: 26, training loss: 298.54013794544153, train time: 578.8836672306061
epoch: 27, training loss: 296.35368377005216, train time: 589.8288421630859
epoch: 28, training loss: 294.54729764792137, train time: 593.9039747714996
epoch: 29, training loss: 292.0922720427625, train time: 591.1629407405853
epo:29 | HR@5:0.4644 | HR@10:0.6332 | HR@20:0.8075 | NDCG@5:0.4945 | NDCG@10:0.5186 | NDCG@20:0.5565 | recall@5:0.2823 | recall@10:0.4524 | recall@20:0.6246 | precision@5:0.3388 | precision@10:0.2714 | precision@20:0.1874 | best_HR@5:0.4644 | best_HR@10:0.6332 | best_HR@20:0.8075 | best_NDCG@5:0.4945 | best_NDCG@10:0.5186 | best_NDCG@20:0.5565 | best_recall@5:0.2823 | best_recall@10:0.4524 | best_recall@20:0.6246 | best_precision@5:0.3388 | best_precision@10:0.2714 | best_precision@20:0.1874 | 
epoch: 30, training loss: 290.706342197489, train time: 587.0638773441315
epoch: 31, training loss: 288.0303112826077, train time: 588.6861262321472
epoch: 32, training loss: 286.8137911610538, train time: 590.5732798576355
epoch: 33, training loss: 284.7111822294537, train time: 591.5760886669159
epoch: 34, training loss: 282.7275142825674, train time: 591.1392078399658
epoch: 35, training loss: 280.04562670586165, train time: 590.2160279750824
epoch: 36, training loss: 279.8346762206638, train time: 589.4172923564911
epoch: 37, training loss: 278.112532913452, train time: 574.9244863986969
epoch: 38, training loss: 275.94854862010106, train time: 589.6965980529785
epoch: 39, training loss: 274.0399418948218, train time: 591.3620278835297
epoch: 40, training loss: 272.006015948602, train time: 590.8212049007416
epoch: 41, training loss: 271.5484469059156, train time: 588.1582190990448
epoch: 42, training loss: 269.2150947691407, train time: 589.1971573829651
epoch: 43, training loss: 267.4523487094557, train time: 587.1164810657501
epoch: 44, training loss: 265.82043335342314, train time: 586.4999895095825
epoch: 45, training loss: 264.178096050513, train time: 586.5367999076843
epoch: 46, training loss: 263.1749805836007, train time: 587.8769385814667
epoch: 47, training loss: 260.8393360461341, train time: 583.952306509018
epoch: 48, training loss: 259.7939499823842, train time: 583.6340224742889
epoch: 49, training loss: 258.2176800563466, train time: 583.2549512386322
epoch: 50, training loss: 257.1016486857552, train time: 581.5261461734772
epoch: 51, training loss: 255.62542953633238, train time: 583.4928696155548
epoch: 52, training loss: 254.36985462822486, train time: 582.3705902099609
epoch: 53, training loss: 252.43144973996095, train time: 583.2275099754333
epoch: 54, training loss: 251.5190576610621, train time: 580.889720916748
epoch: 55, training loss: 249.08597394183744, train time: 579.0121972560883
epoch: 56, training loss: 248.3725151087856, train time: 573.5933496952057
epoch: 57, training loss: 247.96134770545177, train time: 582.5865180492401
epoch: 58, training loss: 246.40793237718754, train time: 583.0543076992035
epoch: 59, training loss: 245.09714094852097, train time: 575.7560062408447
epo:59 | HR@5:0.4535 | HR@10:0.6160 | HR@20:0.7818 | NDCG@5:0.5157 | NDCG@10:0.5400 | NDCG@20:0.5773 | recall@5:0.2823 | recall@10:0.4369 | recall@20:0.6133 | precision@5:0.3388 | precision@10:0.2621 | precision@20:0.1840 | best_HR@5:0.4644 | best_HR@10:0.6332 | best_HR@20:0.8075 | best_NDCG@5:0.5157 | best_NDCG@10:0.5400 | best_NDCG@20:0.5773 | best_recall@5:0.2823 | best_recall@10:0.4524 | best_recall@20:0.6246 | best_precision@5:0.3388 | best_precision@10:0.2714 | best_precision@20:0.1874 | 
epoch: 60, training loss: 243.49486818234436, train time: 583.2753837108612
epoch: 61, training loss: 242.61681724485243, train time: 581.961984872818
epoch: 62, training loss: 241.85042338026688, train time: 580.7901475429535
epoch: 63, training loss: 239.58280860236846, train time: 583.506087064743
epoch: 64, training loss: 239.02577815810218, train time: 581.9425666332245
epoch: 65, training loss: 238.08390097192023, train time: 580.7364566326141
epoch: 66, training loss: 236.62519067921676, train time: 581.8692181110382
epoch: 67, training loss: 234.73133683903143, train time: 577.9338872432709
epoch: 68, training loss: 234.7965532662929, train time: 582.7727980613708
epoch: 69, training loss: 233.22461622365518, train time: 579.9900381565094
epoch: 70, training loss: 232.32488103199285, train time: 580.9987902641296
epoch: 71, training loss: 230.95329421793576, train time: 579.5456762313843
epoch: 72, training loss: 230.63714832888218, train time: 582.9404499530792
epoch: 73, training loss: 228.66175113996724, train time: 580.4586713314056
epoch: 74, training loss: 228.12263799988432, train time: 579.1834855079651
epoch: 75, training loss: 226.63907980825752, train time: 581.4694066047668
epoch: 76, training loss: 226.58978820795892, train time: 580.7824380397797
epoch: 77, training loss: 225.10845611034892, train time: 579.4673354625702
epoch: 78, training loss: 223.6671963343979, train time: 578.7767198085785
epoch: 79, training loss: 224.3523028464988, train time: 580.9139769077301
epoch: 80, training loss: 222.36876088241115, train time: 579.4511551856995
epoch: 81, training loss: 222.40694290591637, train time: 556.489837884903
epoch: 82, training loss: 220.3561608903692, train time: 573.1242582798004
epoch: 83, training loss: 220.1271107411012, train time: 580.0917966365814
epoch: 84, training loss: 218.312785093789, train time: 578.6148178577423
epoch: 85, training loss: 218.28846923430683, train time: 571.47070479393
epoch: 86, training loss: 216.811171996349, train time: 569.9180006980896
epoch: 87, training loss: 216.61818100238452, train time: 580.539276599884
epoch: 88, training loss: 215.24744283978362, train time: 581.3621559143066
epoch: 89, training loss: 213.5453677928308, train time: 581.3797900676727
epo:89 | HR@5:0.4313 | HR@10:0.5993 | HR@20:0.7666 | NDCG@5:0.5233 | NDCG@10:0.5469 | NDCG@20:0.5838 | recall@5:0.2741 | recall@10:0.4308 | recall@20:0.6006 | precision@5:0.3289 | precision@10:0.2585 | precision@20:0.1802 | best_HR@5:0.4644 | best_HR@10:0.6332 | best_HR@20:0.8075 | best_NDCG@5:0.5233 | best_NDCG@10:0.5469 | best_NDCG@20:0.5838 | best_recall@5:0.2823 | best_recall@10:0.4524 | best_recall@20:0.6246 | best_precision@5:0.3388 | best_precision@10:0.2714 | best_precision@20:0.1874 | 
epoch: 90, training loss: 214.19748248698306, train time: 583.8085513114929
epoch: 91, training loss: 213.52331752330065, train time: 587.1599340438843
epoch: 92, training loss: 211.8743573128595, train time: 586.4762649536133
epoch: 93, training loss: 209.936003144423, train time: 586.9149737358093
epoch: 94, training loss: 210.75273871060926, train time: 587.6163420677185
epoch: 95, training loss: 209.40108788601356, train time: 590.5751614570618
epoch: 96, training loss: 209.09625273628626, train time: 590.2828145027161
epoch: 97, training loss: 208.91932660812745, train time: 590.5484569072723
epoch: 98, training loss: 207.78151713038096, train time: 592.3445649147034
epoch: 99, training loss: 207.4294152118382, train time: 590.7861342430115
epoch: 100, training loss: 206.25946730014402, train time: 592.0021097660065
epoch: 101, training loss: 206.95636620442383, train time: 591.9087109565735
epoch: 102, training loss: 203.9145955646527, train time: 580.687646150589
epoch: 103, training loss: 203.61794473556802, train time: 575.1187689304352
epoch: 104, training loss: 203.93907327688066, train time: 526.646463394165
epoch: 105, training loss: 202.3099686233909, train time: 528.7879364490509
epoch: 106, training loss: 203.41529967088718, train time: 533.9641258716583
epoch: 107, training loss: 201.8633926253533, train time: 531.2676560878754
epoch: 108, training loss: 201.41900417848956, train time: 531.591824054718
epoch: 109, training loss: 200.22927587060258, train time: 534.7390658855438
epoch: 110, training loss: 200.1322294845013, train time: 532.8471391201019
epoch: 111, training loss: 198.5147799142287, train time: 535.1236793994904
epoch: 112, training loss: 198.06426476413617, train time: 536.2751457691193
epoch: 113, training loss: 197.57884138508234, train time: 532.2681565284729
epoch: 114, training loss: 198.33986210281728, train time: 533.549889087677
epoch: 115, training loss: 196.26718450098997, train time: 505.3454520702362
epoch: 116, training loss: 195.8596004524734, train time: 523.2884378433228
epoch: 117, training loss: 195.05330589567893, train time: 533.7370419502258
epoch: 118, training loss: 195.0334135342564, train time: 530.0614891052246
epoch: 119, training loss: 194.5941786673502, train time: 535.5498685836792
epo:119 | HR@5:0.4110 | HR@10:0.5759 | HR@20:0.7514 | NDCG@5:0.5287 | NDCG@10:0.5517 | NDCG@20:0.5882 | recall@5:0.2606 | recall@10:0.4137 | recall@20:0.5827 | precision@5:0.3127 | precision@10:0.2483 | precision@20:0.1748 | best_HR@5:0.4644 | best_HR@10:0.6332 | best_HR@20:0.8075 | best_NDCG@5:0.5287 | best_NDCG@10:0.5517 | best_NDCG@20:0.5882 | best_recall@5:0.2823 | best_recall@10:0.4524 | best_recall@20:0.6246 | best_precision@5:0.3388 | best_precision@10:0.2714 | best_precision@20:0.1874 | 
epoch: 120, training loss: 194.3984474571771, train time: 538.0435636043549
epoch: 121, training loss: 192.79984452173812, train time: 535.3385994434357
epoch: 122, training loss: 193.25704363448313, train time: 532.9842855930328
epoch: 123, training loss: 192.8870295106899, train time: 535.961971282959
epoch: 124, training loss: 192.05728312046267, train time: 533.688538312912
epoch: 125, training loss: 192.12868461135076, train time: 534.2192378044128
epoch: 126, training loss: 191.83693028852576, train time: 535.4761881828308
epoch: 127, training loss: 190.79386717360467, train time: 527.8911118507385
epoch: 128, training loss: 190.03400882097776, train time: 523.7156791687012
epoch: 129, training loss: 189.54082534674671, train time: 521.1785645484924
epoch: 130, training loss: 189.3849769886874, train time: 522.0045411586761
epoch: 131, training loss: 188.83213800445083, train time: 519.6940660476685
epoch: 132, training loss: 187.93927036086097, train time: 520.2587897777557
epoch: 133, training loss: 187.44721740673413, train time: 521.218811750412
epoch: 134, training loss: 186.9737314118829, train time: 522.3375256061554
epoch: 135, training loss: 187.0588948718796, train time: 522.2928776741028
epoch: 136, training loss: 186.61369452861254, train time: 523.7224373817444
epoch: 137, training loss: 186.70994940551464, train time: 525.7743005752563
epoch: 138, training loss: 185.84254828331177, train time: 523.8390784263611
epoch: 139, training loss: 184.56076963050873, train time: 526.5355234146118
epoch: 140, training loss: 184.4995432657597, train time: 529.1338531970978
epoch: 141, training loss: 184.7369370169763, train time: 530.2786650657654
epoch: 142, training loss: 184.59203190222615, train time: 533.8816654682159
epoch: 143, training loss: 182.93550211377442, train time: 530.6950459480286
epoch: 144, training loss: 183.0425462028361, train time: 531.3981559276581
epoch: 145, training loss: 182.48371504194802, train time: 502.4387962818146
epoch: 146, training loss: 183.01464029509225, train time: 483.63513231277466
epoch: 147, training loss: 180.76009481641813, train time: 475.3118305206299
epoch: 148, training loss: 181.747124899528, train time: 472.6153211593628
epoch: 149, training loss: 182.5637138273305, train time: 474.1751170158386
epo:149 | HR@5:0.4133 | HR@10:0.5763 | HR@20:0.7516 | NDCG@5:0.5282 | NDCG@10:0.5513 | NDCG@20:0.5880 | recall@5:0.2651 | recall@10:0.4158 | recall@20:0.5843 | precision@5:0.3181 | precision@10:0.2495 | precision@20:0.1753 | best_HR@5:0.4644 | best_HR@10:0.6332 | best_HR@20:0.8075 | best_NDCG@5:0.5287 | best_NDCG@10:0.5517 | best_NDCG@20:0.5882 | best_recall@5:0.2823 | best_recall@10:0.4524 | best_recall@20:0.6246 | best_precision@5:0.3388 | best_precision@10:0.2714 | best_precision@20:0.1874 | 
training finish
