nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.5299530029296875e-06
user  100 time:  24.362326622009277
user  200 time:  46.83498191833496
user  300 time:  66.65014624595642
user  400 time:  92.95407509803772
user  500 time:  115.11026692390442
user  600 time:  133.79637694358826
user  700 time:  163.67475509643555
user  800 time:  186.68629097938538
user  900 time:  210.80491304397583
user  1000 time:  237.35350108146667
user  1100 time:  272.88229608535767
user  1200 time:  293.1477541923523
user  1300 time:  322.38814997673035
user  1400 time:  345.052841424942
start training item-item instance self attention module...
user  0 time:  2.193450927734375e-05
user  100 time:  190.24271655082703
user  200 time:  379.7664165496826
user  300 time:  573.9284627437592
user  400 time:  765.2589581012726
user  500 time:  954.8461470603943
user  600 time:  1149.8042237758636
user  700 time:  1348.8547568321228
user  800 time:  1541.4685077667236
user  900 time:  1734.011206626892
user  1000 time:  1921.160659790039
user  1100 time:  2116.174202442169
user  1200 time:  2308.9512073993683
user  1300 time:  2495.7812807559967
user  1400 time:  2689.386027097702
start updating user and item embedding...
user_name:1450
user  0 time:  1.3113021850585938e-05
user  100 time:  16.25641918182373
user  200 time:  32.32757377624512
user  300 time:  48.32974600791931
user  400 time:  64.20745182037354
user  500 time:  79.83427095413208
user  600 time:  95.93061900138855
user  700 time:  112.08993339538574
user  800 time:  128.02941584587097
user  900 time:  144.13892078399658
user  1000 time:  159.88908004760742
user  1100 time:  175.9092116355896
user  1200 time:  191.70739793777466
user  1300 time:  207.5939610004425
user  1400 time:  223.7650237083435
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 294.8888777144457, train time: 23.75508141517639
epoch: 1, training loss: 225.7319079247536, train time: 23.180900812149048
epoch: 2, training loss: 211.75786441855598, train time: 22.761807680130005
epoch: 3, training loss: 204.28942058666144, train time: 22.81006669998169
epoch: 4, training loss: 198.84335683175595, train time: 22.589935779571533
epoch: 5, training loss: 195.25649710936705, train time: 22.558124780654907
epoch: 6, training loss: 191.29540185513906, train time: 22.849587202072144
epoch: 7, training loss: 188.02664598327829, train time: 22.78284478187561
epoch: 8, training loss: 185.2691857892787, train time: 22.493311405181885
epoch: 9, training loss: 182.60434836242348, train time: 22.958853244781494
epoch: 10, training loss: 179.94189992902102, train time: 22.36841583251953
epoch: 11, training loss: 177.86887847376056, train time: 23.123448133468628
epoch: 12, training loss: 176.3075869047898, train time: 23.44801354408264
epoch: 13, training loss: 175.0791505825473, train time: 23.10075855255127
epoch: 14, training loss: 171.88183258107165, train time: 22.97413182258606
epoch: 15, training loss: 170.50839174282737, train time: 23.04567265510559
epoch: 16, training loss: 169.15416940432624, train time: 22.5143039226532
epoch: 17, training loss: 167.4300701058819, train time: 22.850221633911133
epoch: 18, training loss: 166.32600890443427, train time: 22.8702654838562
epoch: 19, training loss: 164.79593490014668, train time: 23.09519600868225
epoch: 20, training loss: 163.82271909317933, train time: 23.103501081466675
epoch: 21, training loss: 163.74471813830314, train time: 23.439759254455566
epoch: 22, training loss: 161.85040439330623, train time: 23.204296588897705
epoch: 23, training loss: 161.28471080493182, train time: 23.095017671585083
epoch: 24, training loss: 160.16699318372412, train time: 22.625062465667725
epoch: 25, training loss: 160.2401495091617, train time: 23.324044466018677
epoch: 26, training loss: 158.78430925868452, train time: 23.050135374069214
epoch: 27, training loss: 158.33024663478136, train time: 23.299973726272583
epoch: 28, training loss: 157.22443422244396, train time: 23.026031255722046
epoch: 29, training loss: 156.84101975132944, train time: 22.74103808403015
epoch: 30, training loss: 156.2066950360895, train time: 23.223100185394287
epoch: 31, training loss: 154.991481801786, train time: 22.951422929763794
epoch: 32, training loss: 154.15032202773727, train time: 22.51296854019165
epoch: 33, training loss: 154.70001530554146, train time: 22.7106876373291
epoch: 34, training loss: 152.02415895811282, train time: 22.948282480239868
epoch: 35, training loss: 152.75723135398584, train time: 23.024922847747803
epoch: 36, training loss: 151.92477839183994, train time: 22.360249519348145
epoch: 37, training loss: 151.2448319826217, train time: 23.088340759277344
epoch: 38, training loss: 150.56044650927652, train time: 23.54774498939514
epoch: 39, training loss: 150.41371091170004, train time: 22.976792335510254
epoch: 40, training loss: 150.68858182590338, train time: 22.850682020187378
epoch: 41, training loss: 149.41142338584177, train time: 23.078404188156128
epoch: 42, training loss: 148.415400358208, train time: 23.049686193466187
epoch: 43, training loss: 149.16085344587918, train time: 23.083853721618652
epoch: 44, training loss: 147.08202911814442, train time: 22.975987434387207
epoch: 45, training loss: 147.4435908336891, train time: 23.144153594970703
epoch: 46, training loss: 147.012133985525, train time: 23.242889165878296
epoch: 47, training loss: 146.77485435749986, train time: 22.750948667526245
epoch: 48, training loss: 145.0874774553231, train time: 22.71389651298523
epoch: 49, training loss: 147.3106229560217, train time: 23.382107496261597
epo:49|HR@1:0.3299 | HR@5:0.7024 | HR@10:0.8383 | HR@20:0.9329 | HR@50:0.9907 | NDCG@1:0.3525 | NDCG@5:0.3911 | NDCG@10:0.4226| NDCG@20:0.4713| NDCG@50:0.5843| best_HR@1:0.3299 | best_HR@5:0.7024 | best_HR@10:0.8383 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3525 | best_NDCG@5:0.3911 | best_NDCG@10:0.4226 | best_NDCG@20:0.4713 | best_NDCG@50:0.5843 | train_time:23.38 | test_time:374.75
epoch: 50, training loss: 146.38240956061054, train time: 23.056799173355103
epoch: 51, training loss: 146.1190934430051, train time: 22.469465017318726
epoch: 52, training loss: 144.28070597033366, train time: 22.86935830116272
epoch: 53, training loss: 144.77316541294567, train time: 23.113080978393555
epoch: 54, training loss: 145.97869460252696, train time: 22.808182954788208
epoch: 55, training loss: 144.71369752447936, train time: 22.66294240951538
epoch: 56, training loss: 143.2796492560592, train time: 22.653563976287842
epoch: 57, training loss: 142.14515521220164, train time: 22.594410181045532
epoch: 58, training loss: 142.4655946288549, train time: 23.24063229560852
epoch: 59, training loss: 141.818609597045, train time: 22.5539071559906
epoch: 60, training loss: 140.87747432102333, train time: 22.757767915725708
epoch: 61, training loss: 141.32610213922453, train time: 22.894498586654663
epoch: 62, training loss: 141.70621218177257, train time: 22.679535627365112
epoch: 63, training loss: 141.27305257684202, train time: 22.885679960250854
epoch: 64, training loss: 140.55927398521453, train time: 22.89488410949707
epoch: 65, training loss: 141.18137545417994, train time: 22.362664699554443
epoch: 66, training loss: 141.54831817385275, train time: 22.691810131072998
epoch: 67, training loss: 141.4203104202752, train time: 22.901954412460327
epoch: 68, training loss: 140.41736213315744, train time: 23.033011198043823
epoch: 69, training loss: 140.40071961583453, train time: 23.43809676170349
epoch: 70, training loss: 140.93801605553017, train time: 22.71150803565979
epoch: 71, training loss: 140.72528490558034, train time: 22.713133335113525
epoch: 72, training loss: 140.608002541092, train time: 22.507263660430908
epoch: 73, training loss: 139.95180736671318, train time: 23.272269248962402
epoch: 74, training loss: 139.40500830608653, train time: 22.95934510231018
epoch: 75, training loss: 139.04867030572495, train time: 23.125659227371216
epoch: 76, training loss: 138.62449342830223, train time: 22.53675866127014
epoch: 77, training loss: 138.05208410456544, train time: 22.828126192092896
epoch: 78, training loss: 138.16994695199537, train time: 22.813918828964233
epoch: 79, training loss: 137.89880620350596, train time: 22.869969844818115
epoch: 80, training loss: 137.76771006846684, train time: 23.082154512405396
epoch: 81, training loss: 136.96787851653062, train time: 22.880744218826294
epoch: 82, training loss: 137.74260431612493, train time: 22.733712434768677
epoch: 83, training loss: 137.4817412084085, train time: 23.154216289520264
epoch: 84, training loss: 137.67384189760196, train time: 23.07697319984436
epoch: 85, training loss: 137.01930022421584, train time: 22.87652063369751
epoch: 86, training loss: 138.15546378097497, train time: 22.71245789527893
epoch: 87, training loss: 137.03093327546958, train time: 22.774384260177612
epoch: 88, training loss: 137.940261051961, train time: 22.032775402069092
epoch: 89, training loss: 137.54894088406581, train time: 22.533998727798462
epoch: 90, training loss: 137.64313380222302, train time: 23.014169216156006
epoch: 91, training loss: 136.21015409495158, train time: 22.599344491958618
epoch: 92, training loss: 136.1534084348532, train time: 22.797796726226807
epoch: 93, training loss: 136.27706716020475, train time: 23.0447416305542
epoch: 94, training loss: 135.9252244939271, train time: 23.09615993499756
epoch: 95, training loss: 135.67128726679948, train time: 23.189849615097046
epoch: 96, training loss: 135.61714692003443, train time: 22.96395182609558
epoch: 97, training loss: 135.23666436584608, train time: 22.505576848983765
epoch: 98, training loss: 134.91020407438918, train time: 22.904567003250122
epoch: 99, training loss: 134.5732284385158, train time: 22.94176197052002
epo:99|HR@1:0.3429 | HR@5:0.7077 | HR@10:0.8385 | HR@20:0.9266 | HR@50:0.9849 | NDCG@1:0.3586 | NDCG@5:0.3956 | NDCG@10:0.4280| NDCG@20:0.4791| NDCG@50:0.5935| best_HR@1:0.3429 | best_HR@5:0.7077 | best_HR@10:0.8385 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3586 | best_NDCG@5:0.3956 | best_NDCG@10:0.4280 | best_NDCG@20:0.4791 | best_NDCG@50:0.5935 | train_time:22.94 | test_time:375.03
epoch: 100, training loss: 135.9231053077674, train time: 22.287026405334473
epoch: 101, training loss: 135.6747791961534, train time: 22.961795568466187
epoch: 102, training loss: 136.27149164247385, train time: 22.768697500228882
epoch: 103, training loss: 135.5383656906779, train time: 22.597888231277466
epoch: 104, training loss: 134.15259811349097, train time: 22.402196884155273
epoch: 105, training loss: 133.83343915730075, train time: 22.72613501548767
epoch: 106, training loss: 133.35967443005939, train time: 22.961811780929565
epoch: 107, training loss: 133.59086179020233, train time: 22.70413827896118
epoch: 108, training loss: 132.49542186048348, train time: 22.644330263137817
epoch: 109, training loss: 133.101348327109, train time: 23.033713817596436
epoch: 110, training loss: 133.04699786216952, train time: 22.980238914489746
epoch: 111, training loss: 133.04667699869606, train time: 22.757580995559692
epoch: 112, training loss: 133.1536441758508, train time: 22.89954924583435
epoch: 113, training loss: 131.93365456991887, train time: 22.980974197387695
epoch: 114, training loss: 132.4116495710332, train time: 22.98002028465271
epoch: 115, training loss: 132.78557522887422, train time: 22.477239847183228
epoch: 116, training loss: 133.71157851579483, train time: 22.6408851146698
epoch: 117, training loss: 132.60678325241315, train time: 22.811895847320557
epoch: 118, training loss: 132.92108733669738, train time: 22.777415990829468
epoch: 119, training loss: 132.89692236983683, train time: 22.753650665283203
epoch: 120, training loss: 131.76171664326102, train time: 22.817368030548096
epoch: 121, training loss: 130.42332581541268, train time: 22.912914037704468
epoch: 122, training loss: 131.6271484879835, train time: 22.920467615127563
epoch: 123, training loss: 132.33334730149363, train time: 23.051371335983276
epoch: 124, training loss: 131.632385159377, train time: 23.074307918548584
epoch: 125, training loss: 129.70488225454756, train time: 22.688150882720947
epoch: 126, training loss: 130.69689374696463, train time: 22.919912338256836
epoch: 127, training loss: 130.77156853553606, train time: 22.84432029724121
epoch: 128, training loss: 131.31057332262571, train time: 23.2938015460968
epoch: 129, training loss: 131.46219142654445, train time: 23.07869291305542
epoch: 130, training loss: 130.86760592999053, train time: 23.059991598129272
epoch: 131, training loss: 130.25800546862592, train time: 22.971444129943848
epoch: 132, training loss: 130.98473177163396, train time: 23.458206176757812
epoch: 133, training loss: 130.83899044358986, train time: 23.151687622070312
epoch: 134, training loss: 130.6623398431111, train time: 23.20456600189209
epoch: 135, training loss: 131.00716998212738, train time: 22.850137948989868
epoch: 136, training loss: 130.68069221565383, train time: 22.835747957229614
epoch: 137, training loss: 131.3691865572473, train time: 23.123669624328613
epoch: 138, training loss: 131.04963852031506, train time: 22.24027132987976
epoch: 139, training loss: 130.6014933552069, train time: 23.17869281768799
epoch: 140, training loss: 129.97278094873764, train time: 22.941975593566895
epoch: 141, training loss: 130.2102031168033, train time: 22.558305263519287
epoch: 142, training loss: 131.01952492038254, train time: 23.343873262405396
epoch: 143, training loss: 129.62504269578494, train time: 23.105047464370728
epoch: 144, training loss: 130.38182474533096, train time: 23.417453289031982
epoch: 145, training loss: 130.3991657832812, train time: 22.763036966323853
epoch: 146, training loss: 129.71646031437558, train time: 23.248143911361694
epoch: 147, training loss: 130.17728384969814, train time: 22.83005142211914
epoch: 148, training loss: 129.37756509198516, train time: 23.2006196975708
epoch: 149, training loss: 130.60647690476617, train time: 23.141451835632324
epo:149|HR@1:0.3277 | HR@5:0.7024 | HR@10:0.8392 | HR@20:0.9246 | HR@50:0.9857 | NDCG@1:0.3615 | NDCG@5:0.3973 | NDCG@10:0.4293| NDCG@20:0.4792| NDCG@50:0.5932| best_HR@1:0.3429 | best_HR@5:0.7077 | best_HR@10:0.8392 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:23.14 | test_time:374.57
epoch: 150, training loss: 128.6095168456086, train time: 22.548181295394897
epoch: 151, training loss: 129.3081638552103, train time: 22.74146866798401
epoch: 152, training loss: 127.9775064814894, train time: 23.27344560623169
epoch: 153, training loss: 128.36100308397727, train time: 22.866946935653687
epoch: 154, training loss: 127.71455524253543, train time: 23.034845113754272
epoch: 155, training loss: 128.40630327505642, train time: 23.00647258758545
epoch: 156, training loss: 129.1691497163265, train time: 23.119438648223877
epoch: 157, training loss: 127.82236227529938, train time: 22.328526973724365
epoch: 158, training loss: 129.43781116773607, train time: 22.628751039505005
epoch: 159, training loss: 130.31849639193388, train time: 22.67718815803528
epoch: 160, training loss: 129.78390643259627, train time: 22.537660121917725
epoch: 161, training loss: 129.24068589345552, train time: 22.370469093322754
epoch: 162, training loss: 129.75351099015097, train time: 23.20345640182495
epoch: 163, training loss: 129.72635715112847, train time: 22.645989656448364
epoch: 164, training loss: 128.04513758404937, train time: 22.77345299720764
epoch: 165, training loss: 128.6402960668638, train time: 23.204528331756592
epoch: 166, training loss: 127.48086721051368, train time: 22.337087154388428
epoch: 167, training loss: 127.85497825108177, train time: 22.725087642669678
epoch: 168, training loss: 127.9025030239718, train time: 23.030025959014893
epoch: 169, training loss: 128.0670310765854, train time: 22.523770093917847
epoch: 170, training loss: 126.37831131077837, train time: 22.908818006515503
epoch: 171, training loss: 127.91260410071118, train time: 22.86916470527649
epoch: 172, training loss: 128.05145278976124, train time: 22.536723613739014
epoch: 173, training loss: 127.67889265614212, train time: 23.06724524497986
epoch: 174, training loss: 128.2240530695708, train time: 22.588366508483887
epoch: 175, training loss: 127.51390888285823, train time: 22.996607065200806
epoch: 176, training loss: 127.12716785343946, train time: 22.987492322921753
epoch: 177, training loss: 127.3272455863189, train time: 22.899372577667236
epoch: 178, training loss: 127.3360283219954, train time: 23.22341561317444
epoch: 179, training loss: 126.82794267767167, train time: 22.81212592124939
epoch: 180, training loss: 128.58586801204365, train time: 22.891167163848877
epoch: 181, training loss: 127.30909153813263, train time: 22.969501972198486
epoch: 182, training loss: 127.22530605542124, train time: 23.21159315109253
epoch: 183, training loss: 125.71324249303143, train time: 23.601195335388184
epoch: 184, training loss: 126.8330267201818, train time: 22.653547763824463
epoch: 185, training loss: 126.84389868246217, train time: 22.838744401931763
epoch: 186, training loss: 125.71966059322585, train time: 23.239862203598022
epoch: 187, training loss: 127.42731478832138, train time: 22.766266107559204
epoch: 188, training loss: 126.8425570582258, train time: 22.692168474197388
epoch: 189, training loss: 127.19687196724408, train time: 22.63065218925476
epoch: 190, training loss: 126.42719372002466, train time: 23.14490008354187
epoch: 191, training loss: 128.694495759235, train time: 22.98358464241028
epoch: 192, training loss: 127.08805864743772, train time: 22.806920289993286
epoch: 193, training loss: 127.33855621755356, train time: 23.076510190963745
epoch: 194, training loss: 126.47593032268924, train time: 22.955946922302246
epoch: 195, training loss: 126.67262126965215, train time: 23.185172080993652
epoch: 196, training loss: 127.3473015461932, train time: 23.270814180374146
epoch: 197, training loss: 127.83141428809904, train time: 23.288665056228638
epoch: 198, training loss: 126.67652087281749, train time: 23.05523157119751
epoch: 199, training loss: 126.48387772365822, train time: 22.81922936439514
epo:199|HR@1:0.3398 | HR@5:0.7068 | HR@10:0.8345 | HR@20:0.9195 | HR@50:0.9825 | NDCG@1:0.3533 | NDCG@5:0.3928 | NDCG@10:0.4264| NDCG@20:0.4778| NDCG@50:0.5919| best_HR@1:0.3429 | best_HR@5:0.7077 | best_HR@10:0.8392 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:22.82 | test_time:369.94
epoch: 200, training loss: 126.45785963206436, train time: 23.13221502304077
epoch: 201, training loss: 126.5074023706693, train time: 23.04576563835144
epoch: 202, training loss: 125.41240321978694, train time: 22.555318355560303
epoch: 203, training loss: 125.74339040875202, train time: 23.091150283813477
epoch: 204, training loss: 126.73118655702274, train time: 23.369601488113403
epoch: 205, training loss: 126.14316404504643, train time: 22.88864278793335
epoch: 206, training loss: 125.53994450505706, train time: 22.729316473007202
epoch: 207, training loss: 125.51673739866237, train time: 23.058554887771606
epoch: 208, training loss: 126.93779714284756, train time: 22.734161376953125
epoch: 209, training loss: 126.86760962608969, train time: 22.987046003341675
epoch: 210, training loss: 127.02159948309418, train time: 22.916486978530884
epoch: 211, training loss: 125.6164777369413, train time: 22.532070875167847
epoch: 212, training loss: 126.45030023311847, train time: 22.983927726745605
epoch: 213, training loss: 126.53563523414778, train time: 22.715283155441284
epoch: 214, training loss: 125.43859595095273, train time: 23.068605661392212
epoch: 215, training loss: 126.20881732567796, train time: 22.538162231445312
epoch: 216, training loss: 127.26531480830454, train time: 22.849714517593384
epoch: 217, training loss: 125.92666829764494, train time: 22.92844033241272
epoch: 218, training loss: 126.24294065564754, train time: 23.31636142730713
epoch: 219, training loss: 126.48084299916081, train time: 23.051891326904297
epoch: 220, training loss: 127.4450841921207, train time: 22.883673667907715
epoch: 221, training loss: 126.91922364810307, train time: 22.841511726379395
epoch: 222, training loss: 125.71103431623487, train time: 22.895734548568726
epoch: 223, training loss: 125.95594290018198, train time: 22.882834434509277
epoch: 224, training loss: 125.85688216247945, train time: 22.974021196365356
epoch: 225, training loss: 126.98745130043244, train time: 22.136815071105957
epoch: 226, training loss: 126.12212618911872, train time: 22.526264667510986
epoch: 227, training loss: 126.85699609669973, train time: 23.02821373939514
epoch: 228, training loss: 127.07171364614624, train time: 22.825645923614502
epoch: 229, training loss: 126.30873087549116, train time: 22.919075965881348
epoch: 230, training loss: 126.06598534903605, train time: 22.799304485321045
epoch: 231, training loss: 126.08548394915124, train time: 22.550736665725708
epoch: 232, training loss: 127.33643545513041, train time: 22.857181310653687
epoch: 233, training loss: 126.06385139041231, train time: 23.214006662368774
epoch: 234, training loss: 126.46529490032117, train time: 22.803187608718872
epoch: 235, training loss: 126.30057324863446, train time: 22.415043830871582
epoch: 236, training loss: 125.3066662705969, train time: 23.20246648788452
epoch: 237, training loss: 125.47786816812004, train time: 23.331453323364258
epoch: 238, training loss: 126.14764845899481, train time: 22.667372703552246
epoch: 239, training loss: 125.66043412974977, train time: 22.49031972885132
epoch: 240, training loss: 124.69525764955324, train time: 23.20732545852661
epoch: 241, training loss: 127.11894790182123, train time: 22.869993925094604
epoch: 242, training loss: 125.89626642236544, train time: 23.17878532409668
epoch: 243, training loss: 126.89289288432337, train time: 22.73273491859436
epoch: 244, training loss: 125.81807331675373, train time: 22.7733051776886
epoch: 245, training loss: 126.07046413427452, train time: 23.138997554779053
epoch: 246, training loss: 126.0819168400194, train time: 22.349204540252686
epoch: 247, training loss: 125.91537490258634, train time: 23.073699712753296
epoch: 248, training loss: 125.30656138362247, train time: 22.72002649307251
epoch: 249, training loss: 125.32926263622358, train time: 23.316205263137817
epo:249|HR@1:0.3398 | HR@5:0.7160 | HR@10:0.8401 | HR@20:0.9239 | HR@50:0.9820 | NDCG@1:0.3189 | NDCG@5:0.3594 | NDCG@10:0.3943| NDCG@20:0.4482| NDCG@50:0.5674| best_HR@1:0.3429 | best_HR@5:0.7160 | best_HR@10:0.8401 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:23.32 | test_time:376.59
epoch: 250, training loss: 124.85493048299395, train time: 23.293646812438965
epoch: 251, training loss: 126.1985403944127, train time: 22.709470987319946
epoch: 252, training loss: 125.46110811943072, train time: 22.810482025146484
epoch: 253, training loss: 125.41670581187645, train time: 22.379297971725464
epoch: 254, training loss: 126.1777738200035, train time: 22.66609525680542
epoch: 255, training loss: 126.5762954140373, train time: 22.718955278396606
epoch: 256, training loss: 126.42898580439214, train time: 22.861456394195557
epoch: 257, training loss: 125.79400791785156, train time: 22.92614245414734
epoch: 258, training loss: 125.69753079678048, train time: 22.52383327484131
epoch: 259, training loss: 125.83364000846632, train time: 22.54253840446472
epoch: 260, training loss: 124.76077499127132, train time: 23.046895027160645
epoch: 261, training loss: 125.1475081271492, train time: 23.021252870559692
epoch: 262, training loss: 126.75969688784971, train time: 23.19821786880493
epoch: 263, training loss: 125.99969529514783, train time: 23.253976106643677
epoch: 264, training loss: 127.20705108542461, train time: 22.81392812728882
epoch: 265, training loss: 125.5849672943732, train time: 22.740286350250244
epoch: 266, training loss: 124.7719157567044, train time: 22.725489616394043
epoch: 267, training loss: 125.15907712430635, train time: 22.948193550109863
epoch: 268, training loss: 124.90534955718613, train time: 23.095641136169434
epoch: 269, training loss: 126.46545682811848, train time: 22.25012731552124
epoch: 270, training loss: 126.17304891893582, train time: 23.16885805130005
epoch: 271, training loss: 127.32493943398003, train time: 22.834869146347046
epoch: 272, training loss: 126.71140427795763, train time: 22.590169191360474
epoch: 273, training loss: 126.26775174919749, train time: 22.74137544631958
epoch: 274, training loss: 126.03099031423335, train time: 23.04474115371704
epoch: 275, training loss: 124.46634517697385, train time: 22.613792181015015
epoch: 276, training loss: 126.73012950655539, train time: 22.62996530532837
epoch: 277, training loss: 125.78881586661737, train time: 23.071406841278076
epoch: 278, training loss: 125.86105390015291, train time: 22.76866626739502
epoch: 279, training loss: 125.20970380383369, train time: 23.09423542022705
epoch: 280, training loss: 124.91382993180014, train time: 22.91427731513977
epoch: 281, training loss: 125.73718415708572, train time: 22.84782075881958
epoch: 282, training loss: 126.63836463811458, train time: 22.52737331390381
epoch: 283, training loss: 125.78676058574638, train time: 22.804479837417603
epoch: 284, training loss: 125.64948308741441, train time: 22.667860984802246
epoch: 285, training loss: 125.02035651727056, train time: 22.87381410598755
epoch: 286, training loss: 125.90244402934331, train time: 23.266860246658325
epoch: 287, training loss: 124.65943041713035, train time: 22.402170658111572
epoch: 288, training loss: 125.57801060624479, train time: 22.50556492805481
epoch: 289, training loss: 126.77009019926481, train time: 23.00037121772766
epoch: 290, training loss: 126.18470507879101, train time: 22.47111463546753
epoch: 291, training loss: 127.26319608074846, train time: 23.251392602920532
epoch: 292, training loss: 126.96286456336384, train time: 22.918792724609375
epoch: 293, training loss: 127.05964475039218, train time: 22.78633737564087
epoch: 294, training loss: 126.29890454471752, train time: 22.265899658203125
epoch: 295, training loss: 125.38184121028462, train time: 23.15585231781006
epoch: 296, training loss: 125.48429191234754, train time: 23.04524040222168
epoch: 297, training loss: 125.54472785393591, train time: 22.50659441947937
epoch: 298, training loss: 125.97092427551979, train time: 22.5612051486969
epoch: 299, training loss: 125.91025562421419, train time: 22.53914499282837
epo:299|HR@1:0.3432 | HR@5:0.7163 | HR@10:0.8390 | HR@20:0.9202 | HR@50:0.9855 | NDCG@1:0.3304 | NDCG@5:0.3683 | NDCG@10:0.4017| NDCG@20:0.4540| NDCG@50:0.5718| best_HR@1:0.3432 | best_HR@5:0.7163 | best_HR@10:0.8401 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:22.54 | test_time:376.09
epoch: 300, training loss: 126.13258022491937, train time: 23.405474185943604
epoch: 301, training loss: 125.3544928956544, train time: 23.075072526931763
epoch: 302, training loss: 126.00187188854034, train time: 23.260451316833496
epoch: 303, training loss: 125.3370867268386, train time: 22.663125038146973
epoch: 304, training loss: 125.79869952493755, train time: 22.856059312820435
epoch: 305, training loss: 124.89363315247465, train time: 22.709816217422485
epoch: 306, training loss: 125.60609354030748, train time: 23.001627206802368
epoch: 307, training loss: 124.67161532252794, train time: 23.151803255081177
epoch: 308, training loss: 125.99169839909882, train time: 22.49811339378357
epoch: 309, training loss: 125.70288351521594, train time: 23.206925630569458
epoch: 310, training loss: 124.23492561784224, train time: 23.252360343933105
epoch: 311, training loss: 125.0046108012757, train time: 23.169806718826294
epoch: 312, training loss: 125.34091154679481, train time: 22.88671588897705
epoch: 313, training loss: 125.61666355887428, train time: 22.709360122680664
epoch: 314, training loss: 125.42041236255318, train time: 22.84782075881958
epoch: 315, training loss: 125.68465150755947, train time: 22.993025302886963
epoch: 316, training loss: 126.05359881889308, train time: 22.897775411605835
epoch: 317, training loss: 125.73324049037183, train time: 23.03274393081665
epoch: 318, training loss: 126.26571159873856, train time: 22.30013132095337
epoch: 319, training loss: 124.99810219863139, train time: 22.51084804534912
epoch: 320, training loss: 125.2058620617754, train time: 23.022902965545654
epoch: 321, training loss: 124.67373255139682, train time: 23.044188261032104
epoch: 322, training loss: 123.94793741815374, train time: 22.812312841415405
epoch: 323, training loss: 124.98611740929482, train time: 22.965827703475952
epoch: 324, training loss: 124.23229198696208, train time: 22.613261461257935
epoch: 325, training loss: 123.33459093532292, train time: 22.896427154541016
epoch: 326, training loss: 124.1405401873053, train time: 23.086639404296875
epoch: 327, training loss: 125.84887094852456, train time: 22.78937792778015
epoch: 328, training loss: 125.3514820579876, train time: 22.984361171722412
epoch: 329, training loss: 125.17571002469049, train time: 22.78704571723938
epoch: 330, training loss: 126.19064831055584, train time: 22.732455015182495
epoch: 331, training loss: 124.87511381752847, train time: 22.942296981811523
epoch: 332, training loss: 125.07829514432524, train time: 22.724090337753296
epoch: 333, training loss: 126.04497262692894, train time: 23.234374284744263
epoch: 334, training loss: 124.51943596119236, train time: 22.85120916366577
epoch: 335, training loss: 124.05081968281593, train time: 22.326524257659912
epoch: 336, training loss: 123.39657840650762, train time: 22.900830268859863
epoch: 337, training loss: 124.71035311641754, train time: 22.738640069961548
epoch: 338, training loss: 123.78153899760218, train time: 23.31000542640686
epoch: 339, training loss: 124.82181465570466, train time: 23.24346375465393
epoch: 340, training loss: 125.29817188999732, train time: 23.254270553588867
epoch: 341, training loss: 124.0466466708458, train time: 23.287161588668823
epoch: 342, training loss: 124.40521656069905, train time: 22.570202827453613
epoch: 343, training loss: 123.5623368028173, train time: 22.785664558410645
epoch: 344, training loss: 123.71056269724795, train time: 22.682782411575317
epoch: 345, training loss: 125.27409172106127, train time: 22.718451499938965
epoch: 346, training loss: 125.12353202453232, train time: 22.827468156814575
epoch: 347, training loss: 124.14076327900693, train time: 23.18980836868286
epoch: 348, training loss: 125.67889165057568, train time: 22.704800128936768
epoch: 349, training loss: 124.49405969683721, train time: 23.151822090148926
epo:349|HR@1:0.3422 | HR@5:0.7120 | HR@10:0.8418 | HR@20:0.9214 | HR@50:0.9840 | NDCG@1:0.3203 | NDCG@5:0.3585 | NDCG@10:0.3933| NDCG@20:0.4468| NDCG@50:0.5658| best_HR@1:0.3432 | best_HR@5:0.7163 | best_HR@10:0.8418 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:23.15 | test_time:375.66
epoch: 350, training loss: 125.72102217415522, train time: 23.389272451400757
epoch: 351, training loss: 125.1947956294025, train time: 22.976513862609863
epoch: 352, training loss: 126.02486563034472, train time: 23.326369285583496
epoch: 353, training loss: 126.3773554837826, train time: 23.025147438049316
epoch: 354, training loss: 125.08079461079615, train time: 22.780120372772217
epoch: 355, training loss: 125.89050071456586, train time: 22.782341241836548
epoch: 356, training loss: 126.32392015942605, train time: 22.986495971679688
epoch: 357, training loss: 126.4889188665984, train time: 22.718661069869995
epoch: 358, training loss: 125.69052643743635, train time: 23.879770517349243
epoch: 359, training loss: 124.80257178138709, train time: 23.329163551330566
epoch: 360, training loss: 124.2220094428194, train time: 22.980907201766968
epoch: 361, training loss: 125.87655600691505, train time: 22.628972053527832
epoch: 362, training loss: 126.14667979089427, train time: 22.94064450263977
epoch: 363, training loss: 125.94154159777099, train time: 22.888862371444702
epoch: 364, training loss: 125.86833183282579, train time: 22.480698347091675
epoch: 365, training loss: 126.7278084064892, train time: 22.77827000617981
epoch: 366, training loss: 126.51136568677612, train time: 22.838846445083618
epoch: 367, training loss: 125.74243778329401, train time: 23.13842511177063
epoch: 368, training loss: 126.0613691334147, train time: 22.94303822517395
epoch: 369, training loss: 125.06241044492344, train time: 22.885298013687134
epoch: 370, training loss: 125.50329848664114, train time: 22.923063039779663
epoch: 371, training loss: 124.29539992084028, train time: 22.95210599899292
epoch: 372, training loss: 124.85218124536914, train time: 22.646950244903564
epoch: 373, training loss: 124.75931844841398, train time: 23.071945428848267
epoch: 374, training loss: 124.35962002525048, train time: 23.2248694896698
epoch: 375, training loss: 124.94347556654247, train time: 22.783522367477417
epoch: 376, training loss: 124.4428296842234, train time: 22.79599618911743
epoch: 377, training loss: 124.64966720039956, train time: 23.016064167022705
epoch: 378, training loss: 125.20862166161533, train time: 22.95583987236023
epoch: 379, training loss: 124.54420514246158, train time: 23.39985990524292
epoch: 380, training loss: 125.41962963361584, train time: 22.850269556045532
epoch: 381, training loss: 125.47028862757725, train time: 23.202545166015625
epoch: 382, training loss: 125.21305786652374, train time: 23.29872155189514
epoch: 383, training loss: 126.62552863180463, train time: 23.01625084877014
epoch: 384, training loss: 126.01244309261529, train time: 23.024247646331787
epoch: 385, training loss: 125.40644162461103, train time: 23.080260038375854
epoch: 386, training loss: 126.16992405999918, train time: 22.86772632598877
epoch: 387, training loss: 125.51645758512313, train time: 23.108564138412476
epoch: 388, training loss: 125.01586440125539, train time: 22.798061847686768
epoch: 389, training loss: 125.24850051644898, train time: 22.926702737808228
epoch: 390, training loss: 124.85900625285285, train time: 23.12980580329895
epoch: 391, training loss: 124.96263414299756, train time: 22.745070695877075
epoch: 392, training loss: 125.00215974873572, train time: 23.246708154678345
epoch: 393, training loss: 124.33540770350373, train time: 22.750380277633667
epoch: 394, training loss: 125.65126083292125, train time: 22.646181344985962
epoch: 395, training loss: 125.49244618386729, train time: 22.997196197509766
epoch: 396, training loss: 124.76484853282454, train time: 22.844972372055054
epoch: 397, training loss: 124.53743806714192, train time: 22.358486890792847
epoch: 398, training loss: 125.44548773216957, train time: 22.886186838150024
epoch: 399, training loss: 126.29153381753713, train time: 22.931565046310425
epo:399|HR@1:0.3417 | HR@5:0.7134 | HR@10:0.8425 | HR@20:0.9241 | HR@50:0.9817 | NDCG@1:0.2981 | NDCG@5:0.3414 | NDCG@10:0.3762| NDCG@20:0.4302| NDCG@50:0.5511| best_HR@1:0.3432 | best_HR@5:0.7163 | best_HR@10:0.8425 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:22.93 | test_time:375.36
epoch: 400, training loss: 124.9113171656063, train time: 22.81797957420349
epoch: 401, training loss: 124.45966004788352, train time: 22.58791756629944
epoch: 402, training loss: 124.94343626333284, train time: 22.40168571472168
epoch: 403, training loss: 124.4994951216795, train time: 22.464419841766357
epoch: 404, training loss: 125.13261661023716, train time: 23.089319944381714
epoch: 405, training loss: 125.2366583540861, train time: 22.99322509765625
epoch: 406, training loss: 125.07502867655421, train time: 23.246586084365845
epoch: 407, training loss: 124.19122711765522, train time: 23.244670391082764
epoch: 408, training loss: 124.41247533133719, train time: 23.001453399658203
epoch: 409, training loss: 125.49616209513624, train time: 23.114556789398193
epoch: 410, training loss: 125.69855607235513, train time: 22.822730779647827
epoch: 411, training loss: 124.37317267869366, train time: 22.801323652267456
epoch: 412, training loss: 124.79148100748716, train time: 22.632075786590576
epoch: 413, training loss: 126.19524205724883, train time: 23.352018356323242
epoch: 414, training loss: 124.72646732449357, train time: 22.256093502044678
epoch: 415, training loss: 124.9804818531411, train time: 22.50612235069275
epoch: 416, training loss: 125.5439848084934, train time: 22.742539167404175
epoch: 417, training loss: 125.77447796365595, train time: 23.095364570617676
epoch: 418, training loss: 126.53909419997944, train time: 22.97249674797058
epoch: 419, training loss: 125.90087049952126, train time: 23.052861213684082
epoch: 420, training loss: 126.42218966240762, train time: 22.7468683719635
epoch: 421, training loss: 125.23915905348258, train time: 23.066502809524536
epoch: 422, training loss: 126.70264702539862, train time: 22.82169795036316
epoch: 423, training loss: 125.69239451877365, train time: 23.320080280303955
epoch: 424, training loss: 126.0969315362745, train time: 22.352449893951416
epoch: 425, training loss: 127.20475878615252, train time: 22.989468812942505
epoch: 426, training loss: 124.7627271345118, train time: 22.614060640335083
epoch: 427, training loss: 126.72853732936346, train time: 23.39019536972046
epoch: 428, training loss: 125.73753637034679, train time: 23.384544372558594
epoch: 429, training loss: 126.35973553685471, train time: 23.091864347457886
epoch: 430, training loss: 127.33481637167279, train time: 23.03550410270691
epoch: 431, training loss: 126.37452038879564, train time: 22.988146781921387
epoch: 432, training loss: 125.38875640105107, train time: 22.932116746902466
epoch: 433, training loss: 126.84358737294679, train time: 22.600372076034546
epoch: 434, training loss: 125.8633710025606, train time: 23.21133017539978
epoch: 435, training loss: 125.2737935618643, train time: 22.941807508468628
epoch: 436, training loss: 125.48576524150849, train time: 22.912899017333984
epoch: 437, training loss: 126.1305471745145, train time: 22.80336880683899
epoch: 438, training loss: 125.80970654651173, train time: 22.68330216407776
epoch: 439, training loss: 125.58815233701898, train time: 23.268816709518433
epoch: 440, training loss: 126.3713886224723, train time: 22.474365711212158
epoch: 441, training loss: 127.25993486666994, train time: 23.074800968170166
epoch: 442, training loss: 126.00887520691322, train time: 22.885143518447876
epoch: 443, training loss: 124.02986624302866, train time: 23.037803411483765
epoch: 444, training loss: 124.40386536472943, train time: 22.853129625320435
epoch: 445, training loss: 126.01923997522681, train time: 23.12246346473694
epoch: 446, training loss: 127.18714416974399, train time: 22.723240852355957
epoch: 447, training loss: 125.23041802669468, train time: 23.24154806137085
epoch: 448, training loss: 124.70151961808733, train time: 23.13651156425476
epoch: 449, training loss: 125.739944577188, train time: 23.07583260536194
epo:449|HR@1:0.3464 | HR@5:0.7182 | HR@10:0.8392 | HR@20:0.9171 | HR@50:0.9785 | NDCG@1:0.2716 | NDCG@5:0.3151 | NDCG@10:0.3522| NDCG@20:0.4088| NDCG@50:0.5324| best_HR@1:0.3464 | best_HR@5:0.7182 | best_HR@10:0.8425 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:23.08 | test_time:376.33
epoch: 450, training loss: 125.67484906774189, train time: 22.993797779083252
epoch: 451, training loss: 126.21562855085358, train time: 22.832006692886353
epoch: 452, training loss: 125.67871418458526, train time: 22.992091178894043
epoch: 453, training loss: 125.75935660082905, train time: 22.708086013793945
epoch: 454, training loss: 126.3460756817949, train time: 23.10977602005005
epoch: 455, training loss: 127.24379133225011, train time: 22.58810043334961
epoch: 456, training loss: 126.06978290935513, train time: 22.715810775756836
epoch: 457, training loss: 124.78271662598127, train time: 23.10804510116577
epoch: 458, training loss: 125.63072826404823, train time: 22.934715032577515
epoch: 459, training loss: 126.3738889337983, train time: 23.086496591567993
epoch: 460, training loss: 125.52449536303175, train time: 23.137823820114136
epoch: 461, training loss: 125.77160767652094, train time: 22.676098346710205
epoch: 462, training loss: 124.54900789378735, train time: 22.615874528884888
epoch: 463, training loss: 125.15599437044875, train time: 22.356814861297607
epoch: 464, training loss: 126.44872913032304, train time: 22.85042715072632
epoch: 465, training loss: 125.47948443733912, train time: 23.006991863250732
epoch: 466, training loss: 125.5702824092732, train time: 22.971198320388794
epoch: 467, training loss: 125.71493720593571, train time: 23.215754985809326
epoch: 468, training loss: 125.4748964094033, train time: 22.76006531715393
epoch: 469, training loss: 125.9518185503257, train time: 23.220571756362915
epoch: 470, training loss: 124.5856569281168, train time: 23.02859139442444
epoch: 471, training loss: 126.26116836462461, train time: 23.23656964302063
epoch: 472, training loss: 125.72347423736937, train time: 23.224467992782593
epoch: 473, training loss: 125.72306133332313, train time: 22.80477476119995
epoch: 474, training loss: 126.09469733513833, train time: 23.214138984680176
epoch: 475, training loss: 125.86605878060072, train time: 22.92402672767639
epoch: 476, training loss: 125.12112531645107, train time: 22.931273221969604
epoch: 477, training loss: 125.25740026104904, train time: 23.094881772994995
epoch: 478, training loss: 125.45636030087189, train time: 22.461159467697144
epoch: 479, training loss: 125.05215145982947, train time: 23.20979404449463
epoch: 480, training loss: 126.20201414638723, train time: 22.97316575050354
epoch: 481, training loss: 124.77535624832672, train time: 22.746291160583496
epoch: 482, training loss: 124.66046412811556, train time: 23.107059717178345
epoch: 483, training loss: 124.80211171602423, train time: 23.403839588165283
epoch: 484, training loss: 124.78625360963633, train time: 23.381904363632202
epoch: 485, training loss: 125.11713807737397, train time: 23.0124351978302
epoch: 486, training loss: 125.59395042761753, train time: 22.76433300971985
epoch: 487, training loss: 125.52521562254697, train time: 22.98689603805542
epoch: 488, training loss: 125.31802195678756, train time: 23.250778675079346
epoch: 489, training loss: 126.67961233010283, train time: 22.761011838912964
epoch: 490, training loss: 125.37720880621055, train time: 23.092280626296997
epoch: 491, training loss: 125.47303276104503, train time: 22.619879722595215
epoch: 492, training loss: 125.1369427798345, train time: 23.118586540222168
epoch: 493, training loss: 123.43774156495056, train time: 23.324737071990967
epoch: 494, training loss: 125.76263955885952, train time: 22.204811334609985
epoch: 495, training loss: 125.69171541229298, train time: 22.901825666427612
epoch: 496, training loss: 125.71029925646144, train time: 23.506770610809326
epoch: 497, training loss: 125.98239492904395, train time: 22.96755027770996
epoch: 498, training loss: 126.59761027811328, train time: 23.12103772163391
epoch: 499, training loss: 126.83192086368217, train time: 23.04245138168335
epo:499|HR@1:0.3367 | HR@5:0.7149 | HR@10:0.8318 | HR@20:0.9164 | HR@50:0.9782 | NDCG@1:0.2899 | NDCG@5:0.3363 | NDCG@10:0.3737| NDCG@20:0.4296| NDCG@50:0.5497| best_HR@1:0.3464 | best_HR@5:0.7182 | best_HR@10:0.8425 | best_HR@20:0.9329 | best_HR@50:0.9907 | best_NDCG@1:0.3615 | best_NDCG@5:0.3973 | best_NDCG@10:0.4293 | best_NDCG@20:0.4792 | best_NDCG@50:0.5935 | train_time:23.04 | test_time:376.80
training finish
