nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.0531158447265625e-06
user  100 time:  132.4954514503479
user  200 time:  263.39648056030273
user  300 time:  397.907466173172
user  400 time:  544.2340891361237
user  500 time:  668.9309713840485
user  600 time:  809.1597001552582
user  700 time:  937.9341883659363
user  800 time:  1073.058307170868
user  900 time:  1220.4237694740295
user  1000 time:  1377.2515442371368
user  1100 time:  1548.7077343463898
user  1200 time:  1693.5643775463104
user  1300 time:  1865.14097738266
user  1400 time:  2007.5740005970001
start training item-item instance self attention module...
user  0 time:  9.775161743164062e-06
user  100 time:  125.03491401672363
user  200 time:  239.90569186210632
user  300 time:  371.8154647350311
user  400 time:  484.0562834739685
user  500 time:  593.5398530960083
user  600 time:  723.5635440349579
user  700 time:  824.9339253902435
user  800 time:  922.667662858963
user  900 time:  1038.7565031051636
user  1000 time:  1143.8468127250671
user  1100 time:  1265.6979172229767
user  1200 time:  1390.41365981102
user  1300 time:  1502.944875240326
user  1400 time:  1612.9968755245209
start updating user and item embedding...
user_name:1450
user  0 time:  1.1920928955078125e-05
user  100 time:  18.285043478012085
user  200 time:  36.51214933395386
user  300 time:  54.900545835494995
user  400 time:  72.9974946975708
user  500 time:  91.20948791503906
user  600 time:  109.61918544769287
user  700 time:  127.5992910861969
user  800 time:  145.6529450416565
user  900 time:  163.8852469921112
user  1000 time:  182.1064383983612
user  1100 time:  200.46085262298584
user  1200 time:  218.99977445602417
user  1300 time:  237.26153230667114
user  1400 time:  255.41626977920532
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 325.9686075479258, train time: 45.90922713279724
epoch: 1, training loss: 294.8141094951425, train time: 46.78613042831421
epoch: 2, training loss: 238.14403228333686, train time: 45.38427686691284
epoch: 3, training loss: 193.04328719430487, train time: 44.58470273017883
epoch: 4, training loss: 179.76822009071475, train time: 41.72320628166199
epoch: 5, training loss: 173.0811079252453, train time: 42.32254505157471
epoch: 6, training loss: 168.07543010669178, train time: 43.36134457588196
epoch: 7, training loss: 164.05208227518597, train time: 41.88981318473816
epoch: 8, training loss: 160.8715915484936, train time: 49.11304306983948
epoch: 9, training loss: 158.03043324680766, train time: 41.50383806228638
epoch: 10, training loss: 156.32762083725538, train time: 43.99833536148071
epoch: 11, training loss: 152.78527486686653, train time: 46.85988426208496
epoch: 12, training loss: 150.93997832690366, train time: 45.87976336479187
epoch: 13, training loss: 150.25588180434715, train time: 45.265976667404175
epoch: 14, training loss: 148.93653556023492, train time: 46.04668617248535
epoch: 15, training loss: 146.03957176160475, train time: 42.08495736122131
epoch: 16, training loss: 144.8456315578369, train time: 46.16494274139404
epoch: 17, training loss: 143.6122698019317, train time: 43.342873096466064
epoch: 18, training loss: 142.67216580857348, train time: 45.37450098991394
epoch: 19, training loss: 140.22240619792137, train time: 49.25918531417847
epoch: 20, training loss: 139.57086092542158, train time: 40.98960328102112
epoch: 21, training loss: 138.18784206543933, train time: 41.08708739280701
epoch: 22, training loss: 138.13346737955726, train time: 47.2632622718811
epoch: 23, training loss: 137.75993487959204, train time: 43.50933480262756
epoch: 24, training loss: 136.6805625503912, train time: 41.25678992271423
epoch: 25, training loss: 135.59256697220553, train time: 46.31190204620361
epoch: 26, training loss: 134.54230819342774, train time: 42.70751738548279
epoch: 27, training loss: 135.00613141508074, train time: 44.90289568901062
epoch: 28, training loss: 133.54970895886072, train time: 41.36046242713928
epoch: 29, training loss: 131.21464501912124, train time: 41.08483529090881
epoch: 30, training loss: 131.16339246826828, train time: 45.400169372558594
epoch: 31, training loss: 131.28731284513196, train time: 45.67216467857361
epoch: 32, training loss: 130.25636548371403, train time: 46.78347206115723
epoch: 33, training loss: 128.73628948973783, train time: 47.52149200439453
epoch: 34, training loss: 128.86753336071706, train time: 48.61542010307312
epoch: 35, training loss: 129.24920355650102, train time: 47.30172657966614
epoch: 36, training loss: 127.87533038329275, train time: 44.40581798553467
epoch: 37, training loss: 127.65543104285462, train time: 45.70791172981262
epoch: 38, training loss: 126.78996629257017, train time: 45.403491258621216
epoch: 39, training loss: 126.87888567071059, train time: 44.80841565132141
epoch: 40, training loss: 126.25308242900064, train time: 42.23208785057068
epoch: 41, training loss: 125.85142699501012, train time: 45.71020221710205
epoch: 42, training loss: 126.42013379727723, train time: 40.63926410675049
epoch: 43, training loss: 124.03650121801184, train time: 41.714479207992554
epoch: 44, training loss: 124.3210698571711, train time: 47.38181257247925
epoch: 45, training loss: 124.75375125732535, train time: 43.72168803215027
epoch: 46, training loss: 123.64005670828192, train time: 41.4474310874939
epoch: 47, training loss: 123.001543981547, train time: 41.05989909172058
epoch: 48, training loss: 123.62884232154465, train time: 42.53894805908203
epoch: 49, training loss: 122.47013097137824, train time: 43.16786456108093
epo:49|HR@1:0.4684 | HR@5:0.8467 | HR@10:0.9259 | HR@20:0.9647 | HR@50:0.9930 | NDCG@1:0.4202 | NDCG@5:0.4529 | NDCG@10:0.4795| NDCG@20:0.5215| NDCG@50:0.6214| best_HR@1:0.4684 | best_HR@5:0.8467 | best_HR@10:0.9259 | best_HR@20:0.9647 | best_HR@50:0.9930 | best_NDCG@1:0.4202 | best_NDCG@5:0.4529 | best_NDCG@10:0.4795 | best_NDCG@20:0.5215 | best_NDCG@50:0.6214 | train_time:43.17 | test_time:383.81
epoch: 50, training loss: 122.01853299536378, train time: 45.82094097137451
epoch: 51, training loss: 119.67182982494705, train time: 42.26414608955383
epoch: 52, training loss: 119.36217115454929, train time: 48.46073627471924
epoch: 53, training loss: 120.79497623597126, train time: 43.088855504989624
epoch: 54, training loss: 121.87608582964458, train time: 46.5749135017395
epoch: 55, training loss: 120.4469747465846, train time: 41.15745973587036
epoch: 56, training loss: 120.41071912570624, train time: 40.98614740371704
epoch: 57, training loss: 119.18288646287692, train time: 41.086118936538696
epoch: 58, training loss: 119.61465610135201, train time: 48.29973649978638
epoch: 59, training loss: 119.47967172507197, train time: 45.93846082687378
epoch: 60, training loss: 118.5297799954642, train time: 44.90496301651001
epoch: 61, training loss: 118.09444044776683, train time: 47.36746406555176
epoch: 62, training loss: 117.57461020003029, train time: 41.14609098434448
epoch: 63, training loss: 117.8771181025586, train time: 41.02611041069031
epoch: 64, training loss: 117.0668209174546, train time: 41.670238733291626
epoch: 65, training loss: 117.39330545549456, train time: 44.90613579750061
epoch: 66, training loss: 117.90483418008807, train time: 38.58374643325806
epoch: 67, training loss: 117.31676112065179, train time: 43.19512176513672
epoch: 68, training loss: 117.27999515477131, train time: 41.07257843017578
epoch: 69, training loss: 117.41714977337324, train time: 47.718687295913696
epoch: 70, training loss: 115.15825170251264, train time: 46.00407695770264
epoch: 71, training loss: 114.77370909905585, train time: 46.02392864227295
epoch: 72, training loss: 115.7620451152925, train time: 47.012083768844604
epoch: 73, training loss: 116.8359544950763, train time: 48.41691040992737
epoch: 74, training loss: 117.29074896781458, train time: 42.22230386734009
epoch: 75, training loss: 115.7503905334961, train time: 49.49531555175781
epoch: 76, training loss: 115.86143296440059, train time: 46.23278546333313
epoch: 77, training loss: 116.20274642933146, train time: 43.79653835296631
epoch: 78, training loss: 115.07158274292306, train time: 43.42529368400574
epoch: 79, training loss: 114.55210218510183, train time: 45.254398345947266
epoch: 80, training loss: 114.69389124342706, train time: 41.2756929397583
epoch: 81, training loss: 113.7217228529662, train time: 48.8723669052124
epoch: 82, training loss: 114.0354762182651, train time: 45.78323936462402
epoch: 83, training loss: 115.16122310564242, train time: 45.840192794799805
epoch: 84, training loss: 114.06223537894402, train time: 43.2200562953949
epoch: 85, training loss: 113.5320566337632, train time: 41.0054075717926
epoch: 86, training loss: 112.99299821602835, train time: 45.96094369888306
epoch: 87, training loss: 113.7968023250105, train time: 43.744330406188965
epoch: 88, training loss: 112.32074136865413, train time: 45.23140740394592
epoch: 89, training loss: 113.78548176992626, train time: 47.91560435295105
epoch: 90, training loss: 112.01627045303758, train time: 48.53365778923035
epoch: 91, training loss: 112.85337008488568, train time: 42.11509561538696
epoch: 92, training loss: 114.17972227904829, train time: 45.24569630622864
epoch: 93, training loss: 112.72987000646208, train time: 45.66339826583862
epoch: 94, training loss: 111.35253836515403, train time: 46.475109338760376
epoch: 95, training loss: 113.10765688230458, train time: 40.91633749008179
epoch: 96, training loss: 112.348241532869, train time: 42.808067083358765
epoch: 97, training loss: 111.8822498831214, train time: 41.18903827667236
epoch: 98, training loss: 112.22725892804374, train time: 41.17396569252014
epoch: 99, training loss: 110.62133609550074, train time: 41.22933840751648
epo:99|HR@1:0.4710 | HR@5:0.8424 | HR@10:0.9177 | HR@20:0.9609 | HR@50:0.9901 | NDCG@1:0.4014 | NDCG@5:0.4383 | NDCG@10:0.4665| NDCG@20:0.5094| NDCG@50:0.6113| best_HR@1:0.4710 | best_HR@5:0.8467 | best_HR@10:0.9259 | best_HR@20:0.9647 | best_HR@50:0.9930 | best_NDCG@1:0.4202 | best_NDCG@5:0.4529 | best_NDCG@10:0.4795 | best_NDCG@20:0.5215 | best_NDCG@50:0.6214 | train_time:41.23 | test_time:391.00
epoch: 100, training loss: 111.53114515936613, train time: 43.51154041290283
epoch: 101, training loss: 111.69778635590046, train time: 42.987236738204956
epoch: 102, training loss: 112.0623671242538, train time: 43.73291873931885
epoch: 103, training loss: 113.63731733850727, train time: 41.62129807472229
epoch: 104, training loss: 110.94847574627056, train time: 40.891533613204956
epoch: 105, training loss: 111.77158192899515, train time: 40.85869026184082
epoch: 106, training loss: 110.0561860973321, train time: 47.678093910217285
epoch: 107, training loss: 111.22461172965996, train time: 47.368744134902954
epoch: 108, training loss: 112.17522169963195, train time: 42.6234974861145
epoch: 109, training loss: 111.75971688074424, train time: 41.34479594230652
epoch: 110, training loss: 110.70674287437578, train time: 46.21695685386658
epoch: 111, training loss: 111.27942216767406, train time: 40.967418909072876
epoch: 112, training loss: 110.08068020193605, train time: 40.967132806777954
epoch: 113, training loss: 112.09262669290001, train time: 41.13172268867493
epoch: 114, training loss: 110.50078143021528, train time: 40.943939208984375
epoch: 115, training loss: 109.11850910556313, train time: 41.11460518836975
epoch: 116, training loss: 109.82835539889493, train time: 40.94744563102722
epoch: 117, training loss: 110.10476988709888, train time: 42.83123588562012
epoch: 118, training loss: 109.2354747209065, train time: 40.82029104232788
epoch: 119, training loss: 108.75073316647467, train time: 41.64887452125549
epoch: 120, training loss: 109.57658892026666, train time: 43.62632441520691
epoch: 121, training loss: 109.76305825322197, train time: 46.116379261016846
epoch: 122, training loss: 110.13822946566142, train time: 44.99737524986267
epoch: 123, training loss: 108.14686186812469, train time: 41.1541702747345
epoch: 124, training loss: 109.54472543812153, train time: 41.49176073074341
epoch: 125, training loss: 108.71688339931461, train time: 49.11253213882446
epoch: 126, training loss: 108.43240377030452, train time: 42.74191474914551
epoch: 127, training loss: 109.88435605728591, train time: 44.92594122886658
epoch: 128, training loss: 109.67072959433062, train time: 44.78008842468262
epoch: 129, training loss: 111.06059148607528, train time: 45.0243399143219
epoch: 130, training loss: 108.46275230745232, train time: 49.540862798690796
epoch: 131, training loss: 108.22706334139366, train time: 41.81313705444336
epoch: 132, training loss: 107.25390541185152, train time: 41.26371693611145
epoch: 133, training loss: 108.27701548749974, train time: 46.31963396072388
epoch: 134, training loss: 108.35415372016723, train time: 41.25271821022034
epoch: 135, training loss: 108.71541522255757, train time: 46.17013120651245
epoch: 136, training loss: 108.39183104881522, train time: 41.364439487457275
epoch: 137, training loss: 109.61541101597322, train time: 41.499804973602295
epoch: 138, training loss: 108.54095025547576, train time: 42.01245307922363
epoch: 139, training loss: 109.07831421150695, train time: 43.40658926963806
epoch: 140, training loss: 108.13521280029818, train time: 42.60617113113403
epoch: 141, training loss: 108.45675894308806, train time: 44.419248819351196
epoch: 142, training loss: 108.68328275531894, train time: 41.78281760215759
epoch: 143, training loss: 108.22657537503255, train time: 46.11206364631653
epoch: 144, training loss: 108.41138396969473, train time: 47.53209590911865
epoch: 145, training loss: 108.73341768495084, train time: 42.30997157096863
epoch: 146, training loss: 107.27834870540391, train time: 42.23610281944275
epoch: 147, training loss: 108.22790508446633, train time: 42.245901107788086
epoch: 148, training loss: 109.10770919912466, train time: 42.10999298095703
epoch: 149, training loss: 108.75742303894731, train time: 42.06810235977173
epo:149|HR@1:0.4579 | HR@5:0.8382 | HR@10:0.9171 | HR@20:0.9615 | HR@50:0.9889 | NDCG@1:0.4037 | NDCG@5:0.4372 | NDCG@10:0.4649| NDCG@20:0.5079| NDCG@50:0.6102| best_HR@1:0.4710 | best_HR@5:0.8467 | best_HR@10:0.9259 | best_HR@20:0.9647 | best_HR@50:0.9930 | best_NDCG@1:0.4202 | best_NDCG@5:0.4529 | best_NDCG@10:0.4795 | best_NDCG@20:0.5215 | best_NDCG@50:0.6214 | train_time:42.07 | test_time:391.61
epoch: 150, training loss: 107.67196690171477, train time: 42.85340881347656
epoch: 151, training loss: 108.023346788661, train time: 47.8151581287384
epoch: 152, training loss: 108.10190961223634, train time: 48.907750368118286
epoch: 153, training loss: 106.2132734593215, train time: 45.61786103248596
epoch: 154, training loss: 106.23984719333384, train time: 43.882487058639526
epoch: 155, training loss: 108.37457987687594, train time: 44.108763456344604
epoch: 156, training loss: 106.28668819255472, train time: 42.116154193878174
epoch: 157, training loss: 106.88809352216049, train time: 43.76222610473633
epoch: 158, training loss: 106.7392212446066, train time: 42.77943301200867
epoch: 159, training loss: 105.82247892518353, train time: 41.897815227508545
epoch: 160, training loss: 107.74140960730074, train time: 49.37115979194641
epoch: 161, training loss: 108.40205510244414, train time: 48.08355474472046
epoch: 162, training loss: 108.86199320291416, train time: 44.02653431892395
epoch: 163, training loss: 105.96746700305812, train time: 48.10859036445618
epoch: 164, training loss: 106.69934735909192, train time: 41.971380949020386
Traceback (most recent call last):
  File "/home/ubuntu/model/PaperModel/model/recommendation_model.py", line 418, in <module>
    rec_net(train_loader, test_loader, node_emb, sequence_tensor)
  File "/home/ubuntu/model/PaperModel/model/recommendation_model.py", line 136, in rec_net
    for step, batch in enumerate(train_loader):
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 435, in __iter__
    return self._get_iterator()
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 381, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/utils/data/dataloader.py", line 1034, in __init__
    w.start()
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/multiprocessing/popen_fork.py", line 66, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
