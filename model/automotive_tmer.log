nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  1.0728836059570312e-05
user  100 time:  185.28584694862366
user  200 time:  370.6339237689972
user  300 time:  557.0721588134766
user  400 time:  744.696783542633
user  500 time:  930.6803455352783
user  600 time:  1119.915785074234
user  700 time:  1309.9340379238129
user  800 time:  1496.4155910015106
user  900 time:  1685.3420498371124
user  1000 time:  1873.5130109786987
user  1100 time:  2062.5856063365936
user  1200 time:  2251.8783729076385
user  1300 time:  2442.8307909965515
user  1400 time:  2630.456727027893
user  1500 time:  2819.0627641677856
user  1600 time:  3005.3596720695496
user  1700 time:  3192.7420403957367
user  1800 time:  3382.30833029747
user  1900 time:  3571.4793934822083
user  2000 time:  3759.12073469162
user  2100 time:  3945.07314991951
user  2200 time:  4133.066955566406
user  2300 time:  4324.344470977783
user  2400 time:  4513.0809326171875
user  2500 time:  4702.965957164764
user  2600 time:  4892.084748744965
user  2700 time:  5080.616292953491
user  2800 time:  5271.566292524338
user  2900 time:  5460.288494110107
user  3000 time:  5648.995977401733
user  3100 time:  5838.354612827301
user  3200 time:  6025.891325235367
user  3300 time:  6215.727345228195
user  3400 time:  6405.251892566681
user  3500 time:  6594.083877086639
user  3600 time:  6784.538416624069
user  3700 time:  6972.467662334442
user  3800 time:  7159.523647785187
user  3900 time:  7348.344721794128
user  4000 time:  7536.886764526367
user  4100 time:  7726.668493747711
user  4200 time:  7913.773273468018
user  4300 time:  8103.409583568573
user  4400 time:  8294.503856658936
user  4500 time:  8482.407503843307
start training item-item instance self attention module...
user  0 time:  4.5299530029296875e-06
user  100 time:  132.9328064918518
user  200 time:  268.0375759601593
user  300 time:  400.67873191833496
user  400 time:  534.3519651889801
user  500 time:  664.5261056423187
user  600 time:  794.4131746292114
user  700 time:  927.1969513893127
user  800 time:  1059.7761433124542
user  900 time:  1198.650945186615
user  1000 time:  1340.2507209777832
user  1100 time:  1478.2340025901794
user  1200 time:  1611.165849685669
user  1300 time:  1747.6765944957733
user  1400 time:  1873.064484834671
user  1500 time:  2008.8306806087494
user  1600 time:  2140.0861072540283
user  1700 time:  2265.9219839572906
user  1800 time:  2399.986188173294
user  1900 time:  2538.3352897167206
user  2000 time:  2671.7929985523224
user  2100 time:  2797.1552517414093
user  2200 time:  2915.915483236313
user  2300 time:  3035.101866006851
user  2400 time:  3159.737947702408
user  2500 time:  3283.9725890159607
user  2600 time:  3413.1579275131226
user  2700 time:  3545.3630962371826
user  2800 time:  3680.232993364334
user  2900 time:  3805.759190559387
user  3000 time:  3931.525284767151
user  3100 time:  4061.080345392227
user  3200 time:  4180.347330331802
user  3300 time:  4306.4703867435455
user  3400 time:  4435.3813672065735
user  3500 time:  4562.581743955612
user  3600 time:  4699.105507850647
user  3700 time:  4826.002630710602
user  3800 time:  4956.638308048248
user  3900 time:  5072.455796480179
user  4000 time:  5207.735963582993
user  4100 time:  5343.802939414978
user  4200 time:  5472.105229377747
user  4300 time:  5599.27676153183
user  4400 time:  5729.000855922699
user  4500 time:  5850.035051345825
start updating user and item embedding...
user_name:4600
user  0 time:  1.0013580322265625e-05
user  100 time:  15.971855401992798
user  200 time:  31.73459267616272
user  300 time:  47.28019952774048
user  400 time:  63.47720170021057
user  500 time:  79.20899677276611
user  600 time:  94.78840613365173
user  700 time:  110.62583708763123
user  800 time:  126.5387589931488
user  900 time:  142.25445771217346
user  1000 time:  158.11971521377563
user  1100 time:  173.7607033252716
user  1200 time:  189.59941005706787
user  1300 time:  205.30570316314697
user  1400 time:  220.85246896743774
user  1500 time:  236.542706489563
user  1600 time:  252.1598653793335
user  1700 time:  267.92093300819397
user  1800 time:  283.6613116264343
user  1900 time:  299.61427545547485
user  2000 time:  315.63483572006226
user  2100 time:  331.1034026145935
user  2200 time:  346.8724536895752
user  2300 time:  362.3044693470001
user  2400 time:  377.97632002830505
user  2500 time:  393.6280241012573
user  2600 time:  409.3194456100464
user  2700 time:  425.2292935848236
user  2800 time:  441.168833732605
user  2900 time:  456.6971855163574
user  3000 time:  472.44651556015015
user  3100 time:  488.5412130355835
user  3200 time:  504.2026958465576
user  3300 time:  519.7887964248657
user  3400 time:  535.2908134460449
user  3500 time:  550.6813111305237
user  3600 time:  566.5496151447296
user  3700 time:  582.4799070358276
user  3800 time:  598.0159754753113
user  3900 time:  613.3862397670746
user  4000 time:  629.1105532646179
user  4100 time:  644.8311750888824
user  4200 time:  660.7150511741638
user  4300 time:  676.3355441093445
user  4400 time:  692.2840960025787
user  4500 time:  708.1613140106201
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 288.60336115883547, train time: 247.94159054756165
epoch: 1, training loss: 182.5592570905137, train time: 246.31207823753357
epoch: 2, training loss: 169.25926307070768, train time: 249.0850567817688
epoch: 3, training loss: 160.9966691120353, train time: 247.16221380233765
epoch: 4, training loss: 155.83434888074407, train time: 247.91327905654907
epoch: 5, training loss: 150.59194128194213, train time: 246.71081161499023
epoch: 6, training loss: 146.31760284372285, train time: 246.9893560409546
epoch: 7, training loss: 142.5589514108142, train time: 248.32853055000305
epoch: 8, training loss: 138.9146982746388, train time: 247.35917735099792
epoch: 9, training loss: 136.8214944069041, train time: 249.63711428642273
epoch: 10, training loss: 132.2167217479073, train time: 245.34740948677063
epoch: 11, training loss: 130.54277748730965, train time: 247.87995743751526
epoch: 12, training loss: 127.93382135469437, train time: 247.53899240493774
epoch: 13, training loss: 125.85203205294238, train time: 246.06789898872375
epoch: 14, training loss: 124.98111644394521, train time: 246.6563799381256
epoch: 15, training loss: 123.4107402615773, train time: 245.36971282958984
epoch: 16, training loss: 122.31442174904805, train time: 246.00531721115112
epoch: 17, training loss: 120.22041801476007, train time: 263.50240874290466
epoch: 18, training loss: 118.51446797630342, train time: 262.8540985584259
epoch: 19, training loss: 118.65973613772076, train time: 264.4207515716553
epoch: 20, training loss: 113.76369045033789, train time: 262.27344369888306
epoch: 21, training loss: 114.77485853368125, train time: 263.6390018463135
epoch: 22, training loss: 113.04858613690158, train time: 262.1949100494385
epoch: 23, training loss: 112.78243395792379, train time: 264.16662645339966
epoch: 24, training loss: 110.97004465811187, train time: 261.998272895813
epoch: 25, training loss: 110.34191032057424, train time: 263.2475576400757
epoch: 26, training loss: 110.92653972900007, train time: 263.15457940101624
epoch: 27, training loss: 111.55664946383331, train time: 261.3892168998718
epoch: 28, training loss: 108.63758351644356, train time: 263.62020325660706
epoch: 29, training loss: 108.47965470083727, train time: 262.6106336116791
epo:29 | HR@5:0.9054 | HR@10:0.9193 | HR@20:0.9379 | NDCG@5:0.4206 | NDCG@10:0.4649 | NDCG@20:0.5193 | recall@5:0.6428 | recall@10:0.7547 | recall@20:0.7749 | precision@5:0.7714 | precision@10:0.4528 | precision@20:0.2325 | best_HR@5:0.9054 | best_HR@10:0.9193 | best_HR@20:0.9379 | best_NDCG@5:0.4206 | best_NDCG@10:0.4649 | best_NDCG@20:0.5193 | best_recall@5:0.6428 | best_recall@10:0.7547 | best_recall@20:0.7749 | best_precision@5:0.7714 | best_precision@10:0.4528 | best_precision@20:0.2325 | 
epoch: 30, training loss: 108.4069957833708, train time: 262.67404675483704
epoch: 31, training loss: 106.31618748942128, train time: 262.91165566444397
epoch: 32, training loss: 106.42962542782334, train time: 263.14412117004395
epoch: 33, training loss: 105.90110132066911, train time: 262.65222239494324
epoch: 34, training loss: 106.34231274935155, train time: 263.3964772224426
epoch: 35, training loss: 106.69202646221675, train time: 263.2198963165283
epoch: 36, training loss: 105.78991514819063, train time: 287.7401261329651
epoch: 37, training loss: 105.72232413289748, train time: 304.04411816596985
epoch: 38, training loss: 105.47042800115014, train time: 304.8520007133484
epoch: 39, training loss: 107.13062676552363, train time: 292.9597852230072
epoch: 40, training loss: 106.59551801030466, train time: 283.2223975658417
epoch: 41, training loss: 105.0601149403119, train time: 285.60080552101135
epoch: 42, training loss: 104.41174914742442, train time: 304.0534963607788
epoch: 43, training loss: 106.2259118425718, train time: 303.67439699172974
epoch: 44, training loss: 105.11780746716977, train time: 304.7945396900177
epoch: 45, training loss: 105.8744953218993, train time: 286.0518305301666
epoch: 46, training loss: 104.25626569036103, train time: 282.72228145599365
epoch: 47, training loss: 104.74081427253986, train time: 285.34312748908997
epoch: 48, training loss: 105.35657812900172, train time: 304.2390856742859
epoch: 49, training loss: 102.99315764908533, train time: 305.42243361473083
epoch: 50, training loss: 104.77554414185579, train time: 304.2859489917755
epoch: 51, training loss: 102.12578945673158, train time: 289.1414933204651
epoch: 52, training loss: 103.22852784043062, train time: 284.74913215637207
epoch: 53, training loss: 103.32993228542182, train time: 288.19885635375977
epoch: 54, training loss: 103.95113412719365, train time: 305.3532464504242
epoch: 55, training loss: 104.75667195423011, train time: 305.4854302406311
epoch: 56, training loss: 103.39378130494879, train time: 304.61561942100525
epoch: 57, training loss: 103.04292958827864, train time: 288.6301357746124
epoch: 58, training loss: 104.01325776477461, train time: 281.28003692626953
epoch: 59, training loss: 102.09447456376074, train time: 282.22718048095703
epo:59 | HR@5:0.8739 | HR@10:0.8927 | HR@20:0.9151 | NDCG@5:0.4350 | NDCG@10:0.4776 | NDCG@20:0.5303 | recall@5:0.6334 | recall@10:0.7302 | recall@20:0.7545 | precision@5:0.7600 | precision@10:0.4381 | precision@20:0.2263 | best_HR@5:0.9054 | best_HR@10:0.9193 | best_HR@20:0.9379 | best_NDCG@5:0.4350 | best_NDCG@10:0.4776 | best_NDCG@20:0.5303 | best_recall@5:0.6428 | best_recall@10:0.7547 | best_recall@20:0.7749 | best_precision@5:0.7714 | best_precision@10:0.4528 | best_precision@20:0.2325 | 
epoch: 60, training loss: 102.61383103723347, train time: 263.74656534194946
epoch: 61, training loss: 101.45401855255477, train time: 262.5551574230194
epoch: 62, training loss: 100.76378482378641, train time: 267.426397562027
epoch: 63, training loss: 99.39093457639683, train time: 282.114807844162
epoch: 64, training loss: 100.2466028249255, train time: 281.2690317630768
epoch: 65, training loss: 99.6327975828608, train time: 280.4218933582306
epoch: 66, training loss: 99.00081445497926, train time: 260.0130543708801
epoch: 67, training loss: 101.50724002460629, train time: 257.63408184051514
epoch: 68, training loss: 99.9732900611416, train time: 277.6534812450409
epoch: 69, training loss: 99.72807674729847, train time: 281.72273659706116
epoch: 70, training loss: 100.81859456602979, train time: 280.3709440231323
epoch: 71, training loss: 104.04894517310458, train time: 267.88677501678467
epoch: 72, training loss: 103.31644458085066, train time: 257.11848878860474
epoch: 73, training loss: 102.68295854091411, train time: 268.01809883117676
epoch: 74, training loss: 101.9437420122922, train time: 282.18816614151
epoch: 75, training loss: 99.90317307411897, train time: 281.64856719970703
epoch: 76, training loss: 101.64455805825855, train time: 277.98909878730774
epoch: 77, training loss: 102.7079675227069, train time: 257.6106221675873
epoch: 78, training loss: 104.89334047741431, train time: 258.63462829589844
epoch: 79, training loss: 103.85317959363601, train time: 282.6892602443695
epoch: 80, training loss: 102.92669856888097, train time: 281.0178482532501
epoch: 81, training loss: 103.01878711921017, train time: 281.0800049304962
epoch: 82, training loss: 104.37132865774038, train time: 262.70029497146606
epoch: 83, training loss: 104.40412344911601, train time: 257.5742337703705
epoch: 84, training loss: 105.7003724123133, train time: 272.9487042427063
epoch: 85, training loss: 102.93234426173876, train time: 281.7868161201477
epoch: 86, training loss: 101.15250352889416, train time: 282.05514788627625
epoch: 87, training loss: 99.76479082226433, train time: 273.6463634967804
epoch: 88, training loss: 98.67494951662957, train time: 257.8719873428345
epoch: 89, training loss: 97.3141104086244, train time: 254.51339673995972
epo:89 | HR@5:0.8674 | HR@10:0.8861 | HR@20:0.9095 | NDCG@5:0.4250 | NDCG@10:0.4678 | NDCG@20:0.5208 | recall@5:0.6295 | recall@10:0.7240 | recall@20:0.7490 | precision@5:0.7553 | precision@10:0.4344 | precision@20:0.2247 | best_HR@5:0.9054 | best_HR@10:0.9193 | best_HR@20:0.9379 | best_NDCG@5:0.4350 | best_NDCG@10:0.4776 | best_NDCG@20:0.5303 | best_recall@5:0.6428 | best_recall@10:0.7547 | best_recall@20:0.7749 | best_precision@5:0.7714 | best_precision@10:0.4528 | best_precision@20:0.2325 | 
epoch: 90, training loss: 95.35775238848873, train time: 246.0647258758545
epoch: 91, training loss: 94.7248249214972, train time: 245.80345487594604
epoch: 92, training loss: 95.98392451500695, train time: 246.2149908542633
epoch: 93, training loss: 95.69167767142062, train time: 246.169908285141
epoch: 94, training loss: 94.06344541136059, train time: 245.97415614128113
epoch: 95, training loss: 95.09985302574933, train time: 245.22100687026978
epoch: 96, training loss: 95.48617185289186, train time: 246.12317848205566
epoch: 97, training loss: 95.16851624922856, train time: 246.69942045211792
epoch: 98, training loss: 95.41068938409444, train time: 245.61558842658997
epoch: 99, training loss: 94.61947445679834, train time: 246.4067099094391
epoch: 100, training loss: 92.2723982393727, train time: 245.9251470565796
epoch: 101, training loss: 93.7422092017805, train time: 246.40216612815857
epoch: 102, training loss: 93.82944557538576, train time: 246.84837341308594
epoch: 103, training loss: 93.43834671606601, train time: 245.71406960487366
epoch: 104, training loss: 92.91019059847167, train time: 246.03250455856323
epoch: 105, training loss: 92.90156059586297, train time: 245.86652088165283
epoch: 106, training loss: 95.5615940073476, train time: 245.70262169837952
epoch: 107, training loss: 94.83271572754165, train time: 246.45412850379944
epoch: 108, training loss: 94.89152787277271, train time: 245.79739546775818
epoch: 109, training loss: 93.4123100658253, train time: 246.23216009140015
epoch: 110, training loss: 94.94784556171362, train time: 246.44271230697632
epoch: 111, training loss: 95.58307771559339, train time: 246.33089566230774
epoch: 112, training loss: 94.4064522734916, train time: 246.3437888622284
epoch: 113, training loss: 95.65376142257446, train time: 246.56473112106323
epoch: 114, training loss: 93.9537183326538, train time: 245.68925380706787
epoch: 115, training loss: 94.53063369947631, train time: 246.0703661441803
epoch: 116, training loss: 94.85351365939277, train time: 245.8775589466095
epoch: 117, training loss: 95.47730644523836, train time: 246.13614892959595
epoch: 118, training loss: 94.03603796493553, train time: 246.6329436302185
epoch: 119, training loss: 94.99571798963734, train time: 245.50634622573853
epo:119 | HR@5:0.8561 | HR@10:0.8738 | HR@20:0.8979 | NDCG@5:0.4220 | NDCG@10:0.4666 | NDCG@20:0.5206 | recall@5:0.6253 | recall@10:0.7161 | recall@20:0.7403 | precision@5:0.7503 | precision@10:0.4297 | precision@20:0.2221 | best_HR@5:0.9054 | best_HR@10:0.9193 | best_HR@20:0.9379 | best_NDCG@5:0.4350 | best_NDCG@10:0.4776 | best_NDCG@20:0.5303 | best_recall@5:0.6428 | best_recall@10:0.7547 | best_recall@20:0.7749 | best_precision@5:0.7714 | best_precision@10:0.4528 | best_precision@20:0.2325 | 
epoch: 120, training loss: 95.16155573900323, train time: 246.5830159187317
epoch: 121, training loss: 94.95954829997208, train time: 246.86958193778992
epoch: 122, training loss: 94.55805431994668, train time: 246.78030228614807
epoch: 123, training loss: 94.69050604296353, train time: 246.5248990058899
epoch: 124, training loss: 94.00756160922901, train time: 246.42375826835632
epoch: 125, training loss: 93.79405822910485, train time: 246.5310263633728
epoch: 126, training loss: 94.91820313764038, train time: 246.77913117408752
epoch: 127, training loss: 96.46375813673512, train time: 247.1387300491333
epoch: 128, training loss: 96.58372832917667, train time: 246.69932079315186
epoch: 129, training loss: 97.72312149153731, train time: 246.16198897361755
epoch: 130, training loss: 97.70975448632089, train time: 245.8085536956787
epoch: 131, training loss: 96.83713328044541, train time: 246.5740101337433
epoch: 132, training loss: 95.70330449479661, train time: 245.52691268920898
epoch: 133, training loss: 96.36815069997101, train time: 246.70296931266785
epoch: 134, training loss: 96.23077934820321, train time: 246.87697196006775
epoch: 135, training loss: 96.0492878102159, train time: 246.19091200828552
epoch: 136, training loss: 95.90688834113098, train time: 246.8558316230774
epoch: 137, training loss: 95.64249804081919, train time: 247.18170952796936
epoch: 138, training loss: 96.49044975576544, train time: 246.80220794677734
epoch: 139, training loss: 96.36728085268987, train time: 245.90565872192383
epoch: 140, training loss: 97.84678305318812, train time: 247.27711009979248
epoch: 141, training loss: 96.48743883212592, train time: 246.7926151752472
epoch: 142, training loss: 95.11568182582414, train time: 246.90876126289368
epoch: 143, training loss: 96.37091014848556, train time: 246.13796043395996
epoch: 144, training loss: 96.7145766886897, train time: 246.79250168800354
epoch: 145, training loss: 97.61491992952506, train time: 246.5688374042511
epoch: 146, training loss: 96.79400827404606, train time: 246.4241123199463
epoch: 147, training loss: 98.31916454071325, train time: 245.59056615829468
epoch: 148, training loss: 98.73351530264335, train time: 246.3906638622284
epoch: 149, training loss: 96.507072116794, train time: 246.54458904266357
epo:149 | HR@5:0.8614 | HR@10:0.8777 | HR@20:0.9011 | NDCG@5:0.4019 | NDCG@10:0.4473 | NDCG@20:0.5030 | recall@5:0.6246 | recall@10:0.7184 | recall@20:0.7417 | precision@5:0.7495 | precision@10:0.4310 | precision@20:0.2225 | best_HR@5:0.9054 | best_HR@10:0.9193 | best_HR@20:0.9379 | best_NDCG@5:0.4350 | best_NDCG@10:0.4776 | best_NDCG@20:0.5303 | best_recall@5:0.6428 | best_recall@10:0.7547 | best_recall@20:0.7749 | best_precision@5:0.7714 | best_precision@10:0.4528 | best_precision@20:0.2325 | 
training finish
