nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Grocery_Gourmet_Food......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.5762786865234375e-06
user  100 time:  5.406339168548584
user  200 time:  7.431548833847046
user  300 time:  10.877373456954956
user  400 time:  15.43614387512207
user  500 time:  21.68997049331665
user  600 time:  26.108877658843994
user  700 time:  29.019379138946533
user  800 time:  30.879432439804077
user  900 time:  33.11778974533081
user  1000 time:  37.21256875991821
user  1100 time:  44.67330837249756
user  1200 time:  45.10996103286743
user  1300 time:  50.0361692905426
user  1400 time:  51.66349196434021
user  1500 time:  53.41935110092163
user  1600 time:  58.182780742645264
user  1700 time:  63.131473779678345
user  1800 time:  70.06810450553894
user  1900 time:  72.77843427658081
start training item-item instance self attention module...
user  0 time:  3.337860107421875e-06
user  100 time:  91.42383861541748
user  200 time:  164.9537489414215
user  300 time:  240.54398322105408
user  400 time:  332.40236711502075
user  500 time:  408.95639538764954
user  600 time:  497.5974304676056
user  700 time:  586.1387264728546
user  800 time:  681.2255153656006
user  900 time:  771.2722992897034
user  1000 time:  854.0128645896912
user  1100 time:  938.6426200866699
user  1200 time:  1036.6219022274017
user  1300 time:  1137.7527050971985
user  1400 time:  1234.7017855644226
user  1500 time:  1327.4219245910645
user  1600 time:  1419.040288925171
user  1700 time:  1517.5025169849396
user  1800 time:  1607.2086849212646
user  1900 time:  1706.6013638973236
start updating user and item embedding...
user_name:2000
user  0 time:  1.430511474609375e-05
user  100 time:  19.046629905700684
user  200 time:  37.8498592376709
user  300 time:  56.8121554851532
user  400 time:  75.85734438896179
user  500 time:  94.74109101295471
user  600 time:  113.99869132041931
user  700 time:  132.88601398468018
user  800 time:  151.89668583869934
user  900 time:  171.00944590568542
user  1000 time:  190.01086688041687
user  1100 time:  208.97747826576233
user  1200 time:  227.91749548912048
user  1300 time:  246.85870623588562
user  1400 time:  265.97487020492554
user  1500 time:  284.89811420440674
user  1600 time:  304.08867478370667
user  1700 time:  323.10813665390015
user  1800 time:  342.3492760658264
user  1900 time:  361.4936192035675
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 428.83416896755807, train time: 586.537495136261
epoch: 1, training loss: 310.73433355020825, train time: 585.9339418411255
epoch: 2, training loss: 278.5334556242451, train time: 586.2330920696259
epoch: 3, training loss: 265.53457268164493, train time: 587.0458400249481
epoch: 4, training loss: 258.07100230496144, train time: 596.873824596405
epoch: 5, training loss: 250.44581984705292, train time: 598.7722635269165
epoch: 6, training loss: 245.37885451398324, train time: 596.4919633865356
epoch: 7, training loss: 239.98549277434358, train time: 592.5724291801453
epoch: 8, training loss: 235.85592781740706, train time: 594.2553949356079
epoch: 9, training loss: 231.20882977975998, train time: 593.4059000015259
epoch: 10, training loss: 227.97608331567608, train time: 592.4894464015961
epoch: 11, training loss: 224.60738161602058, train time: 592.128664970398
epoch: 12, training loss: 220.76467856421368, train time: 594.8406987190247
epoch: 13, training loss: 217.5971576438751, train time: 592.4306130409241
epoch: 14, training loss: 213.4035461335443, train time: 591.0934522151947
epoch: 15, training loss: 210.91881862381706, train time: 593.2149624824524
epoch: 16, training loss: 207.9911845875904, train time: 590.0377972126007
epoch: 17, training loss: 204.56578484049533, train time: 588.729523897171
epoch: 18, training loss: 202.7395026317099, train time: 591.1225202083588
epoch: 19, training loss: 198.4358264047769, train time: 580.74480676651
epoch: 20, training loss: 196.9024275666452, train time: 570.0434219837189
epoch: 21, training loss: 194.15229720762, train time: 590.2798464298248
epoch: 22, training loss: 191.50382158832508, train time: 588.1171941757202
epoch: 23, training loss: 188.93640502740163, train time: 586.8930666446686
epoch: 24, training loss: 186.3752965402673, train time: 586.0713200569153
epoch: 25, training loss: 184.66770828084555, train time: 586.8854348659515
epoch: 26, training loss: 181.72629760907148, train time: 590.9177660942078
epoch: 27, training loss: 179.5668334038055, train time: 588.2771980762482
epoch: 28, training loss: 177.07168711160193, train time: 589.7347874641418
epoch: 29, training loss: 175.90456033934606, train time: 587.8207566738129
epo:29 | HR@5:0.7352 | HR@10:0.8514 | HR@20:0.9345 | NDCG@5:0.4505 | NDCG@10:0.4797 | NDCG@20:0.5240 | recall@5:0.4487 | recall@10:0.6433 | recall@20:0.7587 | precision@5:0.5384 | precision@10:0.3860 | precision@20:0.2276 | best_HR@5:0.7352 | best_HR@10:0.8514 | best_HR@20:0.9345 | best_NDCG@5:0.4505 | best_NDCG@10:0.4797 | best_NDCG@20:0.5240 | best_recall@5:0.4487 | best_recall@10:0.6433 | best_recall@20:0.7587 | best_precision@5:0.5384 | best_precision@10:0.3860 | best_precision@20:0.2276 | 
epoch: 30, training loss: 173.15189328565612, train time: 588.9804484844208
epoch: 31, training loss: 171.84024956120993, train time: 591.2264637947083
epoch: 32, training loss: 169.83769524015952, train time: 588.3546793460846
epoch: 33, training loss: 167.51314050145447, train time: 580.5357000827789
epoch: 34, training loss: 166.58007064732374, train time: 587.5944352149963
epoch: 35, training loss: 164.1840212343668, train time: 591.0072729587555
epoch: 36, training loss: 162.72839232924161, train time: 590.2956948280334
epoch: 37, training loss: 160.54179976697196, train time: 592.1328439712524
epoch: 38, training loss: 159.59811482427176, train time: 588.1418101787567
epoch: 39, training loss: 158.1291170661716, train time: 589.1246755123138
epoch: 40, training loss: 155.9790701908496, train time: 588.9752414226532
epoch: 41, training loss: 155.2126097859873, train time: 576.0136170387268
epoch: 42, training loss: 151.97793748087133, train time: 588.4264125823975
epoch: 43, training loss: 152.35627026109432, train time: 588.9078099727631
epoch: 44, training loss: 151.8284238572087, train time: 588.819908618927
epoch: 45, training loss: 149.6700226195244, train time: 588.9253542423248
epoch: 46, training loss: 147.46508316422114, train time: 587.4077236652374
epoch: 47, training loss: 147.41444798790326, train time: 590.1807284355164
epoch: 48, training loss: 145.65322539191402, train time: 586.9816789627075
epoch: 49, training loss: 143.80141982410714, train time: 585.7444975376129
epoch: 50, training loss: 142.60656415809353, train time: 582.5042834281921
epoch: 51, training loss: 143.76747348437493, train time: 582.3973517417908
epoch: 52, training loss: 141.87016876386042, train time: 581.9255180358887
epoch: 53, training loss: 139.9992016390097, train time: 583.4902470111847
epoch: 54, training loss: 139.16997949263896, train time: 581.3316676616669
epoch: 55, training loss: 137.86457801240613, train time: 582.2055044174194
epoch: 56, training loss: 137.51621243585396, train time: 579.8653988838196
epoch: 57, training loss: 135.89506995324336, train time: 581.0155646800995
epoch: 58, training loss: 135.3731220415284, train time: 580.7768716812134
epoch: 59, training loss: 134.85493860889983, train time: 579.4171721935272
epo:59 | HR@5:0.7165 | HR@10:0.8323 | HR@20:0.9203 | NDCG@5:0.4760 | NDCG@10:0.5057 | NDCG@20:0.5494 | recall@5:0.4477 | recall@10:0.6312 | recall@20:0.7445 | precision@5:0.5373 | precision@10:0.3787 | precision@20:0.2233 | best_HR@5:0.7352 | best_HR@10:0.8514 | best_HR@20:0.9345 | best_NDCG@5:0.4760 | best_NDCG@10:0.5057 | best_NDCG@20:0.5494 | best_recall@5:0.4487 | best_recall@10:0.6433 | best_recall@20:0.7587 | best_precision@5:0.5384 | best_precision@10:0.3860 | best_precision@20:0.2276 | 
epoch: 60, training loss: 133.04203859138943, train time: 583.9881455898285
epoch: 61, training loss: 132.9801949797402, train time: 579.9227755069733
epoch: 62, training loss: 131.47225077421172, train time: 577.7903764247894
epoch: 63, training loss: 130.26272217014048, train time: 538.0872435569763
epoch: 64, training loss: 129.52865304808074, train time: 555.5268659591675
epoch: 65, training loss: 129.19097238474933, train time: 580.7171998023987
epoch: 66, training loss: 127.94304625861696, train time: 576.7247910499573
epoch: 67, training loss: 126.58814197099127, train time: 581.0614957809448
epoch: 68, training loss: 126.56292904734437, train time: 582.0027143955231
epoch: 69, training loss: 126.20977825498994, train time: 579.8536705970764
epoch: 70, training loss: 125.69668232919503, train time: 578.9108526706696
epoch: 71, training loss: 124.7620578002934, train time: 580.1823947429657
epoch: 72, training loss: 123.36881290307065, train time: 578.9212021827698
epoch: 73, training loss: 123.70181599569332, train time: 579.8553712368011
epoch: 74, training loss: 123.1796830031235, train time: 581.3348479270935
epoch: 75, training loss: 121.54763845221896, train time: 581.4924805164337
epoch: 76, training loss: 121.35749997007588, train time: 579.4256181716919
epoch: 77, training loss: 120.58739939564111, train time: 578.9313638210297
epoch: 78, training loss: 119.11625563722919, train time: 580.5711703300476
epoch: 79, training loss: 119.00364324000839, train time: 578.2906069755554
epoch: 80, training loss: 118.54255484651367, train time: 576.2776458263397
epoch: 81, training loss: 117.75904829834326, train time: 579.2656016349792
epoch: 82, training loss: 117.81911242925707, train time: 579.2731893062592
epoch: 83, training loss: 117.86529453566618, train time: 578.1861138343811
epoch: 84, training loss: 116.77525606441486, train time: 582.1319406032562
epoch: 85, training loss: 116.66320860088308, train time: 575.004275560379
epoch: 86, training loss: 114.08011363202968, train time: 556.6158993244171
epoch: 87, training loss: 113.79190325282252, train time: 574.9781725406647
epoch: 88, training loss: 113.73617226498027, train time: 576.7048058509827
epoch: 89, training loss: 112.5504696142234, train time: 579.2886307239532
epo:89 | HR@5:0.7041 | HR@10:0.8150 | HR@20:0.9093 | NDCG@5:0.4873 | NDCG@10:0.5162 | NDCG@20:0.5589 | recall@5:0.4395 | recall@10:0.6202 | recall@20:0.7322 | precision@5:0.5274 | precision@10:0.3721 | precision@20:0.2196 | best_HR@5:0.7352 | best_HR@10:0.8514 | best_HR@20:0.9345 | best_NDCG@5:0.4873 | best_NDCG@10:0.5162 | best_NDCG@20:0.5589 | best_recall@5:0.4487 | best_recall@10:0.6433 | best_recall@20:0.7587 | best_precision@5:0.5384 | best_precision@10:0.3860 | best_precision@20:0.2276 | 
epoch: 90, training loss: 113.21762239778764, train time: 578.7147922515869
epoch: 91, training loss: 112.87471745579387, train time: 580.3198285102844
epoch: 92, training loss: 112.6263094265887, train time: 578.7245383262634
epoch: 93, training loss: 111.37360484942474, train time: 581.6725771427155
epoch: 94, training loss: 111.52670571656199, train time: 573.5468034744263
epoch: 95, training loss: 110.50309380469844, train time: 583.5932784080505
epoch: 96, training loss: 110.69514760361926, train time: 581.8196585178375
epoch: 97, training loss: 109.56371453127576, train time: 584.5534179210663
epoch: 98, training loss: 109.71953880663568, train time: 587.4612486362457
epoch: 99, training loss: 107.93367100179603, train time: 589.211433172226
epoch: 100, training loss: 108.92136997242415, train time: 590.5507168769836
epoch: 101, training loss: 108.15655529382275, train time: 588.4802768230438
epoch: 102, training loss: 107.63946701597888, train time: 588.5613281726837
epoch: 103, training loss: 106.76649902575446, train time: 589.001665353775
epoch: 104, training loss: 104.99701305476992, train time: 589.2228302955627
epoch: 105, training loss: 106.65386600610509, train time: 589.8771412372589
epoch: 106, training loss: 105.84383620859444, train time: 591.1803481578827
epoch: 107, training loss: 106.63685319334036, train time: 577.8640100955963
epoch: 108, training loss: 105.32602568127186, train time: 568.479651927948
epoch: 109, training loss: 103.71619560020554, train time: 526.7716391086578
epoch: 110, training loss: 106.00038152416164, train time: 530.6345398426056
epoch: 111, training loss: 103.62221381765266, train time: 532.9626047611237
epoch: 112, training loss: 104.69047919210243, train time: 531.3900120258331
epoch: 113, training loss: 104.59033972862744, train time: 531.7288353443146
epoch: 114, training loss: 103.03833978841794, train time: 534.7124483585358
epoch: 115, training loss: 102.28162675219937, train time: 532.9283854961395
epoch: 116, training loss: 102.80017572284123, train time: 536.5799899101257
epoch: 117, training loss: 101.22633529790619, train time: 534.8502666950226
epoch: 118, training loss: 103.42868227481813, train time: 532.4404430389404
epoch: 119, training loss: 101.54841745114754, train time: 533.3816120624542
epo:119 | HR@5:0.6830 | HR@10:0.7990 | HR@20:0.8948 | NDCG@5:0.4925 | NDCG@10:0.5213 | NDCG@20:0.5635 | recall@5:0.4266 | recall@10:0.6010 | recall@20:0.7200 | precision@5:0.5119 | precision@10:0.3606 | precision@20:0.2160 | best_HR@5:0.7352 | best_HR@10:0.8514 | best_HR@20:0.9345 | best_NDCG@5:0.4925 | best_NDCG@10:0.5213 | best_NDCG@20:0.5635 | best_recall@5:0.4487 | best_recall@10:0.6433 | best_recall@20:0.7587 | best_precision@5:0.5384 | best_precision@10:0.3860 | best_precision@20:0.2276 | 
epoch: 120, training loss: 99.9012604994241, train time: 535.9553329944611
epoch: 121, training loss: 101.5160173676577, train time: 530.8460893630981
epoch: 122, training loss: 100.88093068642775, train time: 531.2141199111938
epoch: 123, training loss: 100.51590099712485, train time: 525.2838745117188
epoch: 124, training loss: 99.43645575470691, train time: 509.1766471862793
epoch: 125, training loss: 99.60453972203868, train time: 536.9960119724274
epoch: 126, training loss: 99.11292433904964, train time: 534.7485303878784
epoch: 127, training loss: 99.00310082919532, train time: 533.568731546402
epoch: 128, training loss: 97.58815866338955, train time: 535.8038640022278
epoch: 129, training loss: 98.69572038922706, train time: 533.7185754776001
epoch: 130, training loss: 98.09277622208538, train time: 535.0626792907715
epoch: 131, training loss: 98.19873863307748, train time: 534.2285830974579
epoch: 132, training loss: 98.08327974008353, train time: 526.7434544563293
epoch: 133, training loss: 98.14701460530523, train time: 523.4770021438599
epoch: 134, training loss: 96.94712060485836, train time: 520.5532188415527
epoch: 135, training loss: 97.69869922145335, train time: 522.5083966255188
epoch: 136, training loss: 97.17450228923917, train time: 519.7853684425354
epoch: 137, training loss: 96.44997475477885, train time: 520.4685227870941
epoch: 138, training loss: 97.12169576445376, train time: 521.4495010375977
epoch: 139, training loss: 96.21262392178141, train time: 521.913149356842
epoch: 140, training loss: 96.22794073705973, train time: 522.864857673645
epoch: 141, training loss: 96.14046800815049, train time: 523.8542115688324
epoch: 142, training loss: 96.52511743388823, train time: 525.9942765235901
epoch: 143, training loss: 94.5481242812557, train time: 523.6013488769531
epoch: 144, training loss: 94.75049720141033, train time: 526.7446472644806
epoch: 145, training loss: 96.24678970798777, train time: 529.9982252120972
epoch: 146, training loss: 94.62536348671347, train time: 530.4594209194183
epoch: 147, training loss: 94.1255356657748, train time: 534.3921525478363
epoch: 148, training loss: 92.12417593519694, train time: 530.2649354934692
epoch: 149, training loss: 93.32473721882343, train time: 531.9100642204285
epo:149 | HR@5:0.6857 | HR@10:0.7997 | HR@20:0.8933 | NDCG@5:0.4889 | NDCG@10:0.5184 | NDCG@20:0.5615 | recall@5:0.4312 | recall@10:0.6020 | recall@20:0.7163 | precision@5:0.5175 | precision@10:0.3612 | precision@20:0.2149 | best_HR@5:0.7352 | best_HR@10:0.8514 | best_HR@20:0.9345 | best_NDCG@5:0.4925 | best_NDCG@10:0.5213 | best_NDCG@20:0.5635 | best_recall@5:0.4487 | best_recall@10:0.6433 | best_recall@20:0.7587 | best_precision@5:0.5384 | best_precision@10:0.3860 | best_precision@20:0.2276 | 
training finish
