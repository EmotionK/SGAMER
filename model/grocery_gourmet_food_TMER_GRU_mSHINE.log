nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Grocery_Gourmet_Food......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.5299530029296875e-06
user  100 time:  3.409585952758789
user  200 time:  5.442721128463745
user  300 time:  8.585958003997803
user  400 time:  9.698069095611572
user  500 time:  10.070375919342041
user  600 time:  10.444931745529175
user  700 time:  10.446856498718262
user  800 time:  12.861678838729858
user  900 time:  14.347668409347534
user  1000 time:  15.279081583023071
user  1100 time:  17.87690281867981
user  1200 time:  19.54828119277954
user  1300 time:  21.03502082824707
user  1400 time:  22.895219802856445
user  1500 time:  25.31791090965271
user  1600 time:  26.62418842315674
user  1700 time:  29.0432071685791
user  1800 time:  29.604092359542847
user  1900 time:  30.53650450706482
start training item-item instance self attention module...
user  0 time:  4.291534423828125e-06
user  100 time:  77.7216420173645
user  200 time:  142.24532318115234
user  300 time:  210.30262112617493
user  400 time:  281.0089342594147
user  500 time:  354.9020645618439
user  600 time:  419.7749330997467
user  700 time:  496.1492030620575
user  800 time:  568.6963005065918
user  900 time:  640.7532863616943
user  1000 time:  713.4707641601562
user  1100 time:  785.3797585964203
user  1200 time:  855.4827885627747
user  1300 time:  939.2763137817383
user  1400 time:  1017.9294941425323
user  1500 time:  1095.1049120426178
user  1600 time:  1169.2064397335052
user  1700 time:  1248.7353522777557
user  1800 time:  1323.216941356659
user  1900 time:  1401.3927233219147
start updating user and item embedding...
user_name:2000
user  0 time:  1.7881393432617188e-05
user  100 time:  13.628135681152344
user  200 time:  27.140767335891724
user  300 time:  40.828632831573486
user  400 time:  54.33087873458862
user  500 time:  67.89165902137756
user  600 time:  81.35668992996216
user  700 time:  94.88342952728271
user  800 time:  108.28909873962402
user  900 time:  121.80160593986511
user  1000 time:  135.49148154258728
user  1100 time:  149.1102650165558
user  1200 time:  162.5778477191925
user  1300 time:  176.16088676452637
user  1400 time:  189.75182509422302
user  1500 time:  203.3387565612793
user  1600 time:  216.89828085899353
user  1700 time:  230.55652785301208
user  1800 time:  244.11365914344788
user  1900 time:  257.8819456100464
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:57: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 444.738549713511, train time: 368.32005739212036
epoch: 1, training loss: 419.6955376979895, train time: 368.59074306488037
epoch: 2, training loss: 347.1437464596238, train time: 367.85409808158875
epoch: 3, training loss: 291.12769117741846, train time: 367.3982355594635
epoch: 4, training loss: 270.91451261751354, train time: 367.264093875885
epoch: 5, training loss: 257.3388042594306, train time: 366.3417100906372
epoch: 6, training loss: 249.75295427325182, train time: 366.50094294548035
epoch: 7, training loss: 242.04260367364623, train time: 365.7542679309845
epoch: 8, training loss: 237.0960291849915, train time: 365.83130836486816
epoch: 9, training loss: 232.4504542746581, train time: 364.4394335746765
epoch: 10, training loss: 228.55394390039146, train time: 363.3714048862457
epoch: 11, training loss: 225.3306225243723, train time: 362.91897320747375
epoch: 12, training loss: 222.31784437422175, train time: 362.77226543426514
epoch: 13, training loss: 219.55154990032315, train time: 363.1003246307373
epoch: 14, training loss: 214.95970728155226, train time: 362.9210751056671
epoch: 15, training loss: 212.26020141073968, train time: 363.5814287662506
epoch: 16, training loss: 210.81743040285073, train time: 362.74852561950684
epoch: 17, training loss: 209.22434721619356, train time: 363.5651903152466
epoch: 18, training loss: 207.09572613390628, train time: 364.2644302845001
epoch: 19, training loss: 203.11430634651333, train time: 364.75422859191895
epoch: 20, training loss: 201.13540871359874, train time: 364.00220131874084
epoch: 21, training loss: 199.20093834667932, train time: 364.6610515117645
epoch: 22, training loss: 197.24562351306668, train time: 366.4160580635071
epoch: 23, training loss: 195.665558845154, train time: 365.8473870754242
epoch: 24, training loss: 192.82608791952953, train time: 365.8073842525482
epoch: 25, training loss: 188.67213771003298, train time: 366.2389256954193
epoch: 26, training loss: 189.0589662839775, train time: 365.867094039917
epoch: 27, training loss: 187.85465934639797, train time: 366.7109327316284
epoch: 28, training loss: 185.19689628714696, train time: 368.0960714817047
epoch: 29, training loss: 182.06763856788166, train time: 367.8986933231354
epo:29 | HR@5:0.7653 | HR@10:0.8764 | HR@20:0.9474 | NDCG@5:0.5179 | NDCG@10:0.5397 | NDCG@20:0.5718 | recall@5:0.4493 | recall@10:0.6649 | recall@20:0.7712 | precision@5:0.5392 | precision@10:0.3989 | precision@20:0.2314 | best_HR@5:0.7653 | best_HR@10:0.8764 | best_HR@20:0.9474 | best_NDCG@5:0.5179 | best_NDCG@10:0.5397 | best_NDCG@20:0.5718 | best_recall@5:0.4493 | best_recall@10:0.6649 | best_recall@20:0.7712 | best_precision@5:0.5392 | best_precision@10:0.3989 | best_precision@20:0.2314 | 
epoch: 30, training loss: 181.99714806018164, train time: 368.39294815063477
epoch: 31, training loss: 178.4756177777308, train time: 368.4480850696564
epoch: 32, training loss: 178.24517031566938, train time: 369.40122389793396
epoch: 33, training loss: 174.28011029202025, train time: 368.27547693252563
epoch: 34, training loss: 174.75123832182726, train time: 368.49290132522583
epoch: 35, training loss: 172.64395605621394, train time: 368.3637282848358
epoch: 36, training loss: 170.8944512839662, train time: 368.2810606956482
epoch: 37, training loss: 170.20697392948205, train time: 368.4063153266907
epoch: 38, training loss: 165.9562213429599, train time: 368.07970786094666
epoch: 39, training loss: 165.30865844746586, train time: 368.3915750980377
epoch: 40, training loss: 164.10975578514626, train time: 369.2949757575989
epoch: 41, training loss: 160.9073917895439, train time: 367.75239992141724
epoch: 42, training loss: 159.8085957953008, train time: 368.65393471717834
epoch: 43, training loss: 157.37692682092893, train time: 368.4145550727844
epoch: 44, training loss: 155.5966413183487, train time: 369.0936098098755
epoch: 45, training loss: 154.26582112591132, train time: 368.85040068626404
epoch: 46, training loss: 154.45320542072295, train time: 369.52767276763916
epoch: 47, training loss: 152.96588815251016, train time: 369.02777004241943
epoch: 48, training loss: 151.3186021807196, train time: 368.7384605407715
epoch: 49, training loss: 149.41566337936092, train time: 368.733521938324
epoch: 50, training loss: 147.3259995306289, train time: 368.42092275619507
epoch: 51, training loss: 147.17910977915744, train time: 368.97918605804443
epoch: 52, training loss: 145.42332274543878, train time: 368.9215877056122
epoch: 53, training loss: 145.0951091577881, train time: 369.6744964122772
epoch: 54, training loss: 142.99627040825726, train time: 368.87621903419495
epoch: 55, training loss: 141.33126329132938, train time: 368.60352516174316
epoch: 56, training loss: 141.13066332790186, train time: 369.74385166168213
epoch: 57, training loss: 139.14982955043524, train time: 369.27569484710693
epoch: 58, training loss: 138.42209660552908, train time: 370.43407940864563
epoch: 59, training loss: 136.7720037096442, train time: 367.75389218330383
epo:59 | HR@5:0.7608 | HR@10:0.8748 | HR@20:0.9502 | NDCG@5:0.5405 | NDCG@10:0.5618 | NDCG@20:0.5935 | recall@5:0.4527 | recall@10:0.6639 | recall@20:0.7707 | precision@5:0.5433 | precision@10:0.3983 | precision@20:0.2312 | best_HR@5:0.7653 | best_HR@10:0.8764 | best_HR@20:0.9502 | best_NDCG@5:0.5405 | best_NDCG@10:0.5618 | best_NDCG@20:0.5935 | best_recall@5:0.4527 | best_recall@10:0.6649 | best_recall@20:0.7712 | best_precision@5:0.5433 | best_precision@10:0.3989 | best_precision@20:0.2314 | 
epoch: 60, training loss: 135.96818844361405, train time: 391.09759044647217
epoch: 61, training loss: 134.9479063929466, train time: 377.78874683380127
epoch: 62, training loss: 133.66398761971504, train time: 366.73537373542786
epoch: 63, training loss: 132.3277549819759, train time: 372.7670888900757
epoch: 64, training loss: 131.9205840974755, train time: 371.8842957019806
epoch: 65, training loss: 129.07566008159483, train time: 365.86297369003296
epoch: 66, training loss: 129.95749342757335, train time: 381.3689851760864
epoch: 67, training loss: 128.3558755629856, train time: 406.14477252960205
epoch: 68, training loss: 127.93730694959231, train time: 404.44989466667175
epoch: 69, training loss: 124.74833867761481, train time: 406.101420879364
epoch: 70, training loss: 123.72459902451374, train time: 405.09848260879517
epoch: 71, training loss: 123.72710127068422, train time: 404.6187708377838
epoch: 72, training loss: 122.1696965211595, train time: 407.86156392097473
epoch: 73, training loss: 122.7903114928049, train time: 406.95195150375366
epoch: 74, training loss: 121.27747134430683, train time: 406.51816964149475
epoch: 75, training loss: 119.67177230604284, train time: 405.90445017814636
epoch: 76, training loss: 119.21431456060964, train time: 407.4853570461273
epoch: 77, training loss: 117.89564167343633, train time: 406.26906657218933
epoch: 78, training loss: 117.5317601441784, train time: 406.95503067970276
epoch: 79, training loss: 115.31290179873031, train time: 407.99876713752747
epoch: 80, training loss: 115.32792284546667, train time: 406.18722105026245
epoch: 81, training loss: 114.3820159922252, train time: 426.5646343231201
epoch: 82, training loss: 113.24503087969788, train time: 459.24542236328125
epoch: 83, training loss: 112.60279202643142, train time: 459.7181010246277
epoch: 84, training loss: 111.83169915480539, train time: 459.83136677742004
epoch: 85, training loss: 112.08125476237728, train time: 452.834445476532
epoch: 86, training loss: 108.58916664702701, train time: 383.0078318119049
epoch: 87, training loss: 108.73678694468617, train time: 408.9779386520386
