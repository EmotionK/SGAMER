nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.0531158447265625e-06
user  100 time:  220.20143342018127
user  200 time:  441.02343559265137
user  300 time:  661.7654678821564
user  400 time:  888.9873654842377
user  500 time:  1110.030192375183
user  600 time:  1327.9107937812805
user  700 time:  1546.7626509666443
user  800 time:  1766.3010947704315
user  900 time:  1987.9177603721619
user  1000 time:  2209.4585704803467
user  1100 time:  2428.0178389549255
user  1200 time:  2648.7103374004364
user  1300 time:  2878.621114730835
user  1400 time:  3097.888814687729
start training item-item instance self attention module...
user  0 time:  7.152557373046875e-06
user  100 time:  164.68841886520386
user  200 time:  328.6911449432373
user  300 time:  495.58989119529724
user  400 time:  664.3746566772461
user  500 time:  831.1002867221832
user  600 time:  983.0535273551941
user  700 time:  1155.1550664901733
user  800 time:  1312.6460745334625
user  900 time:  1473.9544863700867
user  1000 time:  1626.419248342514
user  1100 time:  1785.3847205638885
user  1200 time:  1951.4296419620514
user  1300 time:  2125.4685246944427
user  1400 time:  2294.530169248581
start updating user and item embedding...
user_name:1450
user  0 time:  1.5497207641601562e-05
user  100 time:  56.501142740249634
user  200 time:  112.90209174156189
user  300 time:  169.05545353889465
user  400 time:  224.2035858631134
user  500 time:  280.1462755203247
user  600 time:  335.2198820114136
user  700 time:  391.4964506626129
user  800 time:  447.15821504592896
user  900 time:  502.17924094200134
user  1000 time:  557.7281868457794
user  1100 time:  612.6789593696594
user  1200 time:  668.4251048564911
user  1300 time:  724.0025033950806
user  1400 time:  779.6561243534088
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 149.59368184802588, train time: 34.120659828186035
epoch: 1, training loss: 87.80585645552492, train time: 33.82764458656311
epoch: 2, training loss: 73.63408827902458, train time: 34.3263304233551
epoch: 3, training loss: 66.20516792838316, train time: 33.94818592071533
epoch: 4, training loss: 60.816302926024946, train time: 34.075589418411255
epoch: 5, training loss: 56.82839712058194, train time: 33.80545687675476
epoch: 6, training loss: 53.55613637289207, train time: 34.57620930671692
epoch: 7, training loss: 50.115431640260795, train time: 33.509297132492065
epoch: 8, training loss: 47.8980255910501, train time: 33.72030854225159
epoch: 9, training loss: 45.736532914525014, train time: 33.72994303703308
epoch: 10, training loss: 43.99822827027674, train time: 34.193958044052124
epoch: 11, training loss: 41.88108121643745, train time: 33.69896674156189
epoch: 12, training loss: 40.56582979324594, train time: 34.04659867286682
epoch: 13, training loss: 38.09919976093806, train time: 33.65303897857666
epoch: 14, training loss: 37.06717070663581, train time: 33.727298736572266
epoch: 15, training loss: 34.88732907031954, train time: 34.615198850631714
epoch: 16, training loss: 33.519606434256275, train time: 33.629048347473145
epoch: 17, training loss: 30.6952849838126, train time: 33.822587728500366
epoch: 18, training loss: 30.758453706446744, train time: 33.7305793762207
epoch: 19, training loss: 28.110271270204976, train time: 33.980767250061035
epoch: 20, training loss: 27.500711186490662, train time: 34.23272228240967
epoch: 21, training loss: 26.310537802415638, train time: 33.76306748390198
epoch: 22, training loss: 24.628253697406763, train time: 34.65183138847351
epoch: 23, training loss: 23.600435187230687, train time: 34.27666187286377
epoch: 24, training loss: 23.13354400530443, train time: 34.093639612197876
epoch: 25, training loss: 21.007327739680477, train time: 34.46029591560364
epoch: 26, training loss: 21.134505975505817, train time: 34.087157249450684
epoch: 27, training loss: 20.076173522868885, train time: 33.398555755615234
epoch: 28, training loss: 19.744638127623148, train time: 33.34819412231445
epoch: 29, training loss: 18.787465013269866, train time: 33.77384424209595
epo:29 | HR@5:0.7968 | HR@10:0.8313 | HR@20:0.8720 | NDCG@5:0.4426 | NDCG@10:0.4835 | NDCG@20:0.5348 | recall@5:0.5617 | recall@10:0.6668 | recall@20:0.7118 | precision@5:0.6741 | precision@10:0.4001 | precision@20:0.2136 | best_HR@5:0.7968 | best_HR@10:0.8313 | best_HR@20:0.8720 | best_NDCG@5:0.4426 | best_NDCG@10:0.4835 | best_NDCG@20:0.5348 | best_recall@5:0.5617 | best_recall@10:0.6668 | best_recall@20:0.7118 | best_precision@5:0.6741 | best_precision@10:0.4001 | best_precision@20:0.2136 | 
epoch: 30, training loss: 18.93366461719961, train time: 34.13833570480347
epoch: 31, training loss: 18.081278390228817, train time: 34.027175188064575
epoch: 32, training loss: 17.907941425964054, train time: 33.61407971382141
epoch: 33, training loss: 16.928353433231678, train time: 33.83423829078674
epoch: 34, training loss: 16.43511603084653, train time: 33.30136442184448
epoch: 35, training loss: 15.831247476829503, train time: 34.2724506855011
epoch: 36, training loss: 15.757238312356549, train time: 33.86135959625244
epoch: 37, training loss: 15.645716779654776, train time: 33.78820753097534
epoch: 38, training loss: 14.924597484523474, train time: 33.90002799034119
epoch: 39, training loss: 15.024020951342436, train time: 33.85865664482117
epoch: 40, training loss: 15.54117076214834, train time: 33.533337354660034
epoch: 41, training loss: 16.123521046016776, train time: 34.29324293136597
epoch: 42, training loss: 13.96007295380241, train time: 34.16155028343201
epoch: 43, training loss: 14.065369172764804, train time: 33.810213804244995
epoch: 44, training loss: 14.087722336663319, train time: 33.995630741119385
epoch: 45, training loss: 12.953791708329845, train time: 34.37602877616882
epoch: 46, training loss: 14.238236768544994, train time: 33.30811786651611
epoch: 47, training loss: 13.887053296325576, train time: 33.54103636741638
epoch: 48, training loss: 13.782020996042093, train time: 33.92852735519409
epoch: 49, training loss: 13.553380888213724, train time: 33.68984794616699
epoch: 50, training loss: 13.035117533795301, train time: 34.38084936141968
epoch: 51, training loss: 12.696515009039786, train time: 33.96166157722473
epoch: 52, training loss: 12.75474253479797, train time: 33.63977003097534
epoch: 53, training loss: 12.619895264998718, train time: 33.847078800201416
epoch: 54, training loss: 12.445261547531572, train time: 33.76357340812683
epoch: 55, training loss: 12.69176682451939, train time: 32.95726990699768
epoch: 56, training loss: 11.950510251959713, train time: 34.1831910610199
epoch: 57, training loss: 11.728883756047253, train time: 33.67461109161377
epoch: 58, training loss: 12.44779130175425, train time: 33.747087717056274
epoch: 59, training loss: 12.11333843692637, train time: 33.76631259918213
epo:59 | HR@5:0.7689 | HR@10:0.8037 | HR@20:0.8505 | NDCG@5:0.4753 | NDCG@10:0.5115 | NDCG@20:0.5582 | recall@5:0.5517 | recall@10:0.6467 | recall@20:0.6933 | precision@5:0.6621 | precision@10:0.3880 | precision@20:0.2080 | best_HR@5:0.7968 | best_HR@10:0.8313 | best_HR@20:0.8720 | best_NDCG@5:0.4753 | best_NDCG@10:0.5115 | best_NDCG@20:0.5582 | best_recall@5:0.5617 | best_recall@10:0.6668 | best_recall@20:0.7118 | best_precision@5:0.6741 | best_precision@10:0.4001 | best_precision@20:0.2136 | 
epoch: 60, training loss: 11.192166705661975, train time: 34.236592054367065
epoch: 61, training loss: 12.680363815792589, train time: 33.996010541915894
epoch: 62, training loss: 10.681542266901147, train time: 33.392749071121216
epoch: 63, training loss: 13.364376194916758, train time: 33.64158344268799
epoch: 64, training loss: 11.297647617303141, train time: 33.82259798049927
epoch: 65, training loss: 11.896972812097601, train time: 34.2023446559906
epoch: 66, training loss: 11.433048636602962, train time: 34.498865365982056
epoch: 67, training loss: 11.38100356172788, train time: 34.08786082267761
epoch: 68, training loss: 10.986574620109195, train time: 33.4548864364624
epoch: 69, training loss: 11.538387705124535, train time: 33.90165972709656
epoch: 70, training loss: 11.322872648788575, train time: 34.36869668960571
epoch: 71, training loss: 10.24291602789367, train time: 33.89881086349487
epoch: 72, training loss: 11.67024212074108, train time: 33.84711837768555
epoch: 73, training loss: 10.202867477876111, train time: 34.02000856399536
epoch: 74, training loss: 11.212079779122405, train time: 34.0043044090271
epoch: 75, training loss: 10.597780263919958, train time: 33.847352027893066
epoch: 76, training loss: 10.12019455092593, train time: 34.502222537994385
epoch: 77, training loss: 10.253162380522554, train time: 34.51044011116028
epoch: 78, training loss: 10.240477584091877, train time: 34.23388051986694
epoch: 79, training loss: 11.259642722539184, train time: 33.94695830345154
epoch: 80, training loss: 9.861165108428395, train time: 33.58069968223572
epoch: 81, training loss: 10.598724343874437, train time: 34.1528160572052
epoch: 82, training loss: 9.72000901734475, train time: 33.79106640815735
epoch: 83, training loss: 10.745485709208992, train time: 33.28594470024109
epoch: 84, training loss: 11.009467964895975, train time: 34.01310396194458
epoch: 85, training loss: 9.86742513760521, train time: 33.64576458930969
epoch: 86, training loss: 10.525413788866103, train time: 34.205326795578
epoch: 87, training loss: 9.832479630753141, train time: 33.977137327194214
epoch: 88, training loss: 10.554172716722405, train time: 34.32035565376282
epoch: 89, training loss: 10.342900433910813, train time: 33.92739677429199
epo:89 | HR@5:0.7628 | HR@10:0.7993 | HR@20:0.8453 | NDCG@5:0.4856 | NDCG@10:0.5218 | NDCG@20:0.5677 | recall@5:0.5464 | recall@10:0.6401 | recall@20:0.6883 | precision@5:0.6557 | precision@10:0.3841 | precision@20:0.2065 | best_HR@5:0.7968 | best_HR@10:0.8313 | best_HR@20:0.8720 | best_NDCG@5:0.4856 | best_NDCG@10:0.5218 | best_NDCG@20:0.5677 | best_recall@5:0.5617 | best_recall@10:0.6668 | best_recall@20:0.7118 | best_precision@5:0.6741 | best_precision@10:0.4001 | best_precision@20:0.2136 | 
epoch: 90, training loss: 9.690586143871315, train time: 33.74747014045715
epoch: 91, training loss: 9.839792761710555, train time: 33.77902555465698
epoch: 92, training loss: 9.018011968857934, train time: 34.345867395401
epoch: 93, training loss: 10.195803883288477, train time: 34.33479881286621
epoch: 94, training loss: 10.468943651614836, train time: 34.41278290748596
epoch: 95, training loss: 9.71235003428444, train time: 33.25750994682312
epoch: 96, training loss: 9.802366407308, train time: 34.430302143096924
epoch: 97, training loss: 9.894086581094825, train time: 33.95930075645447
epoch: 98, training loss: 10.06388724806169, train time: 33.451685667037964
epoch: 99, training loss: 9.152004789695638, train time: 34.38024139404297
epoch: 100, training loss: 8.685777876999339, train time: 34.245222330093384
epoch: 101, training loss: 9.593194827719799, train time: 34.09299063682556
epoch: 102, training loss: 8.442738327931238, train time: 33.86017727851868
epoch: 103, training loss: 9.028548268229002, train time: 33.65553164482117
epoch: 104, training loss: 9.613247761578805, train time: 34.13489890098572
epoch: 105, training loss: 9.49926435957866, train time: 33.16151189804077
epoch: 106, training loss: 10.027869269633413, train time: 33.85879993438721
epoch: 107, training loss: 9.521276602648356, train time: 33.65216779708862
epoch: 108, training loss: 8.432940840426795, train time: 34.048256397247314
epoch: 109, training loss: 9.629652632922586, train time: 33.58069610595703
epoch: 110, training loss: 8.610183964603891, train time: 34.15222096443176
epoch: 111, training loss: 8.686872226949959, train time: 33.6348659992218
epoch: 112, training loss: 8.698013586182128, train time: 33.994638204574585
epoch: 113, training loss: 9.92039016321803, train time: 34.29550647735596
epoch: 114, training loss: 8.727548967148493, train time: 33.91918659210205
epoch: 115, training loss: 8.549333332113633, train time: 34.1569721698761
epoch: 116, training loss: 7.874129049090939, train time: 34.02771973609924
epoch: 117, training loss: 8.678061863482867, train time: 33.70895576477051
epoch: 118, training loss: 9.034953450100033, train time: 34.00259757041931
epoch: 119, training loss: 9.45665983725172, train time: 34.2481906414032
epo:119 | HR@5:0.7437 | HR@10:0.7801 | HR@20:0.8310 | NDCG@5:0.4857 | NDCG@10:0.5221 | NDCG@20:0.5683 | recall@5:0.5421 | recall@10:0.6264 | recall@20:0.6752 | precision@5:0.6505 | precision@10:0.3759 | precision@20:0.2026 | best_HR@5:0.7968 | best_HR@10:0.8313 | best_HR@20:0.8720 | best_NDCG@5:0.4857 | best_NDCG@10:0.5221 | best_NDCG@20:0.5683 | best_recall@5:0.5617 | best_recall@10:0.6668 | best_recall@20:0.7118 | best_precision@5:0.6741 | best_precision@10:0.4001 | best_precision@20:0.2136 | 
epoch: 120, training loss: 7.933205567247256, train time: 34.147531270980835
epoch: 121, training loss: 8.905055522488794, train time: 34.17664933204651
epoch: 122, training loss: 8.139861455326582, train time: 34.40005826950073
epoch: 123, training loss: 8.77204689803591, train time: 34.32517719268799
epoch: 124, training loss: 8.302896632159673, train time: 33.4078528881073
epoch: 125, training loss: 8.985701803945915, train time: 34.169519662857056
epoch: 126, training loss: 9.518879951765882, train time: 33.945565700531006
epoch: 127, training loss: 8.60165879659769, train time: 34.12488508224487
epoch: 128, training loss: 8.887749259150382, train time: 34.22704291343689
epoch: 129, training loss: 9.02347103039483, train time: 34.14054036140442
epoch: 130, training loss: 8.600674253918356, train time: 33.77866840362549
epoch: 131, training loss: 8.084242911963088, train time: 34.3361930847168
epoch: 132, training loss: 8.576209982285746, train time: 33.89413499832153
epoch: 133, training loss: 8.52220925996619, train time: 34.009758710861206
epoch: 134, training loss: 8.67094673264046, train time: 34.31321692466736
epoch: 135, training loss: 8.85363753063774, train time: 34.226738691329956
epoch: 136, training loss: 8.843457186376327, train time: 33.6510374546051
epoch: 137, training loss: 8.778738177814773, train time: 34.10047435760498
epoch: 138, training loss: 9.40242158464764, train time: 34.30732536315918
epoch: 139, training loss: 7.292517584699908, train time: 34.525002002716064
epoch: 140, training loss: 8.68391174228276, train time: 34.13349509239197
epoch: 141, training loss: 8.850885840443027, train time: 34.21339130401611
epoch: 142, training loss: 6.859450407660006, train time: 33.58430480957031
epoch: 143, training loss: 8.959572605904015, train time: 33.21532154083252
epoch: 144, training loss: 7.992824364257842, train time: 33.679786682128906
epoch: 145, training loss: 8.731166558383052, train time: 34.20280194282532
epoch: 146, training loss: 8.430679302418667, train time: 32.963210105895996
epoch: 147, training loss: 7.133143351120992, train time: 34.154886960983276
epoch: 148, training loss: 7.951223865602429, train time: 33.60731339454651
epoch: 149, training loss: 7.5254738824933725, train time: 33.80899262428284
epo:149 | HR@5:0.7531 | HR@10:0.7882 | HR@20:0.8321 | NDCG@5:0.4887 | NDCG@10:0.5252 | NDCG@20:0.5711 | recall@5:0.5415 | recall@10:0.6313 | recall@20:0.6776 | precision@5:0.6498 | precision@10:0.3788 | precision@20:0.2033 | best_HR@5:0.7968 | best_HR@10:0.8313 | best_HR@20:0.8720 | best_NDCG@5:0.4887 | best_NDCG@10:0.5252 | best_NDCG@20:0.5711 | best_recall@5:0.5617 | best_recall@10:0.6668 | best_recall@20:0.7118 | best_precision@5:0.6741 | best_precision@10:0.4001 | best_precision@20:0.2136 | 
training finish
