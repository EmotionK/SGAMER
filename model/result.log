nohup: ignoring input
run.py device: cuda
../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ../torch/csrc/utils/tensor_new.cpp:201.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.0531158447265625e-06
user  100 time:  198.07196760177612
user  200 time:  393.616739988327
user  300 time:  592.3580131530762
user  400 time:  791.6766250133514
user  500 time:  989.9525136947632
user  600 time:  1190.7716252803802
user  700 time:  1389.5889132022858
user  800 time:  1590.557512998581
user  900 time:  1792.028020620346
user  1000 time:  1993.895430803299
user  1100 time:  2195.9174375534058
user  1200 time:  2397.1368083953857
user  1300 time:  2599.6816396713257
user  1400 time:  2803.775326013565
start training item-item instance self attention module...
user  0 time:  1.1682510375976562e-05
user  100 time:  169.65173411369324
user  200 time:  339.143675327301
user  300 time:  511.19322299957275
user  400 time:  680.1464188098907
user  500 time:  848.1372425556183
user  600 time:  1016.4459593296051
user  700 time:  1182.49080824852
user  800 time:  1349.8798785209656
user  900 time:  1513.7649412155151
user  1000 time:  1683.3486640453339
user  1100 time:  1850.5970406532288
user  1200 time:  2017.8412091732025
user  1300 time:  2187.768476009369
user  1400 time:  2357.177282810211
start updating user and item embedding...
user_name:1450
user  0 time:  1.2636184692382812e-05
user  100 time:  14.45315170288086
user  200 time:  28.88950777053833
user  300 time:  43.57966876029968
user  400 time:  58.12503910064697
user  500 time:  72.61771607398987
user  600 time:  87.3026123046875
user  700 time:  101.59358596801758
user  800 time:  115.98685598373413
user  900 time:  130.50984692573547
user  1000 time:  145.1327908039093
user  1100 time:  159.62900972366333
user  1200 time:  174.11137795448303
user  1300 time:  188.3462197780609
user  1400 time:  202.83531260490417
start training recommendation module...
recommendation_model.py:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 184.57956169254612, train time: 24.139713048934937
epoch: 1, training loss: 111.35806631823652, train time: 23.89322805404663
epoch: 2, training loss: 93.22268851907575, train time: 24.3655686378479
epoch: 3, training loss: 83.83270636725501, train time: 23.88297748565674
epoch: 4, training loss: 76.31683566317224, train time: 23.98529553413391
epoch: 5, training loss: 70.22933743261092, train time: 24.0689914226532
epoch: 6, training loss: 66.04321385132789, train time: 23.971623182296753
epoch: 7, training loss: 62.00162387413002, train time: 23.936566591262817
epoch: 8, training loss: 58.37340351318562, train time: 23.92914843559265
epoch: 9, training loss: 54.94028705148958, train time: 23.781994819641113
epoch: 10, training loss: 52.91693452190157, train time: 23.94133186340332
epoch: 11, training loss: 50.120890451275045, train time: 23.84893226623535
epoch: 12, training loss: 48.18837436884496, train time: 23.898120641708374
epoch: 13, training loss: 46.09109706031086, train time: 23.856898069381714
epoch: 14, training loss: 42.81633596371648, train time: 24.144661903381348
epoch: 15, training loss: 41.89771679369369, train time: 23.992968797683716
epoch: 16, training loss: 38.728998008497, train time: 24.117273330688477
epoch: 17, training loss: 37.39250075311793, train time: 23.967902898788452
epoch: 18, training loss: 35.69679976798761, train time: 24.06063437461853
epoch: 19, training loss: 33.95079039813572, train time: 23.95308804512024
epoch: 20, training loss: 33.28675267807739, train time: 23.917312622070312
epoch: 21, training loss: 31.70033939078894, train time: 23.80392861366272
epoch: 22, training loss: 29.300023603220325, train time: 23.930158853530884
epoch: 23, training loss: 28.833107669820492, train time: 23.899341106414795
epoch: 24, training loss: 27.883100581778308, train time: 23.880610466003418
epoch: 25, training loss: 27.021386543274275, train time: 23.894996404647827
epoch: 26, training loss: 25.791945757142457, train time: 23.73888897895813
epoch: 27, training loss: 24.836159176680667, train time: 24.0960054397583
epoch: 28, training loss: 24.95403168816756, train time: 23.83006739616394
epoch: 29, training loss: 24.616377750472566, train time: 23.84090518951416
epoch: 30, training loss: 24.03465273693064, train time: 23.756528854370117
epoch: 31, training loss: 22.798089187761434, train time: 24.050103664398193
epoch: 32, training loss: 22.422156316873497, train time: 24.027897596359253
epoch: 33, training loss: 21.476583210084755, train time: 23.78072214126587
epoch: 34, training loss: 20.92882568298728, train time: 23.9268000125885
epoch: 35, training loss: 21.804840324737143, train time: 23.774291276931763
epoch: 36, training loss: 20.081913192002503, train time: 23.998229265213013
epoch: 37, training loss: 19.75690456949269, train time: 23.94473147392273
epoch: 38, training loss: 19.319512258167833, train time: 23.736612796783447
epoch: 39, training loss: 20.438908665500094, train time: 23.97020959854126
epoch: 40, training loss: 18.578374566158573, train time: 23.961922883987427
epoch: 41, training loss: 19.918413817227247, train time: 23.955129384994507
epoch: 42, training loss: 19.905303829520108, train time: 23.85325336456299
epoch: 43, training loss: 19.536875403693557, train time: 24.123060941696167
epoch: 44, training loss: 17.99431643010962, train time: 23.850631952285767
epoch: 45, training loss: 18.284903844092128, train time: 23.83987069129944
epoch: 46, training loss: 18.636947386675274, train time: 23.92927837371826
epoch: 47, training loss: 17.471298603188757, train time: 23.925381422042847
epoch: 48, training loss: 17.399227932443523, train time: 24.1530921459198
epoch: 49, training loss: 17.163004656407566, train time: 23.97244930267334
epo:49|HR@1:0.6300 | HR@5:0.7854 | HR@10:0.8224 | HR@20:0.8694 | HR@50:0.9432 | NDCG@1:0.3954 | NDCG@5:0.4630 | NDCG@10:0.5006| NDCG@20:0.5493| NDCG@50:0.6479| best_HR@1:0.6300 | best_HR@5:0.7854 | best_HR@10:0.8224 | best_HR@20:0.8694 | best_HR@50:0.9432 | best_NDCG@1:0.3954 | best_NDCG@5:0.4630 | best_NDCG@10:0.5006 | best_NDCG@20:0.5493 | best_NDCG@50:0.6479 | train_time:23.97 | test_time:350.70
epoch: 50, training loss: 17.322896560951904, train time: 24.13569951057434
epoch: 51, training loss: 17.013671132166905, train time: 24.053462505340576
epoch: 52, training loss: 17.027560992463805, train time: 23.94878125190735
epoch: 53, training loss: 17.618870281059344, train time: 24.063000679016113
epoch: 54, training loss: 15.819784827513104, train time: 24.053650617599487
epoch: 55, training loss: 15.809903334825549, train time: 24.187621593475342
epoch: 56, training loss: 17.00016068120067, train time: 24.130780696868896
epoch: 57, training loss: 15.34216143576566, train time: 24.02435064315796
epoch: 58, training loss: 16.95074964413527, train time: 23.93105721473694
epoch: 59, training loss: 16.359419799280317, train time: 23.798653602600098
epoch: 60, training loss: 15.134141944032763, train time: 23.98463773727417
epoch: 61, training loss: 16.18277162000436, train time: 24.0330011844635
epoch: 62, training loss: 16.367378550057538, train time: 24.109075784683228
epoch: 63, training loss: 14.585782860808536, train time: 23.854496002197266
epoch: 64, training loss: 14.708600416610238, train time: 23.95045232772827
epoch: 65, training loss: 15.13516355952504, train time: 23.988797187805176
epoch: 66, training loss: 15.250322133849977, train time: 23.890817403793335
epoch: 67, training loss: 16.02553318383616, train time: 24.13283133506775
epoch: 68, training loss: 15.12333814849194, train time: 23.886560678482056
epoch: 69, training loss: 14.022961713527138, train time: 23.97339963912964
epoch: 70, training loss: 15.381731022605777, train time: 24.092302799224854
epoch: 71, training loss: 15.039631237007143, train time: 23.95203685760498
epoch: 72, training loss: 14.169006230083824, train time: 23.9631609916687
epoch: 73, training loss: 14.416352449495662, train time: 23.93582034111023
epoch: 74, training loss: 14.474763560186602, train time: 23.89301562309265
epoch: 75, training loss: 13.974741143087158, train time: 23.882873058319092
epoch: 76, training loss: 13.208954337975683, train time: 23.883270740509033
epoch: 77, training loss: 13.902369369552503, train time: 23.83072781562805
epoch: 78, training loss: 14.524508570977787, train time: 23.95557975769043
epoch: 79, training loss: 14.209908934558712, train time: 24.138914585113525
epoch: 80, training loss: 14.729594577067928, train time: 23.94482946395874
epoch: 81, training loss: 15.047268076638943, train time: 24.01244878768921
epoch: 82, training loss: 13.363563206553863, train time: 24.06744623184204
epoch: 83, training loss: 12.81387448995315, train time: 23.949610710144043
epoch: 84, training loss: 13.942185020641546, train time: 23.986928701400757
epoch: 85, training loss: 13.4586158052972, train time: 24.143130779266357
epoch: 86, training loss: 13.77475899392175, train time: 24.114566564559937
epoch: 87, training loss: 12.581631360054416, train time: 24.049569368362427
epoch: 88, training loss: 13.515345180943143, train time: 23.92902970314026
epoch: 89, training loss: 13.52089445185527, train time: 23.935903787612915
epoch: 90, training loss: 12.87590240287534, train time: 24.016663551330566
epoch: 91, training loss: 13.559069945671126, train time: 24.054081678390503
epoch: 92, training loss: 12.603999951975311, train time: 24.118049144744873
epoch: 93, training loss: 12.046047634154093, train time: 23.81157159805298
epoch: 94, training loss: 13.125428599469444, train time: 24.027888774871826
epoch: 95, training loss: 12.896246099685186, train time: 24.0120530128479
epoch: 96, training loss: 13.99595076893354, train time: 23.780548810958862
epoch: 97, training loss: 14.578111118096786, train time: 23.943844079971313
epoch: 98, training loss: 14.064147397391707, train time: 23.817341327667236
epoch: 99, training loss: 11.699309838704266, train time: 23.880407333374023
epo:99|HR@1:0.6113 | HR@5:0.7616 | HR@10:0.7970 | HR@20:0.8409 | HR@50:0.9225 | NDCG@1:0.3968 | NDCG@5:0.4689 | NDCG@10:0.5073| NDCG@20:0.5556| NDCG@50:0.6529| best_HR@1:0.6300 | best_HR@5:0.7854 | best_HR@10:0.8224 | best_HR@20:0.8694 | best_HR@50:0.9432 | best_NDCG@1:0.3968 | best_NDCG@5:0.4689 | best_NDCG@10:0.5073 | best_NDCG@20:0.5556 | best_NDCG@50:0.6529 | train_time:23.88 | test_time:352.07
training finish
