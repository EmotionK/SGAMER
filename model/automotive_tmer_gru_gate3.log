nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.291534423828125e-06
user  100 time:  229.47284936904907
user  200 time:  458.23798394203186
user  300 time:  686.0761027336121
user  400 time:  914.6383140087128
user  500 time:  1141.9042310714722
user  600 time:  1381.5409455299377
user  700 time:  1658.4986655712128
user  800 time:  1928.5753829479218
user  900 time:  2202.3097915649414
user  1000 time:  2478.7502710819244
user  1100 time:  2756.214366197586
user  1200 time:  3032.5425024032593
user  1300 time:  3314.833598136902
user  1400 time:  3593.18913936615
user  1500 time:  3876.544229745865
user  1600 time:  4165.785584688187
user  1700 time:  4516.472636938095
user  1800 time:  4868.536836385727
user  1900 time:  5214.518698215485
user  2000 time:  5543.641748666763
user  2100 time:  5872.462073326111
user  2200 time:  6198.869537591934
user  2300 time:  6536.288172006607
user  2400 time:  6854.236503124237
user  2500 time:  7177.774561405182
user  2600 time:  7501.728912353516
user  2700 time:  7828.242778539658
user  2800 time:  8172.817519903183
user  2900 time:  8508.453558683395
user  3000 time:  8832.539150953293
user  3100 time:  9099.781385660172
user  3200 time:  9364.517569541931
user  3300 time:  9642.933600664139
user  3400 time:  9937.86972784996
user  3500 time:  10206.087296962738
user  3600 time:  10471.225672245026
user  3700 time:  10738.969915866852
user  3800 time:  11012.8539686203
user  3900 time:  11295.619591236115
user  4000 time:  11564.23560142517
user  4100 time:  11840.228979825974
user  4200 time:  12094.570083618164
user  4300 time:  12351.629397630692
user  4400 time:  12608.41359090805
user  4500 time:  12862.832249641418
start training item-item instance self attention module...
user  0 time:  4.291534423828125e-06
user  100 time:  195.53586101531982
user  200 time:  392.4712452888489
user  300 time:  585.3878409862518
user  400 time:  778.4024081230164
user  500 time:  962.5067579746246
user  600 time:  1149.8656513690948
user  700 time:  1343.0421540737152
user  800 time:  1530.1496748924255
user  900 time:  1720.4973669052124
user  1000 time:  1919.6589953899384
user  1100 time:  2114.4808824062347
user  1200 time:  2298.3925931453705
user  1300 time:  2486.6944954395294
user  1400 time:  2664.9246039390564
user  1500 time:  2859.683557987213
user  1600 time:  3049.419198989868
user  1700 time:  3231.966291666031
user  1800 time:  3429.3666503429413
user  1900 time:  3636.0348796844482
user  2000 time:  3830.756331682205
user  2100 time:  4001.2203481197357
user  2200 time:  4158.889603614807
user  2300 time:  4318.799251794815
user  2400 time:  4485.795760631561
user  2500 time:  4651.374065160751
user  2600 time:  4824.471428155899
user  2700 time:  5001.834938764572
user  2800 time:  5182.8475704193115
user  2900 time:  5351.172103643417
user  3000 time:  5519.476359605789
user  3100 time:  5691.837213277817
user  3200 time:  5852.107870340347
user  3300 time:  6020.886084318161
user  3400 time:  6191.70277261734
user  3500 time:  6363.29079413414
user  3600 time:  6544.898121595383
user  3700 time:  6715.460607290268
user  3800 time:  6890.92960691452
user  3900 time:  7045.07014298439
user  4000 time:  7226.930204153061
user  4100 time:  7409.687861204147
user  4200 time:  7582.654561042786
user  4300 time:  7753.483766555786
user  4400 time:  7928.026298046112
user  4500 time:  8093.573807477951
start updating user and item embedding...
user_name:4600
user  0 time:  1.6450881958007812e-05
user  100 time:  64.71248197555542
user  200 time:  128.4841570854187
user  300 time:  192.01839971542358
user  400 time:  256.08203506469727
user  500 time:  320.8770966529846
user  600 time:  384.74889945983887
user  700 time:  449.54789757728577
user  800 time:  513.0060546398163
user  900 time:  577.6219992637634
user  1000 time:  642.3562636375427
user  1100 time:  706.4895262718201
user  1200 time:  770.1681537628174
user  1300 time:  833.906681060791
user  1400 time:  897.4471392631531
user  1500 time:  961.658759355545
user  1600 time:  1025.2759323120117
user  1700 time:  1089.239039182663
user  1800 time:  1153.263349056244
user  1900 time:  1218.4586923122406
user  2000 time:  1283.160474061966
user  2100 time:  1347.0500664710999
user  2200 time:  1410.4289736747742
user  2300 time:  1474.355280637741
user  2400 time:  1538.438661813736
user  2500 time:  1602.455382347107
user  2600 time:  1665.2916860580444
user  2700 time:  1728.2187492847443
user  2800 time:  1792.2111020088196
user  2900 time:  1855.274994134903
user  3000 time:  1920.050441980362
user  3100 time:  1983.9081161022186
user  3200 time:  2046.8438246250153
user  3300 time:  2110.705824136734
user  3400 time:  2174.6564717292786
user  3500 time:  2238.7437710762024
user  3600 time:  2302.192017555237
user  3700 time:  2364.7421002388
user  3800 time:  2429.107328891754
user  3900 time:  2492.917977809906
user  4000 time:  2556.4223964214325
user  4100 time:  2620.739570379257
user  4200 time:  2684.8425788879395
user  4300 time:  2749.660968065262
user  4400 time:  2813.9686608314514
user  4500 time:  2878.1898860931396
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 294.1866795715614, train time: 335.0056653022766
epoch: 1, training loss: 181.73718336757156, train time: 334.7533845901489
epoch: 2, training loss: 167.65119878549012, train time: 336.24319410324097
epoch: 3, training loss: 160.91900025734503, train time: 334.67695450782776
epoch: 4, training loss: 154.49287365371129, train time: 335.01977944374084
epoch: 5, training loss: 151.04355011178995, train time: 338.55299520492554
epoch: 6, training loss: 146.69394668695168, train time: 335.81700801849365
epoch: 7, training loss: 142.07760533229157, train time: 334.4439296722412
epoch: 8, training loss: 137.34075142415531, train time: 336.4683675765991
epoch: 9, training loss: 135.8116230094456, train time: 336.75121879577637
epoch: 10, training loss: 132.3194125083828, train time: 335.6988799571991
epoch: 11, training loss: 129.47499535595125, train time: 335.55285835266113
epoch: 12, training loss: 127.91289078576665, train time: 335.83949875831604
epoch: 13, training loss: 125.58506403906358, train time: 335.28603434562683
epoch: 14, training loss: 124.72556062143121, train time: 334.95418548583984
epoch: 15, training loss: 121.60787010574859, train time: 336.20382380485535
epoch: 16, training loss: 118.8594272541668, train time: 334.0838532447815
epoch: 17, training loss: 118.18575443996815, train time: 336.1139919757843
epoch: 18, training loss: 117.15019847486838, train time: 336.73194646835327
epoch: 19, training loss: 114.79323857365671, train time: 335.45232462882996
epoch: 20, training loss: 115.73056065465062, train time: 334.0850450992584
epoch: 21, training loss: 112.44784571525815, train time: 337.09979820251465
epoch: 22, training loss: 112.57628921009018, train time: 336.4957172870636
epoch: 23, training loss: 112.42002823822259, train time: 335.88338017463684
epoch: 24, training loss: 110.08161911907519, train time: 336.45962142944336
epoch: 25, training loss: 109.67335755957902, train time: 337.1005983352661
epoch: 26, training loss: 108.78198492047522, train time: 336.85054421424866
epoch: 27, training loss: 108.27304284114507, train time: 335.75069427490234
epoch: 28, training loss: 107.26638837387873, train time: 335.3578760623932
epoch: 29, training loss: 106.08512117795908, train time: 333.96183037757874
epo:29 | HR@5:0.9011 | HR@10:0.9170 | HR@20:0.9363 | NDCG@5:0.4376 | NDCG@10:0.4809 | NDCG@20:0.5339 | recall@5:0.6408 | recall@10:0.7509 | recall@20:0.7733 | precision@5:0.7690 | precision@10:0.4505 | precision@20:0.2320 | best_HR@5:0.9011 | best_HR@10:0.9170 | best_HR@20:0.9363 | best_NDCG@5:0.4376 | best_NDCG@10:0.4809 | best_NDCG@20:0.5339 | best_recall@5:0.6408 | best_recall@10:0.7509 | best_recall@20:0.7733 | best_precision@5:0.7690 | best_precision@10:0.4505 | best_precision@20:0.2320 | 
epoch: 30, training loss: 107.47038862235786, train time: 335.1742961406708
epoch: 31, training loss: 104.40584497285454, train time: 337.35947585105896
epoch: 32, training loss: 105.07566078375385, train time: 336.25195360183716
epoch: 33, training loss: 103.50838560036937, train time: 336.06080436706543
epoch: 34, training loss: 104.61720425962267, train time: 337.59441089630127
epoch: 35, training loss: 104.35142379793251, train time: 335.97124552726746
epoch: 36, training loss: 103.3052687248346, train time: 335.664911031723
epoch: 37, training loss: 102.97046691638752, train time: 337.26282000541687
epoch: 38, training loss: 102.42033444996196, train time: 335.2699205875397
epoch: 39, training loss: 104.47469140384783, train time: 336.3745255470276
epoch: 40, training loss: 102.2460128088569, train time: 336.92179322242737
epoch: 41, training loss: 103.42586195798503, train time: 336.6947486400604
epoch: 42, training loss: 102.75674001837615, train time: 337.6856880187988
epoch: 43, training loss: 103.05501399410423, train time: 336.0850009918213
epoch: 44, training loss: 103.0482520185542, train time: 337.8951723575592
epoch: 45, training loss: 105.37323174419726, train time: 334.4820091724396
epoch: 46, training loss: 102.26141883530363, train time: 335.93249440193176
epoch: 47, training loss: 102.02905809889853, train time: 336.9625198841095
epoch: 48, training loss: 102.44647200417239, train time: 335.8932189941406
epoch: 49, training loss: 101.62801608094014, train time: 336.0945146083832
epoch: 50, training loss: 99.67725434726162, train time: 337.095342874527
epoch: 51, training loss: 97.08539745389135, train time: 336.10266637802124
epoch: 52, training loss: 96.47488792082004, train time: 335.46059370040894
epoch: 53, training loss: 96.0773546378914, train time: 336.3413486480713
epoch: 54, training loss: 96.34017912019772, train time: 335.6853280067444
epoch: 55, training loss: 96.56003292988316, train time: 335.90923404693604
epoch: 56, training loss: 95.04115994822132, train time: 336.7297592163086
epoch: 57, training loss: 96.7776073660425, train time: 336.00251054763794
epoch: 58, training loss: 96.18046494058217, train time: 336.6931653022766
epoch: 59, training loss: 94.58751331673557, train time: 331.73912286758423
epo:59 | HR@5:0.8842 | HR@10:0.9005 | HR@20:0.9216 | NDCG@5:0.4408 | NDCG@10:0.4839 | NDCG@20:0.5363 | recall@5:0.6330 | recall@10:0.7382 | recall@20:0.7602 | precision@5:0.7596 | precision@10:0.4429 | precision@20:0.2281 | best_HR@5:0.9011 | best_HR@10:0.9170 | best_HR@20:0.9363 | best_NDCG@5:0.4408 | best_NDCG@10:0.4839 | best_NDCG@20:0.5363 | best_recall@5:0.6408 | best_recall@10:0.7509 | best_recall@20:0.7733 | best_precision@5:0.7690 | best_precision@10:0.4505 | best_precision@20:0.2320 | 
epoch: 60, training loss: 94.63695245443523, train time: 339.67220544815063
epoch: 61, training loss: 94.09170727922174, train time: 335.6198921203613
epoch: 62, training loss: 93.59351406001952, train time: 337.0170204639435
epoch: 63, training loss: 93.25798645025498, train time: 342.8832151889801
epoch: 64, training loss: 92.98322817594453, train time: 338.5692665576935
epoch: 65, training loss: 94.78102103663696, train time: 335.89162373542786
epoch: 66, training loss: 94.5224972920114, train time: 333.82528591156006
epoch: 67, training loss: 95.6780100910837, train time: 340.79382491111755
epoch: 68, training loss: 95.2353133361321, train time: 345.0654194355011
epoch: 69, training loss: 94.71880741172208, train time: 341.7517261505127
epoch: 70, training loss: 95.63459173461888, train time: 355.18944907188416
epoch: 71, training loss: 93.36904635927203, train time: 374.7242052555084
epoch: 72, training loss: 95.90254581820773, train time: 435.4377110004425
epoch: 73, training loss: 94.73437872144132, train time: 436.9675323963165
epoch: 74, training loss: 92.94961852958659, train time: 437.09304332733154
epoch: 75, training loss: 92.16236434333405, train time: 437.61321544647217
epoch: 76, training loss: 92.70290111983195, train time: 437.2671444416046
epoch: 77, training loss: 93.26227763189672, train time: 436.3357141017914
epoch: 78, training loss: 94.18691627596127, train time: 438.2789189815521
epoch: 79, training loss: 94.07604491346137, train time: 434.27578568458557
epoch: 80, training loss: 93.72849511255481, train time: 430.03096055984497
epoch: 81, training loss: 92.75090442904184, train time: 430.3921391963959
epoch: 82, training loss: 95.61246978092822, train time: 431.508887052536
epoch: 83, training loss: 94.5540500019415, train time: 429.5742356777191
epoch: 84, training loss: 92.78289730084362, train time: 429.8577883243561
epoch: 85, training loss: 93.44436037573178, train time: 428.44460344314575
epoch: 86, training loss: 94.66837267685332, train time: 429.898544549942
epoch: 87, training loss: 93.38085541554028, train time: 429.1464183330536
epoch: 88, training loss: 93.3968434191338, train time: 429.2503447532654
epoch: 89, training loss: 93.68232453649398, train time: 418.42346000671387
epo:89 | HR@5:0.8620 | HR@10:0.8808 | HR@20:0.9047 | NDCG@5:0.4418 | NDCG@10:0.4840 | NDCG@20:0.5362 | recall@5:0.6264 | recall@10:0.7200 | recall@20:0.7451 | precision@5:0.7517 | precision@10:0.4320 | precision@20:0.2235 | best_HR@5:0.9011 | best_HR@10:0.9170 | best_HR@20:0.9363 | best_NDCG@5:0.4418 | best_NDCG@10:0.4840 | best_NDCG@20:0.5363 | best_recall@5:0.6408 | best_recall@10:0.7509 | best_recall@20:0.7733 | best_precision@5:0.7690 | best_precision@10:0.4505 | best_precision@20:0.2320 | 
epoch: 90, training loss: 92.93025213427609, train time: 381.72562646865845
epoch: 91, training loss: 94.02941440316499, train time: 382.26508474349976
epoch: 92, training loss: 93.31420044662082, train time: 381.35492730140686
epoch: 93, training loss: 92.12682999422395, train time: 380.67910146713257
epoch: 94, training loss: 92.11598793446319, train time: 380.68639850616455
epoch: 95, training loss: 91.77097914536716, train time: 380.84438967704773
epoch: 96, training loss: 92.00529648434167, train time: 381.2087461948395
epoch: 97, training loss: 93.69333966196427, train time: 371.3135802745819
epoch: 98, training loss: 92.29594451640878, train time: 347.7003242969513
epoch: 99, training loss: 92.25927377921471, train time: 346.3665347099304
epoch: 100, training loss: 91.32661881489184, train time: 346.84921979904175
epoch: 101, training loss: 91.67403310223017, train time: 346.90228819847107
epoch: 102, training loss: 93.75959729798342, train time: 346.24120712280273
epoch: 103, training loss: 92.09013878428959, train time: 346.3410904407501
epoch: 104, training loss: 92.79631377971964, train time: 347.3196816444397
epoch: 105, training loss: 94.74234054012777, train time: 348.21733713150024
epoch: 106, training loss: 94.33705462265789, train time: 346.1950738430023
epoch: 107, training loss: 93.58025275628461, train time: 346.42683458328247
epoch: 108, training loss: 96.56355026384699, train time: 348.1355950832367
epoch: 109, training loss: 96.59228651221201, train time: 346.54571533203125
epoch: 110, training loss: 96.33652248360158, train time: 347.41739225387573
epoch: 111, training loss: 96.70269331576128, train time: 347.9921796321869
epoch: 112, training loss: 96.01669131364906, train time: 346.23319268226624
epoch: 113, training loss: 94.46276612362999, train time: 356.06227469444275
epoch: 114, training loss: 97.37221702299576, train time: 377.0936167240143
epoch: 115, training loss: 96.21839355240081, train time: 377.6177840232849
epoch: 116, training loss: 97.60104203935771, train time: 376.6945140361786
epoch: 117, training loss: 97.65104033249372, train time: 377.51306414604187
epoch: 118, training loss: 96.77588347499841, train time: 377.07323956489563
epoch: 119, training loss: 94.82890668592154, train time: 366.80999732017517
epo:119 | HR@5:0.8586 | HR@10:0.8757 | HR@20:0.9000 | NDCG@5:0.4338 | NDCG@10:0.4771 | NDCG@20:0.5300 | recall@5:0.6232 | recall@10:0.7158 | recall@20:0.7398 | precision@5:0.7478 | precision@10:0.4295 | precision@20:0.2219 | best_HR@5:0.9011 | best_HR@10:0.9170 | best_HR@20:0.9363 | best_NDCG@5:0.4418 | best_NDCG@10:0.4840 | best_NDCG@20:0.5363 | best_recall@5:0.6408 | best_recall@10:0.7509 | best_recall@20:0.7733 | best_precision@5:0.7690 | best_precision@10:0.4505 | best_precision@20:0.2320 | 
epoch: 120, training loss: 96.3710700318843, train time: 376.38983631134033
epoch: 121, training loss: 97.41549360788486, train time: 377.3344006538391
epoch: 122, training loss: 96.48028350798268, train time: 377.91007900238037
epoch: 123, training loss: 97.05730476899771, train time: 378.21965289115906
epoch: 124, training loss: 97.47880631574662, train time: 378.61086988449097
epoch: 125, training loss: 98.6682911666212, train time: 378.5760450363159
epoch: 126, training loss: 96.37715897562884, train time: 392.1508958339691
epoch: 127, training loss: 99.33803294517566, train time: 439.02111554145813
epoch: 128, training loss: 98.32121292448574, train time: 446.2520172595978
epoch: 129, training loss: 96.64097622239206, train time: 458.8692319393158
epoch: 130, training loss: 97.95100898684905, train time: 465.78141951560974
epoch: 131, training loss: 97.89055135721719, train time: 471.1500418186188
epoch: 132, training loss: 97.86360205291567, train time: 466.4831404685974
epoch: 133, training loss: 96.3325342694734, train time: 467.7105031013489
epoch: 134, training loss: 96.26834416895144, train time: 467.60347962379456
epoch: 135, training loss: 98.59960498485452, train time: 470.39184308052063
epoch: 136, training loss: 98.08109608487575, train time: 465.6196653842926
epoch: 137, training loss: 97.68391947793134, train time: 467.01455545425415
epoch: 138, training loss: 96.58346718345274, train time: 469.6156449317932
epoch: 139, training loss: 96.99819406960887, train time: 471.08720302581787
epoch: 140, training loss: 96.25384515873156, train time: 472.0344226360321
epoch: 141, training loss: 97.77279468286724, train time: 471.2130377292633
epoch: 142, training loss: 98.77778591999231, train time: 472.52282762527466
epoch: 143, training loss: 97.5565119785606, train time: 472.8272125720978
epoch: 144, training loss: 96.83237592531077, train time: 469.5607304573059
epoch: 145, training loss: 96.36788364615495, train time: 474.09746289253235
epoch: 146, training loss: 97.72720718484925, train time: 473.3519341945648
epoch: 147, training loss: 98.66567240503355, train time: 469.5060052871704
epoch: 148, training loss: 97.99483174359193, train time: 473.4685802459717
epoch: 149, training loss: 97.88640775851673, train time: 494.8683156967163
epo:149 | HR@5:0.8508 | HR@10:0.8680 | HR@20:0.8932 | NDCG@5:0.4333 | NDCG@10:0.4773 | NDCG@20:0.5303 | recall@5:0.6200 | recall@10:0.7119 | recall@20:0.7347 | precision@5:0.7440 | precision@10:0.4272 | precision@20:0.2204 | best_HR@5:0.9011 | best_HR@10:0.9170 | best_HR@20:0.9363 | best_NDCG@5:0.4418 | best_NDCG@10:0.4840 | best_NDCG@20:0.5363 | best_recall@5:0.6408 | best_recall@10:0.7509 | best_recall@20:0.7733 | best_precision@5:0.7690 | best_precision@10:0.4505 | best_precision@20:0.2320 | 
training finish
