nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  1.0251998901367188e-05
user  100 time:  334.9317147731781
user  200 time:  669.2600650787354
user  300 time:  1006.7976248264313
user  400 time:  1346.4148335456848
user  500 time:  1682.2801957130432
user  600 time:  2026.27348279953
user  700 time:  2370.8751537799835
user  800 time:  2709.8737676143646
user  900 time:  3052.150126218796
user  1000 time:  3393.4588515758514
user  1100 time:  3738.5002768039703
user  1200 time:  4081.579702615738
user  1300 time:  4428.62938284874
user  1400 time:  4770.107172489166
user  1500 time:  5112.818420648575
user  1600 time:  5452.244483709335
user  1700 time:  5793.113164663315
user  1800 time:  6138.164516687393
user  1900 time:  6482.164128303528
user  2000 time:  6823.95539855957
user  2100 time:  7162.254814624786
user  2200 time:  7504.568643331528
user  2300 time:  7852.021762371063
user  2400 time:  8193.193375349045
user  2500 time:  8536.933185338974
user  2600 time:  8881.614110708237
user  2700 time:  9224.018551588058
user  2800 time:  9571.904561281204
user  2900 time:  9916.301891088486
user  3000 time:  10259.548532009125
user  3100 time:  10604.157479286194
user  3200 time:  10945.622278928757
user  3300 time:  11290.826706171036
user  3400 time:  11637.20032787323
user  3500 time:  11979.39299082756
user  3600 time:  12326.916468143463
user  3700 time:  12668.183029413223
user  3800 time:  13007.818569421768
user  3900 time:  13351.416116476059
user  4000 time:  13694.939183473587
user  4100 time:  14040.473786354065
user  4200 time:  14381.286017894745
user  4300 time:  14724.741684913635
user  4400 time:  15071.29964375496
user  4500 time:  15412.547940969467
start training item-item instance self attention module...
user  0 time:  5.9604644775390625e-06
user  100 time:  234.11379408836365
user  200 time:  469.7479546070099
user  300 time:  699.2074730396271
user  400 time:  929.4560182094574
user  500 time:  1154.2211644649506
user  600 time:  1380.102426290512
user  700 time:  1611.793704032898
user  800 time:  1841.3695418834686
user  900 time:  2074.3651456832886
user  1000 time:  2314.6662833690643
user  1100 time:  2548.96084523201
user  1200 time:  2774.0804188251495
user  1300 time:  3006.595845222473
user  1400 time:  3220.671894788742
user  1500 time:  3453.144770860672
user  1600 time:  3677.672981262207
user  1700 time:  3891.861609697342
user  1800 time:  4123.040259599686
user  1900 time:  4365.6881902217865
user  2000 time:  4594.022967576981
user  2100 time:  4800.290913820267
user  2200 time:  5008.504488706589
user  2300 time:  5215.455293893814
user  2400 time:  5432.859588861465
user  2500 time:  5650.36096405983
user  2600 time:  5875.637693166733
user  2700 time:  6105.97047328949
user  2800 time:  6342.4545521736145
user  2900 time:  6562.090322256088
user  3000 time:  6781.810035943985
user  3100 time:  7008.095529794693
user  3200 time:  7215.667662620544
user  3300 time:  7435.847488164902
user  3400 time:  7660.2532477378845
user  3500 time:  7881.9946711063385
user  3600 time:  8119.46005654335
user  3700 time:  8341.20220899582
user  3800 time:  8569.613208770752
user  3900 time:  8772.22249007225
user  4000 time:  9009.151925086975
user  4100 time:  9247.282876968384
user  4200 time:  9472.521676778793
user  4300 time:  9695.974450349808
user  4400 time:  9924.40534377098
user  4500 time:  10137.993855714798
start updating user and item embedding...
user_name:4600
user  0 time:  9.5367431640625e-06
user  100 time:  15.811887264251709
user  200 time:  31.657121896743774
user  300 time:  47.221829891204834
user  400 time:  62.94552993774414
user  500 time:  78.61152482032776
user  600 time:  94.25148892402649
user  700 time:  110.08596563339233
user  800 time:  125.78696751594543
user  900 time:  141.44191026687622
user  1000 time:  157.21247482299805
user  1100 time:  172.81219220161438
user  1200 time:  188.82049703598022
user  1300 time:  204.43342661857605
user  1400 time:  220.1298451423645
user  1500 time:  235.79908299446106
user  1600 time:  251.43673872947693
user  1700 time:  267.12918853759766
user  1800 time:  282.7740740776062
user  1900 time:  298.421870470047
user  2000 time:  314.0338010787964
user  2100 time:  329.39958691596985
user  2200 time:  345.09406781196594
user  2300 time:  360.5280454158783
user  2400 time:  376.1053388118744
user  2500 time:  391.7186143398285
user  2600 time:  407.4904098510742
user  2700 time:  423.26747250556946
user  2800 time:  438.8053677082062
user  2900 time:  454.49999356269836
user  3000 time:  470.11656069755554
user  3100 time:  485.70377373695374
user  3200 time:  501.2660562992096
user  3300 time:  516.9052474498749
user  3400 time:  532.3182384967804
user  3500 time:  547.6781015396118
user  3600 time:  563.2406463623047
user  3700 time:  578.8689036369324
user  3800 time:  594.3626115322113
user  3900 time:  609.5417573451996
user  4000 time:  625.1146728992462
user  4100 time:  640.6620993614197
user  4200 time:  656.3085277080536
user  4300 time:  671.8829836845398
user  4400 time:  687.3021266460419
user  4500 time:  702.7791979312897
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 289.70134384412086, train time: 244.06950879096985
epoch: 1, training loss: 181.50297550785763, train time: 244.45807123184204
epoch: 2, training loss: 168.16060919937445, train time: 243.88443779945374
epoch: 3, training loss: 161.00656472465198, train time: 244.2126748561859
epoch: 4, training loss: 156.10503533488372, train time: 244.11374855041504
epoch: 5, training loss: 151.09042802143085, train time: 244.76961040496826
epoch: 6, training loss: 146.28208161836665, train time: 244.48762440681458
epoch: 7, training loss: 142.78708495694445, train time: 244.09172177314758
epoch: 8, training loss: 138.78984559273522, train time: 244.08152747154236
epoch: 9, training loss: 135.99969271864393, train time: 243.89096093177795
epoch: 10, training loss: 132.18436555712833, train time: 244.30094146728516
epoch: 11, training loss: 129.33174998425238, train time: 243.76451134681702
epoch: 12, training loss: 128.02347959540202, train time: 244.26971888542175
epoch: 13, training loss: 124.96757161804999, train time: 244.67693638801575
epoch: 14, training loss: 121.89596191128658, train time: 244.13493633270264
epoch: 15, training loss: 120.52217055408983, train time: 244.20492029190063
epoch: 16, training loss: 118.24276142834424, train time: 243.9626076221466
epoch: 17, training loss: 116.66868791584784, train time: 244.2826738357544
epoch: 18, training loss: 116.53997968928888, train time: 244.34375023841858
epoch: 19, training loss: 113.57254775481124, train time: 244.56205534934998
epoch: 20, training loss: 113.5595110566428, train time: 244.43694829940796
epoch: 21, training loss: 110.85150999385223, train time: 245.12337946891785
epoch: 22, training loss: 110.32027315044252, train time: 244.22764420509338
epoch: 23, training loss: 109.63945625408815, train time: 243.70260787010193
epoch: 24, training loss: 108.69411792399478, train time: 244.3899655342102
epoch: 25, training loss: 106.74221922724246, train time: 244.10161352157593
epoch: 26, training loss: 106.99693544494949, train time: 244.47236347198486
epoch: 27, training loss: 104.2578217192422, train time: 243.8125696182251
epoch: 28, training loss: 103.18590826138825, train time: 244.82443833351135
epoch: 29, training loss: 103.34916317650641, train time: 244.01138186454773
epo:29 | HR@5:0.8956 | HR@10:0.9118 | HR@20:0.9316 | NDCG@5:0.4370 | NDCG@10:0.4807 | NDCG@20:0.5339 | recall@5:0.6396 | recall@10:0.7475 | recall@20:0.7687 | precision@5:0.7676 | precision@10:0.4485 | precision@20:0.2306 | best_HR@5:0.8956 | best_HR@10:0.9118 | best_HR@20:0.9316 | best_NDCG@5:0.4370 | best_NDCG@10:0.4807 | best_NDCG@20:0.5339 | best_recall@5:0.6396 | best_recall@10:0.7475 | best_recall@20:0.7687 | best_precision@5:0.7676 | best_precision@10:0.4485 | best_precision@20:0.2306 | 
epoch: 30, training loss: 103.41755143726186, train time: 244.5821409225464
epoch: 31, training loss: 103.05202692955208, train time: 243.510906457901
epoch: 32, training loss: 102.29938988580398, train time: 244.33270239830017
epoch: 33, training loss: 100.0596902574107, train time: 243.70874285697937
epoch: 34, training loss: 100.31751517063822, train time: 244.47726917266846
epoch: 35, training loss: 100.15237190431435, train time: 243.9201774597168
epoch: 36, training loss: 98.85451706797176, train time: 243.74561381340027
epoch: 37, training loss: 98.72896522327574, train time: 243.36567950248718
epoch: 38, training loss: 97.09759226953611, train time: 243.936505317688
epoch: 39, training loss: 96.72402501700708, train time: 243.33103823661804
epoch: 40, training loss: 96.22710701959659, train time: 244.19734239578247
epoch: 41, training loss: 95.9228932100159, train time: 243.52950072288513
epoch: 42, training loss: 95.71575166094408, train time: 243.0300052165985
epoch: 43, training loss: 96.33673360675311, train time: 243.7936511039734
epoch: 44, training loss: 96.0042590671219, train time: 243.8054654598236
epoch: 45, training loss: 95.07322830968769, train time: 244.55639910697937
epoch: 46, training loss: 95.99528885649124, train time: 243.62921953201294
epoch: 47, training loss: 94.13205489651591, train time: 243.70479679107666
epoch: 48, training loss: 94.89951597300387, train time: 243.72119164466858
epoch: 49, training loss: 94.14252284293616, train time: 244.0400607585907
epoch: 50, training loss: 94.32203291649057, train time: 243.72437834739685
epoch: 51, training loss: 95.09121015341952, train time: 243.44191908836365
epoch: 52, training loss: 93.85908214166557, train time: 243.87971687316895
epoch: 53, training loss: 95.32636457234912, train time: 243.96077156066895
epoch: 54, training loss: 96.30474470595436, train time: 243.82218194007874
epoch: 55, training loss: 94.50026870618603, train time: 243.79963159561157
epoch: 56, training loss: 93.41531458458849, train time: 244.1391248703003
epoch: 57, training loss: 93.53757002217753, train time: 243.79028797149658
epoch: 58, training loss: 93.08500749489758, train time: 243.4589123725891
epoch: 59, training loss: 93.47778449976613, train time: 243.843896150589
epo:59 | HR@5:0.8759 | HR@10:0.8931 | HR@20:0.9163 | NDCG@5:0.4482 | NDCG@10:0.4898 | NDCG@20:0.5410 | recall@5:0.6334 | recall@10:0.7322 | recall@20:0.7552 | precision@5:0.7600 | precision@10:0.4393 | precision@20:0.2266 | best_HR@5:0.8956 | best_HR@10:0.9118 | best_HR@20:0.9316 | best_NDCG@5:0.4482 | best_NDCG@10:0.4898 | best_NDCG@20:0.5410 | best_recall@5:0.6396 | best_recall@10:0.7475 | best_recall@20:0.7687 | best_precision@5:0.7676 | best_precision@10:0.4485 | best_precision@20:0.2306 | 
epoch: 60, training loss: 92.05831796027633, train time: 243.10339403152466
epoch: 61, training loss: 92.18049785336552, train time: 243.47129440307617
epoch: 62, training loss: 91.68511833374578, train time: 243.02460718154907
epoch: 63, training loss: 90.09826977499324, train time: 243.707284450531
epoch: 64, training loss: 91.64069096062303, train time: 243.58516645431519
epoch: 65, training loss: 92.23941193567589, train time: 243.63998222351074
epoch: 66, training loss: 89.96496822204062, train time: 243.62674713134766
epoch: 67, training loss: 92.68404018325236, train time: 243.54962038993835
epoch: 68, training loss: 92.74560037451738, train time: 244.25999665260315
epoch: 69, training loss: 92.04574321715336, train time: 243.57728052139282
epoch: 70, training loss: 92.65407945464176, train time: 243.4720299243927
epoch: 71, training loss: 90.55406564928126, train time: 243.5788562297821
epoch: 72, training loss: 91.52283255353541, train time: 244.94440603256226
epoch: 73, training loss: 90.86075480038562, train time: 243.83751440048218
epoch: 74, training loss: 89.93261830354459, train time: 243.96482920646667
epoch: 75, training loss: 90.60278646399092, train time: 243.33634424209595
epoch: 76, training loss: 90.97667968654423, train time: 243.20694208145142
epoch: 77, training loss: 91.06345075974241, train time: 243.2568860054016
epoch: 78, training loss: 89.35357673484395, train time: 243.6378092765808
epoch: 79, training loss: 89.01579362478515, train time: 243.61476516723633
epoch: 80, training loss: 89.57837076066062, train time: 243.51712036132812
epoch: 81, training loss: 90.39784495868662, train time: 243.5448682308197
epoch: 82, training loss: 91.30422404684214, train time: 244.0280101299286
epoch: 83, training loss: 90.20602701934695, train time: 244.2667715549469
epoch: 84, training loss: 89.4380082111602, train time: 243.08352136611938
epoch: 85, training loss: 89.99919437176868, train time: 243.62886261940002
epoch: 86, training loss: 89.91750576666527, train time: 243.83759880065918
epoch: 87, training loss: 89.65669626545423, train time: 243.6493992805481
epoch: 88, training loss: 90.91490462912043, train time: 243.5816798210144
epoch: 89, training loss: 88.06736395642656, train time: 243.7725830078125
epo:89 | HR@5:0.8646 | HR@10:0.8832 | HR@20:0.9062 | NDCG@5:0.4487 | NDCG@10:0.4910 | NDCG@20:0.5423 | recall@5:0.6279 | recall@10:0.7217 | recall@20:0.7466 | precision@5:0.7534 | precision@10:0.4330 | precision@20:0.2240 | best_HR@5:0.8956 | best_HR@10:0.9118 | best_HR@20:0.9316 | best_NDCG@5:0.4487 | best_NDCG@10:0.4910 | best_NDCG@20:0.5423 | best_recall@5:0.6396 | best_recall@10:0.7475 | best_recall@20:0.7687 | best_precision@5:0.7676 | best_precision@10:0.4485 | best_precision@20:0.2306 | 
epoch: 90, training loss: 87.52230943601171, train time: 244.6701328754425
epoch: 91, training loss: 87.96307920483014, train time: 243.96092224121094
epoch: 92, training loss: 87.32610200440467, train time: 243.5699999332428
epoch: 93, training loss: 89.82867356377392, train time: 243.91910195350647
epoch: 94, training loss: 87.65843955946548, train time: 244.09788274765015
epoch: 95, training loss: 88.17382277514844, train time: 244.41437935829163
epoch: 96, training loss: 89.02501450538693, train time: 243.66092491149902
epoch: 97, training loss: 88.44216583574598, train time: 243.9451081752777
epoch: 98, training loss: 90.85467077243084, train time: 243.87241387367249
epoch: 99, training loss: 88.8019531175305, train time: 244.43897008895874
epoch: 100, training loss: 89.0030106455597, train time: 243.74882912635803
epoch: 101, training loss: 91.7255352164575, train time: 243.52477192878723
epoch: 102, training loss: 89.7195929956979, train time: 243.98563647270203
epoch: 103, training loss: 89.34606670863286, train time: 244.06247448921204
epoch: 104, training loss: 88.9538882722045, train time: 243.81957459449768
epoch: 105, training loss: 89.51237368410511, train time: 243.34265637397766
epoch: 106, training loss: 89.66403696582711, train time: 243.51199412345886
epoch: 107, training loss: 91.26757333020942, train time: 243.81107473373413
epoch: 108, training loss: 90.60442260308628, train time: 243.78375673294067
epoch: 109, training loss: 91.88640545591625, train time: 243.61940026283264
epoch: 110, training loss: 89.61328142290586, train time: 243.92519211769104
epoch: 111, training loss: 88.41303169127787, train time: 243.83529233932495
epoch: 112, training loss: 90.75884741311893, train time: 244.18033146858215
epoch: 113, training loss: 90.03545420344017, train time: 243.5535707473755
epoch: 114, training loss: 89.77950757018698, train time: 243.7841670513153
epoch: 115, training loss: 89.74798289971659, train time: 244.11005234718323
epoch: 116, training loss: 89.97754135845753, train time: 244.06147861480713
epoch: 117, training loss: 91.26134010691021, train time: 243.67081475257874
epoch: 118, training loss: 89.08699582748523, train time: 243.61430501937866
epoch: 119, training loss: 89.96995632819016, train time: 243.91499662399292
epo:119 | HR@5:0.8607 | HR@10:0.8769 | HR@20:0.9014 | NDCG@5:0.4462 | NDCG@10:0.4883 | NDCG@20:0.5396 | recall@5:0.6234 | recall@10:0.7181 | recall@20:0.7414 | precision@5:0.7480 | precision@10:0.4309 | precision@20:0.2224 | best_HR@5:0.8956 | best_HR@10:0.9118 | best_HR@20:0.9316 | best_NDCG@5:0.4487 | best_NDCG@10:0.4910 | best_NDCG@20:0.5423 | best_recall@5:0.6396 | best_recall@10:0.7475 | best_recall@20:0.7687 | best_precision@5:0.7676 | best_precision@10:0.4485 | best_precision@20:0.2306 | 
epoch: 120, training loss: 91.0907995036323, train time: 243.64143204689026
epoch: 121, training loss: 88.84014173591277, train time: 243.96942496299744
epoch: 122, training loss: 90.27688403485081, train time: 243.6302888393402
epoch: 123, training loss: 89.95334256356, train time: 244.0006046295166
epoch: 124, training loss: 90.0002431027824, train time: 244.08195424079895
epoch: 125, training loss: 91.0280245531394, train time: 243.8821680545807
epoch: 126, training loss: 89.43644758844312, train time: 243.84961032867432
epoch: 127, training loss: 91.29073595340742, train time: 243.85121130943298
epoch: 128, training loss: 90.83654726426903, train time: 243.5913565158844
epoch: 129, training loss: 91.52857575817325, train time: 243.4086627960205
epoch: 130, training loss: 89.40695584255445, train time: 243.70523595809937
epoch: 131, training loss: 90.24113176781248, train time: 243.47333931922913
epoch: 132, training loss: 90.91923741374922, train time: 243.55505347251892
epoch: 133, training loss: 89.5038587957606, train time: 243.66760563850403
epoch: 134, training loss: 90.59945311499177, train time: 243.8567042350769
epoch: 135, training loss: 91.52118033375882, train time: 244.40756726264954
epoch: 136, training loss: 90.97689179538429, train time: 244.00141215324402
epoch: 137, training loss: 90.0748969876513, train time: 244.05133748054504
epoch: 138, training loss: 91.4810977036359, train time: 244.19913029670715
epoch: 139, training loss: 91.27573577519797, train time: 244.11781692504883
epoch: 140, training loss: 93.68981473467284, train time: 243.9349513053894
epoch: 141, training loss: 93.34978079529537, train time: 243.86683559417725
epoch: 142, training loss: 92.69423249760439, train time: 244.17147088050842
epoch: 143, training loss: 93.50759745415417, train time: 243.9994626045227
epoch: 144, training loss: 95.89897362083866, train time: 243.97742128372192
epoch: 145, training loss: 94.12604092547554, train time: 243.7189166545868
epoch: 146, training loss: 95.1109258093602, train time: 243.99361729621887
epoch: 147, training loss: 95.72067354847968, train time: 243.85131978988647
epoch: 148, training loss: 94.4535882230266, train time: 243.9918270111084
epoch: 149, training loss: 94.65830004099553, train time: 243.70113396644592
epo:149 | HR@5:0.8479 | HR@10:0.8665 | HR@20:0.8904 | NDCG@5:0.4380 | NDCG@10:0.4811 | NDCG@20:0.5336 | recall@5:0.6206 | recall@10:0.7092 | recall@20:0.7336 | precision@5:0.7447 | precision@10:0.4255 | precision@20:0.2201 | best_HR@5:0.8956 | best_HR@10:0.9118 | best_HR@20:0.9316 | best_NDCG@5:0.4487 | best_NDCG@10:0.4910 | best_NDCG@20:0.5423 | best_recall@5:0.6396 | best_recall@10:0.7475 | best_recall@20:0.7687 | best_precision@5:0.7676 | best_precision@10:0.4485 | best_precision@20:0.2306 | 
training finish
