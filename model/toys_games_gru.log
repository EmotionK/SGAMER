nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.0994415283203125e-06
user  100 time:  248.97716116905212
user  200 time:  501.2080132961273
user  300 time:  749.3960268497467
user  400 time:  1002.0134177207947
user  500 time:  1252.7903411388397
user  600 time:  1508.88432431221
user  700 time:  1761.3285462856293
user  800 time:  2012.1468646526337
user  900 time:  2266.9380180835724
user  1000 time:  2522.53071308136
user  1100 time:  2839.4789674282074
user  1200 time:  3159.0855116844177
user  1300 time:  3482.1271200180054
user  1400 time:  3801.6651492118835
start training item-item instance self attention module...
user  0 time:  1.0728836059570312e-05
user  100 time:  144.16605043411255
user  200 time:  267.9879434108734
user  300 time:  401.124577999115
user  400 time:  505.78819608688354
user  500 time:  639.3696610927582
user  600 time:  759.9807839393616
user  700 time:  904.5546202659607
user  800 time:  1033.2179265022278
user  900 time:  1169.2513074874878
user  1000 time:  1313.1410737037659
user  1100 time:  1442.6197278499603
user  1200 time:  1566.8146433830261
user  1300 time:  1721.6849019527435
user  1400 time:  1857.0473597049713
start updating user and item embedding...
user_name:1450
user  0 time:  1.2636184692382812e-05
user  100 time:  24.161011695861816
user  200 time:  49.338590145111084
user  300 time:  74.19350409507751
user  400 time:  99.05254411697388
user  500 time:  124.03575611114502
user  600 time:  148.98795747756958
user  700 time:  174.11689281463623
user  800 time:  199.08975052833557
user  900 time:  224.1273114681244
user  1000 time:  249.421537399292
user  1100 time:  274.3934850692749
user  1200 time:  299.1504373550415
user  1300 time:  324.96142530441284
user  1400 time:  349.72159695625305
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 158.09253595789778, train time: 70.92684578895569
epoch: 1, training loss: 86.13783466077439, train time: 70.73797821998596
epoch: 2, training loss: 71.59956624627375, train time: 71.18638682365417
epoch: 3, training loss: 62.11531731511786, train time: 70.73641800880432
epoch: 4, training loss: 56.540231681778096, train time: 70.34989094734192
epoch: 5, training loss: 52.08291450935212, train time: 70.57857179641724
epoch: 6, training loss: 47.55230891642714, train time: 70.86470103263855
epoch: 7, training loss: 44.60078089882518, train time: 70.79804110527039
epoch: 8, training loss: 42.464945585550595, train time: 70.8374810218811
epoch: 9, training loss: 39.85214947916029, train time: 71.28219819068909
epoch: 10, training loss: 38.01320311062591, train time: 70.53210163116455
epoch: 11, training loss: 36.352875605871304, train time: 71.21042919158936
epoch: 12, training loss: 34.17739739869103, train time: 58.61189341545105
epoch: 13, training loss: 32.677195522834154, train time: 60.92966604232788
epoch: 14, training loss: 31.721502051677817, train time: 61.267131090164185
epoch: 15, training loss: 30.048780211444864, train time: 55.77617645263672
epoch: 16, training loss: 28.57791825516688, train time: 55.76768755912781
epoch: 17, training loss: 27.14234213831969, train time: 55.41835141181946
epoch: 18, training loss: 27.070409192671832, train time: 56.11102771759033
epoch: 19, training loss: 25.49836874509765, train time: 55.64295530319214
epoch: 20, training loss: 25.685627493927313, train time: 59.6344530582428
epoch: 21, training loss: 23.223426610641127, train time: 68.87854862213135
epoch: 22, training loss: 24.045786725379003, train time: 71.19867944717407
epoch: 23, training loss: 22.594709880001574, train time: 70.85494184494019
epoch: 24, training loss: 21.89248119919648, train time: 70.84376811981201
epoch: 25, training loss: 21.558263387639727, train time: 71.08113050460815
epoch: 26, training loss: 21.201095660916508, train time: 70.7038164138794
epoch: 27, training loss: 19.576190596613742, train time: 70.5084502696991
epoch: 28, training loss: 20.606079909700384, train time: 63.37749361991882
epoch: 29, training loss: 18.874748523882317, train time: 57.14632987976074
epoch: 30, training loss: 18.719693171513427, train time: 55.320526361465454
epoch: 31, training loss: 17.668234973127255, train time: 55.73665690422058
epoch: 32, training loss: 16.729260535384128, train time: 55.76332354545593
epoch: 33, training loss: 17.901659002066935, train time: 59.16582703590393
epoch: 34, training loss: 17.171219302716736, train time: 55.17191219329834
epoch: 35, training loss: 17.778327585975944, train time: 62.58362817764282
epoch: 36, training loss: 16.621642824598666, train time: 55.540048122406006
epoch: 37, training loss: 16.880760008974903, train time: 55.57433104515076
epoch: 38, training loss: 16.18969471719629, train time: 71.06309103965759
epoch: 39, training loss: 15.800215724253576, train time: 71.07020998001099
epoch: 40, training loss: 16.419061596644042, train time: 70.89072346687317
epoch: 41, training loss: 15.865105547520102, train time: 71.27064514160156
epoch: 42, training loss: 16.3744453495367, train time: 71.10149884223938
epoch: 43, training loss: 16.6817887091305, train time: 71.16680836677551
epoch: 44, training loss: 15.052331274768108, train time: 71.17706871032715
epoch: 45, training loss: 15.39953435245934, train time: 71.00793886184692
epoch: 46, training loss: 14.828289083870459, train time: 70.93169784545898
epoch: 47, training loss: 16.960436943328887, train time: 70.59567356109619
epoch: 48, training loss: 14.389147919652487, train time: 70.77936315536499
epoch: 49, training loss: 15.5696545143594, train time: 70.56788992881775
epo:49|HR@1:0.6947 | HR@5:0.8332 | HR@10:0.8591 | HR@20:0.8914 | HR@50:0.9494 | NDCG@1:0.3817 | NDCG@5:0.4654 | NDCG@10:0.5056| NDCG@20:0.5550| NDCG@50:0.6515| best_HR@1:0.6947 | best_HR@5:0.8332 | best_HR@10:0.8591 | best_HR@20:0.8914 | best_HR@50:0.9494 | best_NDCG@1:0.3817 | best_NDCG@5:0.4654 | best_NDCG@10:0.5056 | best_NDCG@20:0.5550 | best_NDCG@50:0.6515 | train_time:70.57 | test_time:538.42
epoch: 50, training loss: 14.315217856849586, train time: 70.69892954826355
epoch: 51, training loss: 15.08231664011771, train time: 70.93980956077576
epoch: 52, training loss: 15.522400800613013, train time: 70.92990326881409
epoch: 53, training loss: 14.761614999990456, train time: 70.92629313468933
epoch: 54, training loss: 14.693601611295435, train time: 71.06560707092285
epoch: 55, training loss: 14.821804844229746, train time: 70.82146072387695
epoch: 56, training loss: 13.145190376640699, train time: 71.2716383934021
epoch: 57, training loss: 14.380052107943584, train time: 70.62345266342163
epoch: 58, training loss: 13.408205365470394, train time: 70.9741096496582
epoch: 59, training loss: 13.13797407905031, train time: 71.08993101119995
epoch: 60, training loss: 12.902545925422771, train time: 70.48937010765076
epoch: 61, training loss: 13.643736646143509, train time: 69.51311564445496
epoch: 62, training loss: 14.43859357479721, train time: 57.48222875595093
epoch: 63, training loss: 12.989862744270795, train time: 56.180399656295776
epoch: 64, training loss: 13.916082618919859, train time: 56.39560341835022
epoch: 65, training loss: 13.529142544098477, train time: 57.530060052871704
epoch: 66, training loss: 13.24183712740296, train time: 62.32062125205994
epoch: 67, training loss: 12.773231374652426, train time: 57.67824459075928
epoch: 68, training loss: 13.670414639187243, train time: 59.5775625705719
epoch: 69, training loss: 14.255298564913346, train time: 59.40211057662964
epoch: 70, training loss: 13.07549803469783, train time: 56.334269523620605
epoch: 71, training loss: 13.29749120969359, train time: 47.83765912055969
epoch: 72, training loss: 12.038404008666191, train time: 47.62030386924744
epoch: 73, training loss: 12.460041241527733, train time: 47.49522566795349
epoch: 74, training loss: 12.746051248075787, train time: 47.38887619972229
epoch: 75, training loss: 12.478980395648875, train time: 47.37055420875549
epoch: 76, training loss: 12.068920363021277, train time: 47.64504146575928
epoch: 77, training loss: 11.12990036439794, train time: 47.4280309677124
epoch: 78, training loss: 11.806813831963439, train time: 47.42648911476135
epoch: 79, training loss: 12.277911903383, train time: 39.58551383018494
epoch: 80, training loss: 13.354086269210825, train time: 37.67548990249634
epoch: 81, training loss: 12.071241445236751, train time: 37.754420042037964
epoch: 82, training loss: 12.792595929462209, train time: 37.843416929244995
epoch: 83, training loss: 10.733185166036094, train time: 38.13799786567688
epoch: 84, training loss: 11.608958539746084, train time: 38.06845045089722
epoch: 85, training loss: 12.36741701256318, train time: 37.810731172561646
epoch: 86, training loss: 11.701027717783973, train time: 37.817362785339355
epoch: 87, training loss: 12.033258312248506, train time: 37.48745584487915
epoch: 88, training loss: 11.242990034537002, train time: 37.911619424819946
epoch: 89, training loss: 12.243446732442038, train time: 38.43918204307556
epoch: 90, training loss: 11.03566934966426, train time: 38.80315923690796
epoch: 91, training loss: 11.855971915234875, train time: 46.91764044761658
epoch: 92, training loss: 13.105116439914468, train time: 47.312334299087524
epoch: 93, training loss: 10.00583014259496, train time: 47.10172748565674
epoch: 94, training loss: 11.961534635455621, train time: 47.36012077331543
epoch: 95, training loss: 12.110109154114639, train time: 47.41776490211487
epoch: 96, training loss: 11.454141769728324, train time: 47.90025448799133
epoch: 97, training loss: 11.1095461965499, train time: 47.24914193153381
epoch: 98, training loss: 11.016575001663853, train time: 47.975202798843384
epoch: 99, training loss: 9.932132938937272, train time: 47.90397644042969
epo:99|HR@1:0.6768 | HR@5:0.8141 | HR@10:0.8336 | HR@20:0.8687 | HR@50:0.9368 | NDCG@1:0.4020 | NDCG@5:0.4877 | NDCG@10:0.5264| NDCG@20:0.5735| NDCG@50:0.6659| best_HR@1:0.6947 | best_HR@5:0.8332 | best_HR@10:0.8591 | best_HR@20:0.8914 | best_HR@50:0.9494 | best_NDCG@1:0.4020 | best_NDCG@5:0.4877 | best_NDCG@10:0.5264 | best_NDCG@20:0.5735 | best_NDCG@50:0.6659 | train_time:47.90 | test_time:477.92
training finish
