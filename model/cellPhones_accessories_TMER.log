nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_CellPhones_Accessories......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  5.0067901611328125e-06
user  100 time:  166.75574445724487
user  200 time:  344.8422341346741
user  300 time:  526.7656660079956
user  400 time:  708.32088804245
user  500 time:  887.9718887805939
user  600 time:  1070.4007053375244
user  700 time:  1243.8907279968262
user  800 time:  1413.3661940097809
user  900 time:  1583.0347611904144
user  1000 time:  1760.1401600837708
user  1100 time:  1934.6246275901794
user  1200 time:  2107.2137138843536
user  1300 time:  2297.274921655655
user  1400 time:  2484.1055364608765
user  1500 time:  2669.717109680176
user  1600 time:  2861.127209186554
user  1700 time:  3052.469745159149
user  1800 time:  3242.019327402115
user  1900 time:  3435.6122958660126
start training item-item instance self attention module...
user  0 time:  6.67572021484375e-06
user  100 time:  34.38552141189575
user  200 time:  63.96793460845947
user  300 time:  98.59778451919556
user  400 time:  131.1974310874939
user  500 time:  166.715669631958
user  600 time:  204.08498406410217
user  700 time:  236.41760969161987
user  800 time:  274.7695610523224
user  900 time:  309.5159845352173
user  1000 time:  341.44000267982483
user  1100 time:  380.76666235923767
user  1200 time:  415.5201835632324
user  1300 time:  446.1699175834656
user  1400 time:  474.68067955970764
user  1500 time:  506.80258226394653
user  1600 time:  545.3354046344757
user  1700 time:  579.0368256568909
user  1800 time:  609.975056886673
user  1900 time:  642.6762354373932
start updating user and item embedding...
user_name:2000
user  0 time:  1.1205673217773438e-05
user  100 time:  18.95870280265808
user  200 time:  37.31534194946289
user  300 time:  56.194063663482666
user  400 time:  74.66850709915161
user  500 time:  93.42522764205933
user  600 time:  112.40970206260681
user  700 time:  131.01119923591614
user  800 time:  149.72193503379822
user  900 time:  168.61825299263
user  1000 time:  187.5824315547943
user  1100 time:  206.25986051559448
user  1200 time:  224.94989228248596
user  1300 time:  243.22506642341614
user  1400 time:  261.90857124328613
user  1500 time:  280.63905024528503
user  1600 time:  299.6268689632416
user  1700 time:  318.63801169395447
user  1800 time:  337.1440668106079
user  1900 time:  356.2097501754761
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 176.3975365406077, train time: 104.34992933273315
epoch: 1, training loss: 107.23540755119757, train time: 104.50583052635193
epoch: 2, training loss: 95.02705741190584, train time: 104.37375092506409
epoch: 3, training loss: 91.14370150032482, train time: 104.42833614349365
epoch: 4, training loss: 86.41659557295498, train time: 103.75084781646729
epoch: 5, training loss: 81.61333033612755, train time: 103.41758918762207
epoch: 6, training loss: 79.43301260563021, train time: 104.03436493873596
epoch: 7, training loss: 76.88343201074895, train time: 103.86610627174377
epoch: 8, training loss: 76.82751912312233, train time: 103.55206274986267
epoch: 9, training loss: 72.67254283459624, train time: 103.47182512283325
epoch: 10, training loss: 74.02485670189344, train time: 103.34494876861572
epoch: 11, training loss: 72.25627324001834, train time: 111.08984279632568
epoch: 12, training loss: 70.90980825883526, train time: 118.56465411186218
epoch: 13, training loss: 68.76885285956087, train time: 118.49698519706726
epoch: 14, training loss: 70.27814808025141, train time: 118.85112524032593
epoch: 15, training loss: 69.65615101704316, train time: 118.81283044815063
epoch: 16, training loss: 68.59999504142252, train time: 118.82556819915771
epoch: 17, training loss: 66.40812613639719, train time: 118.83465909957886
epoch: 18, training loss: 67.01547491399106, train time: 118.80529642105103
epoch: 19, training loss: 65.86182023393485, train time: 118.85578536987305
epoch: 20, training loss: 66.60932943868465, train time: 118.47542238235474
epoch: 21, training loss: 63.98177505340573, train time: 118.76147437095642
epoch: 22, training loss: 64.17135526428683, train time: 118.71314525604248
epoch: 23, training loss: 63.22121875402445, train time: 118.65369296073914
epoch: 24, training loss: 62.39224508096959, train time: 118.63202595710754
epoch: 25, training loss: 61.90164801412902, train time: 118.51997184753418
epoch: 26, training loss: 64.52898448606356, train time: 118.53073167800903
epoch: 27, training loss: 62.2484053820117, train time: 118.60871505737305
epoch: 28, training loss: 61.814490457294596, train time: 118.62934136390686
epoch: 29, training loss: 63.11492377948889, train time: 118.23550868034363
epo:29 | HR@5:0.8589 | HR@10:0.8797 | HR@20:0.9083 | NDCG@5:0.3289 | NDCG@10:0.3757 | NDCG@20:0.4335 | recall@5:0.6262 | recall@10:0.7203 | recall@20:0.7471 | precision@5:0.7515 | precision@10:0.4322 | precision@20:0.2241 | best_HR@5:0.8589 | best_HR@10:0.8797 | best_HR@20:0.9083 | best_NDCG@5:0.3289 | best_NDCG@10:0.3757 | best_NDCG@20:0.4335 | best_recall@5:0.6262 | best_recall@10:0.7203 | best_recall@20:0.7471 | best_precision@5:0.7515 | best_precision@10:0.4322 | best_precision@20:0.2241 | 
epoch: 30, training loss: 62.37752442334022, train time: 112.0060646533966
epoch: 31, training loss: 63.82864326005074, train time: 109.45099425315857
epoch: 32, training loss: 61.41510648819167, train time: 105.49670815467834
epoch: 33, training loss: 62.64513275536228, train time: 114.10106635093689
epoch: 34, training loss: 59.235934035685204, train time: 118.33278012275696
epoch: 35, training loss: 62.15615423605777, train time: 118.42122888565063
epoch: 36, training loss: 66.28237080025428, train time: 118.29910778999329
epoch: 37, training loss: 61.7072047045076, train time: 118.51216673851013
epoch: 38, training loss: 61.49353431041891, train time: 132.35129499435425
epoch: 39, training loss: 62.877449729920045, train time: 143.69757986068726
epoch: 40, training loss: 62.209209713983, train time: 144.43046259880066
epoch: 41, training loss: 62.057722244251636, train time: 144.30817484855652
epoch: 42, training loss: 61.01370031995975, train time: 144.65441608428955
epoch: 43, training loss: 62.96985629201663, train time: 144.82542753219604
epoch: 44, training loss: 63.08020536256663, train time: 144.4195351600647
epoch: 45, training loss: 62.6332397636761, train time: 143.9262454509735
epoch: 46, training loss: 60.67115123015901, train time: 144.76379489898682
epoch: 47, training loss: 61.743592290185916, train time: 144.9761197566986
epoch: 48, training loss: 60.105462500047, train time: 144.32550024986267
epoch: 49, training loss: 60.5756778879113, train time: 143.79976511001587
epoch: 50, training loss: 61.93109944839307, train time: 144.54628491401672
epoch: 51, training loss: 61.1068253359299, train time: 144.15587854385376
epoch: 52, training loss: 60.32284421417535, train time: 144.6235237121582
epoch: 53, training loss: 62.07386155899803, train time: 144.5658040046692
epoch: 54, training loss: 60.223229418406845, train time: 144.65123414993286
epoch: 55, training loss: 61.015881134339, train time: 142.14658093452454
epoch: 56, training loss: 61.246563889159006, train time: 122.94207835197449
epoch: 57, training loss: 59.61752016990067, train time: 131.7711386680603
epoch: 58, training loss: 62.194472150938964, train time: 127.03457951545715
epoch: 59, training loss: 61.376802259412216, train time: 130.6644411087036
epo:59 | HR@5:0.8458 | HR@10:0.8684 | HR@20:0.9009 | NDCG@5:0.3084 | NDCG@10:0.3502 | NDCG@20:0.4032 | recall@5:0.6167 | recall@10:0.7071 | recall@20:0.7378 | precision@5:0.7400 | precision@10:0.4242 | precision@20:0.2213 | best_HR@5:0.8589 | best_HR@10:0.8797 | best_HR@20:0.9083 | best_NDCG@5:0.3289 | best_NDCG@10:0.3757 | best_NDCG@20:0.4335 | best_recall@5:0.6262 | best_recall@10:0.7203 | best_recall@20:0.7471 | best_precision@5:0.7515 | best_precision@10:0.4322 | best_precision@20:0.2241 | 
epoch: 60, training loss: 59.838012424523185, train time: 140.82074809074402
epoch: 61, training loss: 58.382208163642645, train time: 132.95157098770142
epoch: 62, training loss: 57.33713333932246, train time: 131.80133199691772
epoch: 63, training loss: 59.416553766110155, train time: 130.3030183315277
epoch: 64, training loss: 59.23665717358017, train time: 132.01175212860107
epoch: 65, training loss: 57.28703620722081, train time: 127.428884267807
epoch: 66, training loss: 56.82730705574795, train time: 143.12556529045105
epoch: 67, training loss: 59.255173882072995, train time: 144.68037152290344
epoch: 68, training loss: 58.202814503496484, train time: 145.02982139587402
epoch: 69, training loss: 58.56858708794607, train time: 144.7355306148529
epoch: 70, training loss: 60.448984873950394, train time: 144.8751184940338
epoch: 71, training loss: 59.887576518765854, train time: 145.09390497207642
epoch: 72, training loss: 60.17330269666127, train time: 145.84243035316467
epoch: 73, training loss: 59.63263196297339, train time: 145.43865728378296
epoch: 74, training loss: 57.09318015327517, train time: 145.94594383239746
epoch: 75, training loss: 59.23247398079184, train time: 145.3439962863922
epoch: 76, training loss: 57.49768948657038, train time: 144.12676429748535
epoch: 77, training loss: 59.14824770748237, train time: 144.64629554748535
epoch: 78, training loss: 60.73433701707472, train time: 138.53450226783752
epoch: 79, training loss: 58.825512054554565, train time: 127.40651273727417
epoch: 80, training loss: 59.67111419118737, train time: 131.6437726020813
epoch: 81, training loss: 57.37598872165472, train time: 125.25854706764221
epoch: 82, training loss: 58.27262730364964, train time: 130.62522196769714
epoch: 83, training loss: 58.440724690648494, train time: 127.42162442207336
epoch: 84, training loss: 59.80781863715674, train time: 144.47264409065247
epoch: 85, training loss: 62.20411685004365, train time: 144.988365650177
epoch: 86, training loss: 61.404903630682384, train time: 143.9771819114685
epoch: 87, training loss: 61.42842505962108, train time: 143.8187770843506
epoch: 88, training loss: 62.57471604390594, train time: 144.19555854797363
epoch: 89, training loss: 63.6756789720057, train time: 143.3942744731903
epo:89 | HR@5:0.8544 | HR@10:0.8740 | HR@20:0.9032 | NDCG@5:0.2542 | NDCG@10:0.2960 | NDCG@20:0.3512 | recall@5:0.6183 | recall@10:0.7125 | recall@20:0.7431 | precision@5:0.7420 | precision@10:0.4275 | precision@20:0.2229 | best_HR@5:0.8589 | best_HR@10:0.8797 | best_HR@20:0.9083 | best_NDCG@5:0.3289 | best_NDCG@10:0.3757 | best_NDCG@20:0.4335 | best_recall@5:0.6262 | best_recall@10:0.7203 | best_recall@20:0.7471 | best_precision@5:0.7515 | best_precision@10:0.4322 | best_precision@20:0.2241 | 
epoch: 90, training loss: 60.174255593445196, train time: 126.58366107940674
epoch: 91, training loss: 59.97823615172456, train time: 133.20094871520996
epoch: 92, training loss: 59.479211274639965, train time: 128.49950170516968
epoch: 93, training loss: 60.92398659826722, train time: 136.69635367393494
epoch: 94, training loss: 61.90556623719567, train time: 144.13448333740234
epoch: 95, training loss: 60.11508150483132, train time: 144.0148527622223
epoch: 96, training loss: 60.663561181099794, train time: 144.62457275390625
epoch: 97, training loss: 60.289520892816654, train time: 144.04687333106995
epoch: 98, training loss: 61.057935928576626, train time: 144.19028639793396
epoch: 99, training loss: 63.20753909078849, train time: 143.8721787929535
epoch: 100, training loss: 60.252007671719184, train time: 139.90811419487
epoch: 101, training loss: 60.672937436498614, train time: 128.8332395553589
epoch: 102, training loss: 61.20675090437362, train time: 128.57921361923218
epoch: 103, training loss: 60.76022379708593, train time: 129.328959941864
epoch: 104, training loss: 62.83370795721203, train time: 123.98437714576721
epoch: 105, training loss: 61.67585783524373, train time: 124.12516975402832
epoch: 106, training loss: 60.16671208045227, train time: 143.96422147750854
epoch: 107, training loss: 60.87379662233434, train time: 144.44471287727356
epoch: 108, training loss: 61.371472130145776, train time: 143.60474610328674
epoch: 109, training loss: 61.881727265345035, train time: 152.90051794052124
epoch: 110, training loss: 60.899075491615804, train time: 273.8667516708374
epoch: 111, training loss: 58.9973534581477, train time: 292.70431685447693
epoch: 112, training loss: 61.118652859688154, train time: 292.38100814819336
epoch: 113, training loss: 61.40838296727816, train time: 292.0663049221039
epoch: 114, training loss: 61.61360927348869, train time: 293.4987995624542
epoch: 115, training loss: 60.853312751180056, train time: 291.31084752082825
epoch: 116, training loss: 60.98430450799424, train time: 291.5439450740814
epoch: 117, training loss: 60.817003673091676, train time: 289.76973509788513
epoch: 118, training loss: 62.951817357901746, train time: 290.6823661327362
epoch: 119, training loss: 63.56410000231335, train time: 293.1832764148712
epo:119 | HR@5:0.8343 | HR@10:0.8553 | HR@20:0.8856 | NDCG@5:0.2004 | NDCG@10:0.2415 | NDCG@20:0.2988 | recall@5:0.6115 | recall@10:0.6967 | recall@20:0.7252 | precision@5:0.7338 | precision@10:0.4180 | precision@20:0.2175 | best_HR@5:0.8589 | best_HR@10:0.8797 | best_HR@20:0.9083 | best_NDCG@5:0.3289 | best_NDCG@10:0.3757 | best_NDCG@20:0.4335 | best_recall@5:0.6262 | best_recall@10:0.7203 | best_recall@20:0.7471 | best_precision@5:0.7515 | best_precision@10:0.4322 | best_precision@20:0.2241 | 
epoch: 120, training loss: 60.76590606707032, train time: 280.6629683971405
epoch: 121, training loss: 59.50907631359223, train time: 279.77913308143616
epoch: 122, training loss: 62.518988175172126, train time: 286.33222460746765
epoch: 123, training loss: 63.359334787979606, train time: 291.4057893753052
epoch: 124, training loss: 63.31215858046926, train time: 283.0857949256897
epoch: 125, training loss: 62.629286328476155, train time: 265.8589951992035
epoch: 126, training loss: 61.24730502394232, train time: 266.30164098739624
epoch: 127, training loss: 61.64674576928883, train time: 265.1960747241974
epoch: 128, training loss: 61.91717841880018, train time: 227.96018767356873
epoch: 129, training loss: 60.38760006780649, train time: 217.50840067863464
epoch: 130, training loss: 61.2125215837259, train time: 217.00995874404907
epoch: 131, training loss: 60.36418104854965, train time: 215.80385303497314
epoch: 132, training loss: 60.28930923891312, train time: 216.21116304397583
epoch: 133, training loss: 60.340130194581434, train time: 216.41594743728638
epoch: 134, training loss: 62.04323725424183, train time: 216.18209624290466
epoch: 135, training loss: 61.61523717109958, train time: 216.2451992034912
epoch: 136, training loss: 63.58710150812476, train time: 217.55142903327942
epoch: 137, training loss: 61.72087240178735, train time: 214.48831629753113
epoch: 138, training loss: 59.813148139724944, train time: 216.04962158203125
epoch: 139, training loss: 61.18908085463772, train time: 215.970538854599
epoch: 140, training loss: 62.26692972508681, train time: 216.33661890029907
epoch: 141, training loss: 64.1730095411258, train time: 216.68815326690674
epoch: 142, training loss: 62.58242966291982, train time: 216.11600995063782
epoch: 143, training loss: 63.36161000579159, train time: 216.78619408607483
epoch: 144, training loss: 61.98338529108332, train time: 215.34274220466614
epoch: 145, training loss: 62.720987213415356, train time: 216.48577547073364
epoch: 146, training loss: 63.335829379429924, train time: 216.76254892349243
epoch: 147, training loss: 65.13033105325303, train time: 215.95062232017517
epoch: 148, training loss: 63.446940574511245, train time: 216.40506076812744
epoch: 149, training loss: 64.54621644558574, train time: 216.2215757369995
epo:149 | HR@5:0.8379 | HR@10:0.8551 | HR@20:0.8854 | NDCG@5:0.1446 | NDCG@10:0.1770 | NDCG@20:0.2229 | recall@5:0.6122 | recall@10:0.6986 | recall@20:0.7277 | precision@5:0.7347 | precision@10:0.4191 | precision@20:0.2183 | best_HR@5:0.8589 | best_HR@10:0.8797 | best_HR@20:0.9083 | best_NDCG@5:0.3289 | best_NDCG@10:0.3757 | best_NDCG@20:0.4335 | best_recall@5:0.6262 | best_recall@10:0.7203 | best_recall@20:0.7471 | best_precision@5:0.7515 | best_precision@10:0.4322 | best_precision@20:0.2241 | 
training finish
