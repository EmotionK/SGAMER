nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.5762786865234375e-06
user  100 time:  224.4278905391693
user  200 time:  472.1458520889282
user  300 time:  781.6499688625336
user  400 time:  1111.3671352863312
user  500 time:  1443.6773533821106
user  600 time:  1772.9094789028168
user  700 time:  2102.107624053955
user  800 time:  2432.2154936790466
user  900 time:  2767.846138238907
user  1000 time:  3096.6835463047028
user  1100 time:  3424.361979484558
user  1200 time:  3756.107805967331
user  1300 time:  4086.8012914657593
user  1400 time:  4415.52854681015
start training item-item instance self attention module...
user  0 time:  6.67572021484375e-06
user  100 time:  276.5110499858856
user  200 time:  548.6359820365906
user  300 time:  828.3700351715088
user  400 time:  1111.1903913021088
user  500 time:  1387.7701828479767
user  600 time:  1644.2863221168518
user  700 time:  1929.038510799408
user  800 time:  2192.855013847351
user  900 time:  2463.7524638175964
user  1000 time:  2717.9810762405396
user  1100 time:  2985.284840106964
user  1200 time:  3265.436821460724
user  1300 time:  3553.252744436264
user  1400 time:  3832.704153060913
start updating user and item embedding...
user_name:1450
user  0 time:  1.6689300537109375e-05
user  100 time:  99.20241475105286
user  200 time:  198.99781274795532
user  300 time:  298.30666875839233
user  400 time:  398.5466628074646
user  500 time:  497.96571111679077
user  600 time:  597.6351191997528
user  700 time:  697.9101929664612
user  800 time:  797.4509696960449
user  900 time:  896.7462916374207
user  1000 time:  995.3429861068726
user  1100 time:  1093.8772492408752
user  1200 time:  1193.271099805832
user  1300 time:  1292.1885268688202
user  1400 time:  1391.465840101242
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 149.1463164097513, train time: 61.931708097457886
epoch: 1, training loss: 87.89054032036802, train time: 60.79983854293823
epoch: 2, training loss: 74.36423123473651, train time: 61.819156885147095
epoch: 3, training loss: 66.28131502914766, train time: 61.08642768859863
epoch: 4, training loss: 61.63451676622208, train time: 61.61765122413635
epoch: 5, training loss: 57.28379375912482, train time: 61.783817291259766
epoch: 6, training loss: 54.39409956158488, train time: 61.7250771522522
epoch: 7, training loss: 51.00591176328453, train time: 61.77990746498108
epoch: 8, training loss: 48.542181784505374, train time: 61.292593240737915
epoch: 9, training loss: 45.89002315963444, train time: 61.43467664718628
epoch: 10, training loss: 44.11958382229932, train time: 61.5526978969574
epoch: 11, training loss: 41.81052816133888, train time: 61.71793985366821
epoch: 12, training loss: 40.45792282131879, train time: 62.03565692901611
epoch: 13, training loss: 38.84672168188263, train time: 62.033896684646606
epoch: 14, training loss: 37.16305352773634, train time: 61.81012511253357
epoch: 15, training loss: 35.179487917725055, train time: 61.79451894760132
epoch: 16, training loss: 33.259689935723145, train time: 61.802998542785645
epoch: 17, training loss: 31.6166852130591, train time: 61.151249408721924
epoch: 18, training loss: 30.337321490264003, train time: 61.62622880935669
epoch: 19, training loss: 29.004863375568675, train time: 61.717430114746094
epoch: 20, training loss: 27.4912465747002, train time: 61.6007239818573
epoch: 21, training loss: 26.451604443136603, train time: 61.59316372871399
epoch: 22, training loss: 26.138034794084888, train time: 61.37360596656799
epoch: 23, training loss: 24.169208228548086, train time: 61.598244190216064
epoch: 24, training loss: 23.301943937036413, train time: 61.72779440879822
epoch: 25, training loss: 21.416442976700637, train time: 61.96611928939819
epoch: 26, training loss: 21.388641287013343, train time: 61.77307415008545
epoch: 27, training loss: 20.765199726780338, train time: 61.72623538970947
epoch: 28, training loss: 19.662474855474102, train time: 61.86695575714111
epoch: 29, training loss: 19.7520861182511, train time: 61.45167398452759
epo:29 | HR@5:0.7901 | HR@10:0.8347 | HR@20:0.8778 | NDCG@5:0.4462 | NDCG@10:0.4858 | NDCG@20:0.5362 | recall@5:0.5600 | recall@10:0.6660 | recall@20:0.7163 | precision@5:0.6720 | precision@10:0.3996 | precision@20:0.2149 | best_HR@5:0.7901 | best_HR@10:0.8347 | best_HR@20:0.8778 | best_NDCG@5:0.4462 | best_NDCG@10:0.4858 | best_NDCG@20:0.5362 | best_recall@5:0.5600 | best_recall@10:0.6660 | best_recall@20:0.7163 | best_precision@5:0.6720 | best_precision@10:0.3996 | best_precision@20:0.2149 | 
epoch: 30, training loss: 18.888361531024202, train time: 61.9588508605957
epoch: 31, training loss: 19.029635843945016, train time: 73.69877433776855
epoch: 32, training loss: 17.39524135563397, train time: 74.2096917629242
epoch: 33, training loss: 16.71548885242464, train time: 73.57205510139465
epoch: 34, training loss: 17.66163318197232, train time: 73.31488847732544
epoch: 35, training loss: 16.355625232013608, train time: 73.60748410224915
epoch: 36, training loss: 16.70478000493722, train time: 72.2965259552002
epoch: 37, training loss: 16.112865960455565, train time: 71.88817143440247
epoch: 38, training loss: 15.562732601114476, train time: 73.43573594093323
epoch: 39, training loss: 16.24974166732045, train time: 73.20346117019653
epoch: 40, training loss: 14.876219505842073, train time: 73.40295767784119
epoch: 41, training loss: 15.66679033011792, train time: 73.63618731498718
epoch: 42, training loss: 14.714460295057961, train time: 73.84184718132019
epoch: 43, training loss: 14.22376209093477, train time: 74.1452283859253
epoch: 44, training loss: 14.44967728439724, train time: 73.47012329101562
epoch: 45, training loss: 13.973898966733486, train time: 73.18482947349548
epoch: 46, training loss: 13.80195724650548, train time: 74.14842009544373
epoch: 47, training loss: 13.949026571239756, train time: 73.73641228675842
epoch: 48, training loss: 13.74267863015848, train time: 74.18022274971008
epoch: 49, training loss: 12.921778111803633, train time: 73.68418526649475
epoch: 50, training loss: 13.004411812580884, train time: 73.81103014945984
epoch: 51, training loss: 13.759308004408922, train time: 74.45813298225403
epoch: 52, training loss: 13.176270188676654, train time: 73.70928645133972
epoch: 53, training loss: 12.266014729347603, train time: 74.20039391517639
epoch: 54, training loss: 13.55199345339679, train time: 73.48280215263367
epoch: 55, training loss: 12.318042017810058, train time: 74.23321032524109
epoch: 56, training loss: 12.43772906909635, train time: 73.34048986434937
epoch: 57, training loss: 11.953656934730589, train time: 73.99468445777893
epoch: 58, training loss: 11.002946906244006, train time: 73.50561428070068
epoch: 59, training loss: 12.590579451396934, train time: 74.22473740577698
epo:59 | HR@5:0.7711 | HR@10:0.8132 | HR@20:0.8636 | NDCG@5:0.4652 | NDCG@10:0.5027 | NDCG@20:0.5508 | recall@5:0.5498 | recall@10:0.6503 | recall@20:0.7033 | precision@5:0.6597 | precision@10:0.3902 | precision@20:0.2110 | best_HR@5:0.7901 | best_HR@10:0.8347 | best_HR@20:0.8778 | best_NDCG@5:0.4652 | best_NDCG@10:0.5027 | best_NDCG@20:0.5508 | best_recall@5:0.5600 | best_recall@10:0.6660 | best_recall@20:0.7163 | best_precision@5:0.6720 | best_precision@10:0.3996 | best_precision@20:0.2149 | 
epoch: 60, training loss: 11.886899696965884, train time: 74.6441798210144
epoch: 61, training loss: 12.453035215990894, train time: 74.48080372810364
epoch: 62, training loss: 11.769994013632186, train time: 73.46057152748108
epoch: 63, training loss: 11.563733240593592, train time: 74.38077688217163
epoch: 64, training loss: 12.451073637750824, train time: 73.5135977268219
epoch: 65, training loss: 11.721240732187653, train time: 74.13467836380005
epoch: 66, training loss: 10.894596907765163, train time: 74.1324315071106
epoch: 67, training loss: 12.042387488225927, train time: 73.96593523025513
epoch: 68, training loss: 11.550678920899827, train time: 74.15280175209045
epoch: 69, training loss: 10.929743576469491, train time: 73.5279815196991
epoch: 70, training loss: 11.00674671363356, train time: 73.62444949150085
epoch: 71, training loss: 10.224971163728583, train time: 74.1393301486969
epoch: 72, training loss: 10.740328514249995, train time: 74.0079128742218
epoch: 73, training loss: 10.906962975447186, train time: 74.3019950389862
epoch: 74, training loss: 10.99429642844541, train time: 73.59583926200867
epoch: 75, training loss: 11.170051673010335, train time: 73.76340103149414
epoch: 76, training loss: 10.309959140492765, train time: 73.6662666797638
epoch: 77, training loss: 10.432369286466837, train time: 74.10349416732788
epoch: 78, training loss: 12.035555466838105, train time: 74.11254119873047
epoch: 79, training loss: 10.492962764545041, train time: 74.07801342010498
epoch: 80, training loss: 10.46597928885285, train time: 74.12780237197876
epoch: 81, training loss: 10.666214417826325, train time: 73.89644193649292
epoch: 82, training loss: 9.454131422819387, train time: 74.4261040687561
epoch: 83, training loss: 10.434222908281924, train time: 73.71595597267151
epoch: 84, training loss: 10.664375972354946, train time: 74.24318814277649
epoch: 85, training loss: 9.625839241965082, train time: 74.04275465011597
epoch: 86, training loss: 10.197780997242205, train time: 73.99041199684143
epoch: 87, training loss: 9.785921218038766, train time: 74.30461621284485
epoch: 88, training loss: 10.455324096961363, train time: 73.91863131523132
epoch: 89, training loss: 11.127594522306936, train time: 73.98780941963196
epo:89 | HR@5:0.7632 | HR@10:0.8020 | HR@20:0.8452 | NDCG@5:0.4817 | NDCG@10:0.5175 | NDCG@20:0.5638 | recall@5:0.5469 | recall@10:0.6439 | recall@20:0.6897 | precision@5:0.6563 | precision@10:0.3863 | precision@20:0.2069 | best_HR@5:0.7901 | best_HR@10:0.8347 | best_HR@20:0.8778 | best_NDCG@5:0.4817 | best_NDCG@10:0.5175 | best_NDCG@20:0.5638 | best_recall@5:0.5600 | best_recall@10:0.6660 | best_recall@20:0.7163 | best_precision@5:0.6720 | best_precision@10:0.3996 | best_precision@20:0.2149 | 
epoch: 90, training loss: 10.039991167116796, train time: 58.273295640945435
epoch: 91, training loss: 10.038233121872622, train time: 60.30992245674133
epoch: 92, training loss: 9.615013414750365, train time: 60.93606162071228
epoch: 93, training loss: 9.474531937741972, train time: 60.76915240287781
epoch: 94, training loss: 11.247270947803145, train time: 61.02516722679138
epoch: 95, training loss: 9.613835007970465, train time: 61.01800489425659
epoch: 96, training loss: 9.608390336480738, train time: 60.96814751625061
epoch: 97, training loss: 10.03080913981239, train time: 61.22435450553894
epoch: 98, training loss: 9.674610898592505, train time: 60.894461154937744
epoch: 99, training loss: 9.408082623463201, train time: 61.194302797317505
epoch: 100, training loss: 10.40725692443209, train time: 60.837197065353394
epoch: 101, training loss: 9.438045755061921, train time: 60.723292112350464
epoch: 102, training loss: 9.773435280388867, train time: 60.669474840164185
epoch: 103, training loss: 9.364616253935083, train time: 60.51058077812195
epoch: 104, training loss: 9.703012373740194, train time: 60.98783469200134
epoch: 105, training loss: 8.958371462002674, train time: 61.097155809402466
epoch: 106, training loss: 9.719619105196557, train time: 60.9900848865509
epoch: 107, training loss: 9.189072623701804, train time: 61.1264283657074
epoch: 108, training loss: 9.261100876093451, train time: 60.478062868118286
epoch: 109, training loss: 9.41651884932378, train time: 60.777672290802
epoch: 110, training loss: 10.138072081939413, train time: 60.77268314361572
epoch: 111, training loss: 8.892590990334384, train time: 61.04723024368286
epoch: 112, training loss: 9.770826132445563, train time: 60.79389190673828
epoch: 113, training loss: 9.190582618782457, train time: 61.217687129974365
epoch: 114, training loss: 9.12152023928013, train time: 61.017847537994385
epoch: 115, training loss: 9.22660661196062, train time: 60.97065544128418
epoch: 116, training loss: 8.81816283094031, train time: 60.50697445869446
epoch: 117, training loss: 9.070038817064187, train time: 60.96362233161926
epoch: 118, training loss: 9.594161884186008, train time: 61.02973294258118
epoch: 119, training loss: 8.059770364385258, train time: 60.88355588912964
epo:119 | HR@5:0.7383 | HR@10:0.7793 | HR@20:0.8298 | NDCG@5:0.4853 | NDCG@10:0.5205 | NDCG@20:0.5662 | recall@5:0.5308 | recall@10:0.6207 | recall@20:0.6745 | precision@5:0.6370 | precision@10:0.3724 | precision@20:0.2023 | best_HR@5:0.7901 | best_HR@10:0.8347 | best_HR@20:0.8778 | best_NDCG@5:0.4853 | best_NDCG@10:0.5205 | best_NDCG@20:0.5662 | best_recall@5:0.5600 | best_recall@10:0.6660 | best_recall@20:0.7163 | best_precision@5:0.6720 | best_precision@10:0.3996 | best_precision@20:0.2149 | 
epoch: 120, training loss: 8.718482993572678, train time: 58.49655103683472
epoch: 121, training loss: 9.866284086989708, train time: 60.04134559631348
epoch: 122, training loss: 8.70117892077235, train time: 60.89189577102661
epoch: 123, training loss: 8.932907723025096, train time: 61.05570864677429
epoch: 124, training loss: 9.037871114323764, train time: 60.94682216644287
epoch: 125, training loss: 9.311524039761395, train time: 60.95898938179016
epoch: 126, training loss: 8.531103551038086, train time: 61.070037841796875
epoch: 127, training loss: 8.647352900033525, train time: 60.9307324886322
epoch: 128, training loss: 9.042397264027557, train time: 60.83136487007141
epoch: 129, training loss: 8.934780723905646, train time: 61.16599202156067
epoch: 130, training loss: 8.75318477484177, train time: 60.93394756317139
epoch: 131, training loss: 8.594413159806663, train time: 60.94707775115967
epoch: 132, training loss: 7.78972720721265, train time: 60.75372910499573
epoch: 133, training loss: 8.926984375670912, train time: 61.26858711242676
epoch: 134, training loss: 7.868028562346581, train time: 60.876781940460205
epoch: 135, training loss: 8.502736282596857, train time: 61.10165214538574
epoch: 136, training loss: 8.69066582861359, train time: 61.24345541000366
epoch: 137, training loss: 8.08122129290507, train time: 60.97891592979431
epoch: 138, training loss: 7.676407903595191, train time: 60.74628925323486
epoch: 139, training loss: 8.36961906592569, train time: 61.10522651672363
epoch: 140, training loss: 8.253852301602024, train time: 61.05261707305908
epoch: 141, training loss: 9.43881124028536, train time: 60.7383074760437
epoch: 142, training loss: 7.9011589992581435, train time: 61.16461157798767
epoch: 143, training loss: 8.461417699480876, train time: 61.04861879348755
epoch: 144, training loss: 8.157370266808584, train time: 60.866132974624634
epoch: 145, training loss: 7.810872985803655, train time: 61.01145315170288
epoch: 146, training loss: 8.583762588413038, train time: 61.45237755775452
epoch: 147, training loss: 8.354122943397442, train time: 61.29985475540161
epoch: 148, training loss: 7.6592911209937995, train time: 61.281134605407715
epoch: 149, training loss: 8.455855506604905, train time: 60.991331338882446
epo:149 | HR@5:0.7446 | HR@10:0.7828 | HR@20:0.8348 | NDCG@5:0.4771 | NDCG@10:0.5136 | NDCG@20:0.5605 | recall@5:0.5355 | recall@10:0.6289 | recall@20:0.6774 | precision@5:0.6426 | precision@10:0.3773 | precision@20:0.2032 | best_HR@5:0.7901 | best_HR@10:0.8347 | best_HR@20:0.8778 | best_NDCG@5:0.4853 | best_NDCG@10:0.5205 | best_NDCG@20:0.5662 | best_recall@5:0.5600 | best_recall@10:0.6660 | best_recall@20:0.7163 | best_precision@5:0.6720 | best_precision@10:0.3996 | best_precision@20:0.2149 | 
training finish
