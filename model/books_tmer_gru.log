nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Books......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  1.0013580322265625e-05
user  100 time:  312.59223198890686
user  200 time:  627.3302140235901
user  300 time:  947.1585209369659
user  400 time:  1270.35134267807
user  500 time:  1593.7558164596558
user  600 time:  1923.174881696701
user  700 time:  2254.2000381946564
user  800 time:  2585.7050790786743
user  900 time:  2923.8104083538055
user  1000 time:  3253.3140926361084
user  1100 time:  3587.4483585357666
user  1200 time:  3920.9988145828247
user  1300 time:  4258.64945936203
user  1400 time:  4591.0919506549835
user  1500 time:  4893.507154941559
user  1600 time:  5163.236532926559
user  1700 time:  5431.412968158722
user  1800 time:  5704.796046972275
user  1900 time:  5977.09836935997
start training item-item instance self attention module...
user  0 time:  6.198883056640625e-06
user  100 time:  47.38227415084839
user  200 time:  93.89376974105835
user  300 time:  141.39601111412048
user  400 time:  191.0834562778473
user  500 time:  236.91989421844482
user  600 time:  282.9210991859436
user  700 time:  331.5329110622406
user  800 time:  378.52317214012146
user  900 time:  423.7621924877167
user  1000 time:  476.14276337623596
user  1100 time:  522.3360702991486
user  1200 time:  573.6105527877808
user  1300 time:  626.3759126663208
user  1400 time:  675.0074226856232
user  1500 time:  717.6904346942902
user  1600 time:  764.3683183193207
user  1700 time:  810.4849517345428
user  1800 time:  859.251091003418
user  1900 time:  901.4100775718689
start updating user and item embedding...
user_name:2000
user  0 time:  1.0013580322265625e-05
user  100 time:  18.719286680221558
user  200 time:  37.22912859916687
user  300 time:  55.80956768989563
user  400 time:  74.81375885009766
user  500 time:  93.51490521430969
user  600 time:  112.0318672657013
user  700 time:  130.47837567329407
user  800 time:  149.16987586021423
user  900 time:  167.90265727043152
user  1000 time:  186.60289525985718
user  1100 time:  204.78829050064087
user  1200 time:  223.44901752471924
user  1300 time:  242.07314729690552
user  1400 time:  260.87033891677856
user  1500 time:  279.2446177005768
user  1600 time:  297.6947252750397
user  1700 time:  316.0553367137909
user  1800 time:  334.23137879371643
user  1900 time:  352.43732714653015
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 156.66336696199141, train time: 76.14957070350647
epoch: 1, training loss: 86.13840957119828, train time: 75.8388741016388
epoch: 2, training loss: 72.3268761135405, train time: 75.93076753616333
epoch: 3, training loss: 63.33788356544392, train time: 75.66355347633362
epoch: 4, training loss: 58.452860411052825, train time: 89.22091674804688
epoch: 5, training loss: 53.01970395511307, train time: 89.93397831916809
epoch: 6, training loss: 49.928234803985106, train time: 90.03717184066772
epoch: 7, training loss: 47.44968881587192, train time: 90.34309935569763
epoch: 8, training loss: 45.04740638027215, train time: 90.40212488174438
epoch: 9, training loss: 42.93869216422172, train time: 89.94852232933044
epoch: 10, training loss: 41.22261966086444, train time: 90.63726854324341
epoch: 11, training loss: 39.08736730063174, train time: 89.99714040756226
epoch: 12, training loss: 37.758975424152595, train time: 90.20777583122253
epoch: 13, training loss: 35.99452223446315, train time: 90.35918617248535
epoch: 14, training loss: 35.63586005722573, train time: 90.50561666488647
epoch: 15, training loss: 32.702314687387116, train time: 90.23951458930969
epoch: 16, training loss: 33.246800633532985, train time: 90.64761638641357
epoch: 17, training loss: 32.08976072463338, train time: 90.37113618850708
epoch: 18, training loss: 30.583676801735237, train time: 90.13164854049683
epoch: 19, training loss: 31.010040518111055, train time: 90.43067526817322
epoch: 20, training loss: 29.86796128422975, train time: 90.18140268325806
epoch: 21, training loss: 29.066180159992655, train time: 90.09251880645752
epoch: 22, training loss: 28.084916439863264, train time: 90.48070168495178
epoch: 23, training loss: 28.009799139798815, train time: 90.59721875190735
epoch: 24, training loss: 27.36656875308654, train time: 90.66217684745789
epoch: 25, training loss: 26.651784384525854, train time: 90.75365614891052
epoch: 26, training loss: 26.65844623614612, train time: 91.23445582389832
epoch: 27, training loss: 26.085853509021035, train time: 90.71501755714417
epoch: 28, training loss: 25.02888921009162, train time: 90.67949104309082
epoch: 29, training loss: 25.290648317288287, train time: 90.32279443740845
epo:29 | HR@5:0.8505 | HR@10:0.8782 | HR@20:0.9097 | NDCG@5:0.4461 | NDCG@10:0.4871 | NDCG@20:0.5382 | recall@5:0.6055 | recall@10:0.7102 | recall@20:0.7444 | precision@5:0.7266 | precision@10:0.4261 | precision@20:0.2233 | best_HR@5:0.8505 | best_HR@10:0.8782 | best_HR@20:0.9097 | best_NDCG@5:0.4461 | best_NDCG@10:0.4871 | best_NDCG@20:0.5382 | best_recall@5:0.6055 | best_recall@10:0.7102 | best_recall@20:0.7444 | best_precision@5:0.7266 | best_precision@10:0.4261 | best_precision@20:0.2233 | 
epoch: 30, training loss: 24.503703916499944, train time: 91.07767820358276
epoch: 31, training loss: 23.99248715489466, train time: 90.82629299163818
epoch: 32, training loss: 24.38593995096562, train time: 90.4788646697998
epoch: 33, training loss: 24.8379962414283, train time: 90.74198985099792
epoch: 34, training loss: 22.796248436259702, train time: 91.29248189926147
epoch: 35, training loss: 23.06579963898821, train time: 90.69966053962708
epoch: 36, training loss: 23.285791656759102, train time: 90.92425990104675
epoch: 37, training loss: 22.371383019570203, train time: 90.55576229095459
epoch: 38, training loss: 21.398855717109655, train time: 84.66831350326538
epoch: 39, training loss: 22.790673530697404, train time: 85.08793330192566
epoch: 40, training loss: 22.022261497451836, train time: 87.39960217475891
epoch: 41, training loss: 21.536761208943517, train time: 86.82238149642944
epoch: 42, training loss: 21.727966636534802, train time: 85.08211541175842
epoch: 43, training loss: 21.158990958268078, train time: 84.96616125106812
epoch: 44, training loss: 20.89796678733819, train time: 86.31398034095764
epoch: 45, training loss: 21.111458103067434, train time: 88.55134010314941
epoch: 46, training loss: 20.196096283711995, train time: 90.69847249984741
epoch: 47, training loss: 20.10525276639737, train time: 90.30171823501587
epoch: 48, training loss: 19.260790339112646, train time: 90.56125116348267
epoch: 49, training loss: 19.506130164269052, train time: 92.83521938323975
epoch: 50, training loss: 20.575450016722698, train time: 96.37379217147827
epoch: 51, training loss: 20.160912156062295, train time: 94.79537844657898
epoch: 52, training loss: 20.052886042557475, train time: 91.81552743911743
epoch: 53, training loss: 20.264346658457725, train time: 98.42461657524109
epoch: 54, training loss: 18.847806919145114, train time: 96.63504600524902
epoch: 55, training loss: 19.221144253996954, train time: 93.60803174972534
epoch: 56, training loss: 21.650401045512808, train time: 84.09231472015381
epoch: 57, training loss: 20.010541971240855, train time: 78.57151222229004
epoch: 58, training loss: 20.625098029339824, train time: 78.21307325363159
epoch: 59, training loss: 18.293057324372057, train time: 78.57835745811462
epo:59 | HR@5:0.8487 | HR@10:0.8756 | HR@20:0.9075 | NDCG@5:0.4420 | NDCG@10:0.4831 | NDCG@20:0.5346 | recall@5:0.6042 | recall@10:0.7111 | recall@20:0.7467 | precision@5:0.7250 | precision@10:0.4266 | precision@20:0.2240 | best_HR@5:0.8505 | best_HR@10:0.8782 | best_HR@20:0.9097 | best_NDCG@5:0.4461 | best_NDCG@10:0.4871 | best_NDCG@20:0.5382 | best_recall@5:0.6055 | best_recall@10:0.7111 | best_recall@20:0.7467 | best_precision@5:0.7266 | best_precision@10:0.4266 | best_precision@20:0.2240 | 
epoch: 60, training loss: 19.853333964215835, train time: 78.50081133842468
epoch: 61, training loss: 20.925767422520494, train time: 78.4900848865509
epoch: 62, training loss: 19.609845380438855, train time: 78.16021776199341
epoch: 63, training loss: 19.316075057510716, train time: 78.38280773162842
epoch: 64, training loss: 19.126343346111753, train time: 78.2512719631195
epoch: 65, training loss: 18.434546100224907, train time: 78.32607460021973
epoch: 66, training loss: 18.964539810954193, train time: 78.66891598701477
epoch: 67, training loss: 18.729526020620142, train time: 78.39696788787842
epoch: 68, training loss: 18.527103750780043, train time: 78.458575963974
epoch: 69, training loss: 21.211015282350672, train time: 78.31536912918091
epoch: 70, training loss: 18.33481874529207, train time: 79.35556101799011
epoch: 71, training loss: 19.555053156952454, train time: 78.41354250907898
epoch: 72, training loss: 19.045847695547536, train time: 78.26180648803711
epoch: 73, training loss: 20.345462775238957, train time: 78.19585108757019
epoch: 74, training loss: 18.654761040582343, train time: 78.56295943260193
epoch: 75, training loss: 17.35944294361161, train time: 78.22025346755981
epoch: 76, training loss: 18.75559740376184, train time: 78.45600724220276
epoch: 77, training loss: 19.17016526509633, train time: 78.36207175254822
epoch: 78, training loss: 18.399069230554232, train time: 78.66500520706177
epoch: 79, training loss: 18.83461077965785, train time: 72.46824932098389
epoch: 80, training loss: 18.54781405025642, train time: 69.54178357124329
epoch: 81, training loss: 18.23389006936577, train time: 69.49512767791748
epoch: 82, training loss: 20.37267957100903, train time: 69.58152318000793
epoch: 83, training loss: 18.84827428787412, train time: 69.43461036682129
epoch: 84, training loss: 18.69597695752691, train time: 69.55606603622437
epoch: 85, training loss: 18.26182992570375, train time: 69.50237250328064
epoch: 86, training loss: 18.841609459228494, train time: 69.57673907279968
epoch: 87, training loss: 18.66983922914551, train time: 72.24517297744751
epoch: 88, training loss: 18.589526076386846, train time: 78.21625804901123
epoch: 89, training loss: 17.424330410843595, train time: 78.41962218284607
epo:89 | HR@5:0.8323 | HR@10:0.8587 | HR@20:0.8924 | NDCG@5:0.4337 | NDCG@10:0.4772 | NDCG@20:0.5306 | recall@5:0.5958 | recall@10:0.6958 | recall@20:0.7306 | precision@5:0.7150 | precision@10:0.4175 | precision@20:0.2192 | best_HR@5:0.8505 | best_HR@10:0.8782 | best_HR@20:0.9097 | best_NDCG@5:0.4461 | best_NDCG@10:0.4871 | best_NDCG@20:0.5382 | best_recall@5:0.6055 | best_recall@10:0.7111 | best_recall@20:0.7467 | best_precision@5:0.7266 | best_precision@10:0.4266 | best_precision@20:0.2240 | 
epoch: 90, training loss: 17.94646884400163, train time: 78.23135876655579
epoch: 91, training loss: 17.384181514484226, train time: 78.23907732963562
epoch: 92, training loss: 19.044618516211358, train time: 78.45339751243591
epoch: 93, training loss: 16.68597906177456, train time: 78.42813777923584
epoch: 94, training loss: 19.455380904819776, train time: 78.1814968585968
epoch: 95, training loss: 17.146629298327753, train time: 78.49402499198914
epoch: 96, training loss: 19.6221505366384, train time: 78.22232866287231
epoch: 97, training loss: 16.697006770025382, train time: 78.4404559135437
epoch: 98, training loss: 17.35284750177607, train time: 78.39204144477844
epoch: 99, training loss: 17.154683340216252, train time: 78.529132604599
epoch: 100, training loss: 16.608956582582323, train time: 78.34677052497864
epoch: 101, training loss: 16.160889618339752, train time: 78.39128828048706
epoch: 102, training loss: 17.6763390406295, train time: 78.15877270698547
epoch: 103, training loss: 18.413334181469963, train time: 78.37164568901062
epoch: 104, training loss: 17.76962507696635, train time: 78.28235697746277
epoch: 105, training loss: 16.194732774908516, train time: 78.55638194084167
epoch: 106, training loss: 16.391996407344777, train time: 78.19419407844543
epoch: 107, training loss: 16.118122231162033, train time: 78.29846835136414
epoch: 108, training loss: 16.898802844066267, train time: 78.32680821418762
epoch: 109, training loss: 16.860829276815366, train time: 78.36295557022095
epoch: 110, training loss: 15.966598148159392, train time: 78.45612263679504
epoch: 111, training loss: 16.24037710922721, train time: 78.16830658912659
epoch: 112, training loss: 16.216692427168255, train time: 78.42461323738098
epoch: 113, training loss: 16.473457114854455, train time: 78.4621946811676
epoch: 114, training loss: 16.084840282467667, train time: 78.44297194480896
epoch: 115, training loss: 17.099420965902027, train time: 78.23845839500427
epoch: 116, training loss: 17.48511238345509, train time: 78.49846887588501
epoch: 117, training loss: 17.03242209358268, train time: 78.23718786239624
epoch: 118, training loss: 17.13765703648187, train time: 78.39678382873535
epoch: 119, training loss: 17.57139793515853, train time: 78.61042666435242
epo:119 | HR@5:0.8181 | HR@10:0.8420 | HR@20:0.8800 | NDCG@5:0.4360 | NDCG@10:0.4784 | NDCG@20:0.5305 | recall@5:0.5867 | recall@10:0.6835 | recall@20:0.7211 | precision@5:0.7040 | precision@10:0.4101 | precision@20:0.2163 | best_HR@5:0.8505 | best_HR@10:0.8782 | best_HR@20:0.9097 | best_NDCG@5:0.4461 | best_NDCG@10:0.4871 | best_NDCG@20:0.5382 | best_recall@5:0.6055 | best_recall@10:0.7111 | best_recall@20:0.7467 | best_precision@5:0.7266 | best_precision@10:0.4266 | best_precision@20:0.2240 | 
epoch: 120, training loss: 17.62913123912176, train time: 68.39334845542908
epoch: 121, training loss: 16.899999349536756, train time: 69.3735044002533
epoch: 122, training loss: 16.320747008586295, train time: 78.32898807525635
epoch: 123, training loss: 18.697484354280732, train time: 78.49120664596558
epoch: 124, training loss: 18.060969899795964, train time: 78.37663149833679
epoch: 125, training loss: 16.268885421744017, train time: 78.30048847198486
epoch: 126, training loss: 16.293628698135308, train time: 78.29870557785034
epoch: 127, training loss: 16.806616366262915, train time: 78.22006011009216
epoch: 128, training loss: 16.42124179648954, train time: 78.4706552028656
epoch: 129, training loss: 18.405649252728836, train time: 78.4282009601593
epoch: 130, training loss: 18.184174999594234, train time: 78.24128794670105
epoch: 131, training loss: 17.162401021801088, train time: 78.21259713172913
epoch: 132, training loss: 18.241505783356843, train time: 78.4788384437561
epoch: 133, training loss: 17.046679518255587, train time: 78.56162214279175
epoch: 134, training loss: 18.592821424050726, train time: 78.49530601501465
epoch: 135, training loss: 16.63763667778403, train time: 78.41434168815613
epoch: 136, training loss: 16.672247639257876, train time: 78.52538633346558
epoch: 137, training loss: 18.171325122424605, train time: 78.41499543190002
epoch: 138, training loss: 16.805761552152944, train time: 78.19721031188965
epoch: 139, training loss: 16.69046358783953, train time: 78.58253693580627
epoch: 140, training loss: 17.889086729318933, train time: 78.56208729743958
epoch: 141, training loss: 16.26815444055501, train time: 78.74054074287415
epoch: 142, training loss: 16.509722821174364, train time: 78.4138822555542
epoch: 143, training loss: 16.514679460986827, train time: 78.27843117713928
epoch: 144, training loss: 16.39697627222904, train time: 78.2263674736023
epoch: 145, training loss: 16.236309030342454, train time: 78.22326421737671
epoch: 146, training loss: 16.97442723100403, train time: 78.68250942230225
epoch: 147, training loss: 15.948103110076318, train time: 78.58017230033875
epoch: 148, training loss: 16.02497931518178, train time: 78.35790967941284
epoch: 149, training loss: 17.92586484361982, train time: 78.33763551712036
epo:149 | HR@5:0.8212 | HR@10:0.8452 | HR@20:0.8795 | NDCG@5:0.4208 | NDCG@10:0.4631 | NDCG@20:0.5159 | recall@5:0.5911 | recall@10:0.6860 | recall@20:0.7216 | precision@5:0.7093 | precision@10:0.4116 | precision@20:0.2165 | best_HR@5:0.8505 | best_HR@10:0.8782 | best_HR@20:0.9097 | best_NDCG@5:0.4461 | best_NDCG@10:0.4871 | best_NDCG@20:0.5382 | best_recall@5:0.6055 | best_recall@10:0.7111 | best_recall@20:0.7467 | best_precision@5:0.7266 | best_precision@10:0.4266 | best_precision@20:0.2240 | 
training finish
