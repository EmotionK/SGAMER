nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.0994415283203125e-06
user  100 time:  237.3889937400818
user  200 time:  476.5291602611542
user  300 time:  716.3640565872192
user  400 time:  960.4800589084625
user  500 time:  1204.913608789444
user  600 time:  1452.9312274456024
user  700 time:  1702.7659056186676
user  800 time:  1952.1511597633362
user  900 time:  2203.027995109558
user  1000 time:  2455.95557308197
user  1100 time:  2708.641443014145
user  1200 time:  2957.5979838371277
user  1300 time:  3209.4185178279877
user  1400 time:  3459.733564376831
start training item-item instance self attention module...
user  0 time:  1.0728836059570312e-05
user  100 time:  220.74944257736206
user  200 time:  441.74510431289673
user  300 time:  657.6088871955872
user  400 time:  872.3300743103027
user  500 time:  1089.2796528339386
user  600 time:  1306.176299571991
user  700 time:  1522.3334696292877
user  800 time:  1732.1206221580505
user  900 time:  1948.0826761722565
user  1000 time:  2160.9361383914948
user  1100 time:  2368.560269355774
user  1200 time:  2582.543699979782
user  1300 time:  2796.0081703662872
user  1400 time:  3006.654628276825
start updating user and item embedding...
user_name:1450
user  0 time:  1.33514404296875e-05
user  100 time:  16.963614463806152
user  200 time:  33.69713616371155
user  300 time:  50.35728311538696
user  400 time:  66.71419954299927
user  500 time:  83.29427528381348
user  600 time:  99.78636193275452
user  700 time:  116.13960385322571
user  800 time:  132.65614366531372
user  900 time:  148.9283275604248
user  1000 time:  165.53215765953064
user  1100 time:  182.06648516654968
user  1200 time:  198.57965731620789
user  1300 time:  215.58096742630005
user  1400 time:  232.35633254051208
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 188.8761475691863, train time: 23.648957014083862
epoch: 1, training loss: 114.08155334976618, train time: 22.677968978881836
epoch: 2, training loss: 97.07146683942119, train time: 22.952712774276733
epoch: 3, training loss: 87.32848010717134, train time: 23.14590287208557
epoch: 4, training loss: 80.3848567417881, train time: 22.6590895652771
epoch: 5, training loss: 74.31591511313309, train time: 22.936325073242188
epoch: 6, training loss: 69.52219393583073, train time: 22.662184476852417
epoch: 7, training loss: 66.82983957313263, train time: 22.826906204223633
epoch: 8, training loss: 63.040889699957916, train time: 23.72074818611145
epoch: 9, training loss: 59.32845332044235, train time: 23.198826551437378
epoch: 10, training loss: 57.41360734817863, train time: 23.018136739730835
epoch: 11, training loss: 55.27435487032926, train time: 22.825803995132446
epoch: 12, training loss: 52.30204950241023, train time: 22.900971174240112
epoch: 13, training loss: 50.181719034237176, train time: 23.248462915420532
epoch: 14, training loss: 48.46462233873899, train time: 23.256398677825928
epoch: 15, training loss: 46.39377582236557, train time: 23.24631977081299
epoch: 16, training loss: 44.88791798495731, train time: 22.814510345458984
epoch: 17, training loss: 42.16508612523103, train time: 22.590080499649048
epoch: 18, training loss: 40.4600415937457, train time: 22.888899564743042
epoch: 19, training loss: 38.22141320663377, train time: 22.98563861846924
epoch: 20, training loss: 36.916425656376305, train time: 22.83470630645752
epoch: 21, training loss: 35.833869135161876, train time: 23.08405089378357
epoch: 22, training loss: 33.874740393490356, train time: 22.77751064300537
epoch: 23, training loss: 33.485037830336296, train time: 22.934269666671753
epoch: 24, training loss: 31.810542542532858, train time: 22.54771089553833
epoch: 25, training loss: 31.51495551294829, train time: 22.802111864089966
epoch: 26, training loss: 29.75282971357501, train time: 22.791390657424927
epoch: 27, training loss: 28.728038443772675, train time: 22.79366445541382
epoch: 28, training loss: 28.59166305637268, train time: 23.151084423065186
epoch: 29, training loss: 26.880714176311812, train time: 22.81681752204895
epoch: 30, training loss: 27.050534077474367, train time: 22.346264362335205
epoch: 31, training loss: 26.24471676102894, train time: 22.963855981826782
epoch: 32, training loss: 25.49296658278149, train time: 23.023443698883057
epoch: 33, training loss: 25.851561265589226, train time: 22.548065423965454
epoch: 34, training loss: 24.164428827184565, train time: 23.151459217071533
epoch: 35, training loss: 24.836913836102212, train time: 22.890270948410034
epoch: 36, training loss: 23.91017682847405, train time: 22.66209888458252
epoch: 37, training loss: 22.495231556104955, train time: 22.838042736053467
epoch: 38, training loss: 23.191406828913387, train time: 22.93795394897461
epoch: 39, training loss: 23.035978839431664, train time: 22.619192123413086
epoch: 40, training loss: 22.612430058245536, train time: 23.105856895446777
epoch: 41, training loss: 20.121831807122362, train time: 22.656089544296265
epoch: 42, training loss: 21.20185174593871, train time: 22.759478092193604
epoch: 43, training loss: 20.779604788586084, train time: 23.070956230163574
epoch: 44, training loss: 21.09656832691985, train time: 22.786254167556763
epoch: 45, training loss: 20.000483359664486, train time: 23.004475355148315
epoch: 46, training loss: 21.016295883267503, train time: 22.836756229400635
epoch: 47, training loss: 19.329207735711634, train time: 22.884321928024292
epoch: 48, training loss: 19.87873100552042, train time: 22.443563222885132
epoch: 49, training loss: 19.962730357689452, train time: 22.816569328308105
epo:49|HR@1:0.6129 | HR@5:0.7785 | HR@10:0.8190 | HR@20:0.8659 | HR@50:0.9332 | NDCG@1:0.3993 | NDCG@5:0.4683 | NDCG@10:0.5057| NDCG@20:0.5539| NDCG@50:0.6519| best_HR@1:0.6129 | best_HR@5:0.7785 | best_HR@10:0.8190 | best_HR@20:0.8659 | best_HR@50:0.9332 | best_NDCG@1:0.3993 | best_NDCG@5:0.4683 | best_NDCG@10:0.5057 | best_NDCG@20:0.5539 | best_NDCG@50:0.6519 | train_time:22.82 | test_time:400.91
epoch: 50, training loss: 18.21698826406987, train time: 23.244385480880737
epoch: 51, training loss: 19.300524806538647, train time: 22.510181188583374
epoch: 52, training loss: 20.140923206090974, train time: 22.420257329940796
epoch: 53, training loss: 17.949255255747403, train time: 23.113856315612793
epoch: 54, training loss: 18.41580891304352, train time: 22.627817630767822
epoch: 55, training loss: 17.870434097462066, train time: 22.833499908447266
epoch: 56, training loss: 18.71288624267447, train time: 22.750481367111206
epoch: 57, training loss: 19.014723683349303, train time: 22.73629093170166
epoch: 58, training loss: 18.36852254543942, train time: 22.505513191223145
epoch: 59, training loss: 16.925792746134675, train time: 22.205199241638184
epoch: 60, training loss: 18.31775778682345, train time: 22.673354864120483
epoch: 61, training loss: 16.910044974988523, train time: 22.678810596466064
epoch: 62, training loss: 17.392476092761626, train time: 22.834903955459595
epoch: 63, training loss: 16.38954375525759, train time: 22.769258737564087
epoch: 64, training loss: 18.06425236377447, train time: 22.834152936935425
epoch: 65, training loss: 17.347131792234904, train time: 22.912190437316895
epoch: 66, training loss: 16.41973849112003, train time: 22.615310668945312
epoch: 67, training loss: 17.41540784424342, train time: 22.36491560935974
epoch: 68, training loss: 16.871867823835714, train time: 23.209557056427002
epoch: 69, training loss: 16.583408489435783, train time: 22.496607303619385
epoch: 70, training loss: 17.487205439793797, train time: 21.99398183822632
epoch: 71, training loss: 15.193065337432131, train time: 22.884596824645996
epoch: 72, training loss: 16.990813065059456, train time: 22.775561332702637
epoch: 73, training loss: 15.058251839023114, train time: 23.072031021118164
epoch: 74, training loss: 17.15458725034773, train time: 22.956923723220825
epoch: 75, training loss: 17.46630557004164, train time: 22.47717785835266
epoch: 76, training loss: 15.851267583816934, train time: 22.25341033935547
epoch: 77, training loss: 15.587422072002028, train time: 22.70464038848877
epoch: 78, training loss: 15.907090551816395, train time: 22.856890201568604
epoch: 79, training loss: 16.616637125366765, train time: 22.37530541419983
epoch: 80, training loss: 15.7988145786328, train time: 22.676183700561523
epoch: 81, training loss: 14.739395915341902, train time: 22.69322919845581
epoch: 82, training loss: 15.648322769691958, train time: 22.77588415145874
epoch: 83, training loss: 16.16985816971146, train time: 22.691012859344482
epoch: 84, training loss: 15.118475014553724, train time: 22.444294452667236
epoch: 85, training loss: 15.3438072682859, train time: 22.305251836776733
epoch: 86, training loss: 14.470051382004499, train time: 22.462324857711792
epoch: 87, training loss: 15.20885394327206, train time: 22.490099668502808
epoch: 88, training loss: 14.523998140371077, train time: 22.919572591781616
epoch: 89, training loss: 15.187346290755613, train time: 22.994120597839355
epoch: 90, training loss: 15.01250250119017, train time: 22.63450336456299
epoch: 91, training loss: 13.884952623394952, train time: 22.731772422790527
epoch: 92, training loss: 15.441435395919484, train time: 22.43230628967285
epoch: 93, training loss: 14.163403940268267, train time: 22.241315603256226
epoch: 94, training loss: 14.601225746829073, train time: 23.087624073028564
epoch: 95, training loss: 14.453569293603437, train time: 22.896036386489868
epoch: 96, training loss: 13.968713248094105, train time: 22.582698106765747
epoch: 97, training loss: 13.86932988724243, train time: 22.69940710067749
epoch: 98, training loss: 14.260362165098059, train time: 22.543848037719727
epoch: 99, training loss: 14.420076770980586, train time: 22.717684745788574
epo:99|HR@1:0.5900 | HR@5:0.7525 | HR@10:0.7909 | HR@20:0.8382 | HR@50:0.9221 | NDCG@1:0.4131 | NDCG@5:0.4798 | NDCG@10:0.5165| NDCG@20:0.5636| NDCG@50:0.6593| best_HR@1:0.6129 | best_HR@5:0.7785 | best_HR@10:0.8190 | best_HR@20:0.8659 | best_HR@50:0.9332 | best_NDCG@1:0.4131 | best_NDCG@5:0.4798 | best_NDCG@10:0.5165 | best_NDCG@20:0.5636 | best_NDCG@50:0.6593 | train_time:22.72 | test_time:400.65
training finish
