nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  8.344650268554688e-06
user  100 time:  183.5689914226532
user  200 time:  367.47627902030945
user  300 time:  551.6962306499481
user  400 time:  738.1105031967163
user  500 time:  925.3505389690399
user  600 time:  1109.9647951126099
user  700 time:  1294.580722808838
user  800 time:  1480.2950007915497
user  900 time:  1667.825679063797
user  1000 time:  1853.7024292945862
user  1100 time:  2039.3150923252106
user  1200 time:  2227.1243097782135
user  1300 time:  2413.687979221344
user  1400 time:  2600.5601727962494
start training item-item instance self attention module...
user  0 time:  5.4836273193359375e-06
user  100 time:  132.9772186279297
user  200 time:  265.9328157901764
user  300 time:  402.3980519771576
user  400 time:  540.1720857620239
user  500 time:  675.2031483650208
user  600 time:  800.3926403522491
user  700 time:  939.690744638443
user  800 time:  1069.0276849269867
user  900 time:  1202.7334597110748
user  1000 time:  1328.566603422165
user  1100 time:  1460.5796782970428
user  1200 time:  1598.8785145282745
user  1300 time:  1740.902009010315
user  1400 time:  1878.9629714488983
start updating user and item embedding...
user_name:1450
user  0 time:  9.059906005859375e-06
user  100 time:  15.73677682876587
user  200 time:  31.648399591445923
user  300 time:  47.427061557769775
user  400 time:  63.26055455207825
user  500 time:  78.99904179573059
user  600 time:  94.5908260345459
user  700 time:  110.4991979598999
user  800 time:  126.24337410926819
user  900 time:  142.60784697532654
user  1000 time:  158.5595190525055
user  1100 time:  174.38967275619507
user  1200 time:  190.58567214012146
user  1300 time:  206.57414150238037
user  1400 time:  222.45105648040771
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 150.1970812632353, train time: 28.100820064544678
epoch: 1, training loss: 87.79292646076647, train time: 30.459492444992065
epoch: 2, training loss: 74.80793111409002, train time: 28.82277512550354
epoch: 3, training loss: 66.09641019393166, train time: 28.600621223449707
epoch: 4, training loss: 61.415380264035775, train time: 28.053441286087036
epoch: 5, training loss: 57.0192708678951, train time: 28.096763372421265
epoch: 6, training loss: 53.901352903587394, train time: 27.85299849510193
epoch: 7, training loss: 50.93431147667434, train time: 27.86554479598999
epoch: 8, training loss: 48.60189635676943, train time: 27.85230779647827
epoch: 9, training loss: 46.3316095688715, train time: 27.649840593338013
epoch: 10, training loss: 43.96401130497543, train time: 27.163299560546875
epoch: 11, training loss: 42.24416586170992, train time: 27.04656410217285
epoch: 12, training loss: 40.88948236451688, train time: 27.109328746795654
epoch: 13, training loss: 38.6026350230095, train time: 27.09842014312744
epoch: 14, training loss: 37.67193428501196, train time: 27.27055311203003
epoch: 15, training loss: 35.328598003332445, train time: 27.339287996292114
epoch: 16, training loss: 34.39350284148168, train time: 27.115666151046753
epoch: 17, training loss: 32.419364533314365, train time: 27.199798345565796
epoch: 18, training loss: 30.94445829637334, train time: 27.248146057128906
epoch: 19, training loss: 29.392547335835843, train time: 27.199143648147583
epoch: 20, training loss: 28.0073077649904, train time: 27.155303478240967
epoch: 21, training loss: 27.21799104702768, train time: 27.19852066040039
epoch: 22, training loss: 26.20093161655859, train time: 27.097411632537842
epoch: 23, training loss: 25.195669592245395, train time: 27.09600520133972
epoch: 24, training loss: 22.186040277278153, train time: 27.157917499542236
epoch: 25, training loss: 22.921121899524223, train time: 27.171730995178223
epoch: 26, training loss: 21.8663960462909, train time: 27.065526008605957
epoch: 27, training loss: 21.562019600948588, train time: 27.092663049697876
epoch: 28, training loss: 19.38095202164004, train time: 27.16623878479004
epoch: 29, training loss: 19.991266846897815, train time: 26.97370147705078
epo:29 | HR@5:0.8025 | HR@10:0.8448 | HR@20:0.8882 | NDCG@5:0.4540 | NDCG@10:0.4921 | NDCG@20:0.5413 | recall@5:0.5645 | recall@10:0.6756 | recall@20:0.7251 | precision@5:0.6774 | precision@10:0.4054 | precision@20:0.2175 | best_HR@5:0.8025 | best_HR@10:0.8448 | best_HR@20:0.8882 | best_NDCG@5:0.4540 | best_NDCG@10:0.4921 | best_NDCG@20:0.5413 | best_recall@5:0.5645 | best_recall@10:0.6756 | best_recall@20:0.7251 | best_precision@5:0.6774 | best_precision@10:0.4054 | best_precision@20:0.2175 | 
epoch: 30, training loss: 19.428279489237866, train time: 27.14934277534485
epoch: 31, training loss: 18.593642576748607, train time: 27.220176219940186
epoch: 32, training loss: 18.197182583293397, train time: 27.16486644744873
epoch: 33, training loss: 18.450976401376465, train time: 27.28084897994995
epoch: 34, training loss: 17.9640884580931, train time: 27.268532276153564
epoch: 35, training loss: 15.996954213630488, train time: 27.202889919281006
epoch: 36, training loss: 18.05650377410666, train time: 27.431397199630737
epoch: 37, training loss: 15.30081673048926, train time: 27.088509798049927
epoch: 38, training loss: 16.074929938875357, train time: 27.17315363883972
epoch: 39, training loss: 15.048209689486384, train time: 27.418041229248047
epoch: 40, training loss: 15.780377327645056, train time: 27.119462966918945
epoch: 41, training loss: 15.41308571314903, train time: 27.05471181869507
epoch: 42, training loss: 13.662791582939235, train time: 27.272001266479492
epoch: 43, training loss: 15.000398105083832, train time: 27.247646808624268
epoch: 44, training loss: 14.551228899759053, train time: 27.253459215164185
epoch: 45, training loss: 15.165190369326865, train time: 27.15097999572754
epoch: 46, training loss: 14.032971566901324, train time: 27.28857684135437
epoch: 47, training loss: 13.155748124346928, train time: 27.156358003616333
epoch: 48, training loss: 12.893810886886968, train time: 27.138880968093872
epoch: 49, training loss: 13.758526940918955, train time: 27.073432683944702
epoch: 50, training loss: 13.618553460427393, train time: 27.07900309562683
epoch: 51, training loss: 12.736942663803575, train time: 27.23338508605957
epoch: 52, training loss: 13.337771982088725, train time: 27.226906538009644
epoch: 53, training loss: 12.629278065721337, train time: 27.091742515563965
epoch: 54, training loss: 13.27378225846337, train time: 27.269969701766968
epoch: 55, training loss: 12.88504331971103, train time: 27.224255800247192
epoch: 56, training loss: 12.27448357139474, train time: 27.26094651222229
epoch: 57, training loss: 12.739009263068965, train time: 27.203340530395508
epoch: 58, training loss: 12.294973275159919, train time: 27.276488304138184
epoch: 59, training loss: 12.456011365342192, train time: 27.15264916419983
epo:59 | HR@5:0.7801 | HR@10:0.8217 | HR@20:0.8687 | NDCG@5:0.4679 | NDCG@10:0.5042 | NDCG@20:0.5520 | recall@5:0.5567 | recall@10:0.6571 | recall@20:0.7070 | precision@5:0.6680 | precision@10:0.3943 | precision@20:0.2121 | best_HR@5:0.8025 | best_HR@10:0.8448 | best_HR@20:0.8882 | best_NDCG@5:0.4679 | best_NDCG@10:0.5042 | best_NDCG@20:0.5520 | best_recall@5:0.5645 | best_recall@10:0.6756 | best_recall@20:0.7251 | best_precision@5:0.6774 | best_precision@10:0.4054 | best_precision@20:0.2175 | 
epoch: 60, training loss: 12.644238471046947, train time: 27.03817582130432
epoch: 61, training loss: 12.133814730932727, train time: 27.165473222732544
epoch: 62, training loss: 12.064412886872447, train time: 27.2209632396698
epoch: 63, training loss: 13.275964775076659, train time: 27.21910309791565
epoch: 64, training loss: 11.304698038856941, train time: 27.1647367477417
epoch: 65, training loss: 12.906726375925132, train time: 27.179211854934692
epoch: 66, training loss: 11.143705256129238, train time: 27.18162488937378
epoch: 67, training loss: 12.555601556518923, train time: 27.273142337799072
epoch: 68, training loss: 11.013384558941198, train time: 27.430715799331665
epoch: 69, training loss: 11.54417997935775, train time: 27.305545806884766
epoch: 70, training loss: 11.082696557974941, train time: 27.1611647605896
epoch: 71, training loss: 11.88684682116002, train time: 27.264994621276855
epoch: 72, training loss: 10.982370962846971, train time: 27.10205864906311
epoch: 73, training loss: 10.578867809492749, train time: 27.195032119750977
epoch: 74, training loss: 11.71844913161874, train time: 27.219667673110962
epoch: 75, training loss: 11.073650607021023, train time: 27.416027545928955
epoch: 76, training loss: 11.027218552743534, train time: 27.54836916923523
epoch: 77, training loss: 10.919673562538264, train time: 27.30032444000244
epoch: 78, training loss: 11.532757613913873, train time: 27.20001983642578
epoch: 79, training loss: 9.841201221406322, train time: 27.052457332611084
epoch: 80, training loss: 11.808518111777516, train time: 27.22350573539734
epoch: 81, training loss: 10.330721386615323, train time: 27.20642900466919
epoch: 82, training loss: 11.254032283597326, train time: 27.12502646446228
epoch: 83, training loss: 11.20987633018217, train time: 27.10003423690796
epoch: 84, training loss: 11.153208916248673, train time: 27.07468819618225
epoch: 85, training loss: 9.93213779949133, train time: 27.25919532775879
epoch: 86, training loss: 10.058720523903162, train time: 27.089634656906128
epoch: 87, training loss: 10.28870314404935, train time: 27.16832208633423
epoch: 88, training loss: 11.025135626477436, train time: 27.1826012134552
epoch: 89, training loss: 10.07455871025752, train time: 27.2990243434906
epo:89 | HR@5:0.7646 | HR@10:0.8064 | HR@20:0.8492 | NDCG@5:0.4748 | NDCG@10:0.5115 | NDCG@20:0.5588 | recall@5:0.5428 | recall@10:0.6456 | recall@20:0.6922 | precision@5:0.6513 | precision@10:0.3874 | precision@20:0.2077 | best_HR@5:0.8025 | best_HR@10:0.8448 | best_HR@20:0.8882 | best_NDCG@5:0.4748 | best_NDCG@10:0.5115 | best_NDCG@20:0.5588 | best_recall@5:0.5645 | best_recall@10:0.6756 | best_recall@20:0.7251 | best_precision@5:0.6774 | best_precision@10:0.4054 | best_precision@20:0.2175 | 
epoch: 90, training loss: 8.881788740032334, train time: 27.241932153701782
epoch: 91, training loss: 10.53989972557838, train time: 27.220985889434814
epoch: 92, training loss: 9.49902811312603, train time: 27.15343689918518
epoch: 93, training loss: 9.924045605271687, train time: 27.251811027526855
epoch: 94, training loss: 10.318508054457084, train time: 27.04909586906433
epoch: 95, training loss: 10.196639965340637, train time: 27.070188522338867
epoch: 96, training loss: 8.999672036452068, train time: 27.101458311080933
epoch: 97, training loss: 9.94989203257569, train time: 27.38633370399475
epoch: 98, training loss: 9.336883039916529, train time: 27.024144411087036
epoch: 99, training loss: 9.403982038600532, train time: 26.948031187057495
epoch: 100, training loss: 10.475868455268426, train time: 27.15384817123413
epoch: 101, training loss: 10.176015330044265, train time: 27.13231873512268
epoch: 102, training loss: 10.050899568319323, train time: 27.140952825546265
epoch: 103, training loss: 9.89389799766468, train time: 27.137558937072754
epoch: 104, training loss: 9.602454479338348, train time: 27.12646198272705
epoch: 105, training loss: 8.95343993749566, train time: 27.275593280792236
epoch: 106, training loss: 8.796325099794217, train time: 27.218782663345337
epoch: 107, training loss: 9.547678788896292, train time: 27.00931191444397
epoch: 108, training loss: 8.925523003489161, train time: 27.07797646522522
epoch: 109, training loss: 9.128147669370435, train time: 27.145368814468384
epoch: 110, training loss: 9.721497301532622, train time: 27.16386365890503
epoch: 111, training loss: 8.554302365160424, train time: 27.09983468055725
epoch: 112, training loss: 9.103942978416853, train time: 27.025209426879883
epoch: 113, training loss: 9.589208356876554, train time: 27.219261407852173
epoch: 114, training loss: 9.170742438425236, train time: 27.009120225906372
epoch: 115, training loss: 9.95304047556624, train time: 27.19366955757141
epoch: 116, training loss: 8.107459187475683, train time: 27.18443489074707
epoch: 117, training loss: 9.794376601460613, train time: 27.233174324035645
epoch: 118, training loss: 8.853874777524425, train time: 27.114681482315063
epoch: 119, training loss: 8.662843701234237, train time: 27.143152952194214
epo:119 | HR@5:0.7571 | HR@10:0.7979 | HR@20:0.8413 | NDCG@5:0.4791 | NDCG@10:0.5144 | NDCG@20:0.5609 | recall@5:0.5467 | recall@10:0.6387 | recall@20:0.6846 | precision@5:0.6560 | precision@10:0.3832 | precision@20:0.2054 | best_HR@5:0.8025 | best_HR@10:0.8448 | best_HR@20:0.8882 | best_NDCG@5:0.4791 | best_NDCG@10:0.5144 | best_NDCG@20:0.5609 | best_recall@5:0.5645 | best_recall@10:0.6756 | best_recall@20:0.7251 | best_precision@5:0.6774 | best_precision@10:0.4054 | best_precision@20:0.2175 | 
epoch: 120, training loss: 9.072502048420404, train time: 27.022159814834595
epoch: 121, training loss: 8.329961154109697, train time: 27.32108759880066
epoch: 122, training loss: 9.069810448411147, train time: 27.177700996398926
epoch: 123, training loss: 9.198650073623526, train time: 27.30193281173706
epoch: 124, training loss: 8.575371805867803, train time: 27.287562131881714
epoch: 125, training loss: 9.765246303340234, train time: 27.19718074798584
epoch: 126, training loss: 8.607923116979975, train time: 27.051990509033203
epoch: 127, training loss: 7.66844340919306, train time: 26.969146728515625
epoch: 128, training loss: 8.964971789152798, train time: 27.0651113986969
epoch: 129, training loss: 8.200466816569474, train time: 27.149065256118774
epoch: 130, training loss: 8.511238158783613, train time: 27.389091968536377
epoch: 131, training loss: 9.364538178804992, train time: 28.547834873199463
epoch: 132, training loss: 8.108097623455308, train time: 28.767778158187866
epoch: 133, training loss: 8.417057798207452, train time: 29.795907497406006
epoch: 134, training loss: 8.357730051665044, train time: 29.425577402114868
epoch: 135, training loss: 7.383857022500081, train time: 28.490596055984497
epoch: 136, training loss: 9.517473003551913, train time: 27.111048936843872
epoch: 137, training loss: 8.635726283092282, train time: 27.185443878173828
epoch: 138, training loss: 9.034704184571638, train time: 27.202772855758667
epoch: 139, training loss: 8.350146480989451, train time: 27.215613842010498
epoch: 140, training loss: 7.455088672405509, train time: 27.426806688308716
epoch: 141, training loss: 8.980371706184087, train time: 30.75783395767212
epoch: 142, training loss: 8.543386577590752, train time: 27.83833122253418
epoch: 143, training loss: 8.618259784386112, train time: 27.40731143951416
epoch: 144, training loss: 8.685199676377124, train time: 27.162089824676514
epoch: 145, training loss: 8.696301183308833, train time: 27.261155366897583
epoch: 146, training loss: 8.26814742317464, train time: 27.67980122566223
epoch: 147, training loss: 7.382440969355514, train time: 27.469391584396362
epoch: 148, training loss: 9.812494648971835, train time: 27.301178455352783
epoch: 149, training loss: 8.168708318519123, train time: 27.67972445487976
epo:149 | HR@5:0.7684 | HR@10:0.8095 | HR@20:0.8529 | NDCG@5:0.4744 | NDCG@10:0.5111 | NDCG@20:0.5589 | recall@5:0.5464 | recall@10:0.6477 | recall@20:0.6949 | precision@5:0.6557 | precision@10:0.3886 | precision@20:0.2085 | best_HR@5:0.8025 | best_HR@10:0.8448 | best_HR@20:0.8882 | best_NDCG@5:0.4791 | best_NDCG@10:0.5144 | best_NDCG@20:0.5609 | best_recall@5:0.5645 | best_recall@10:0.6756 | best_recall@20:0.7251 | best_precision@5:0.6774 | best_precision@10:0.4054 | best_precision@20:0.2175 | 
training finish
