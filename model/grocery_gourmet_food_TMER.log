nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Grocery_Gourmet_Food......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.814697265625e-06
user  100 time:  187.36699628829956
user  200 time:  371.8537986278534
user  300 time:  562.7779884338379
user  400 time:  751.7685301303864
user  500 time:  939.5923194885254
user  600 time:  1122.3624448776245
user  700 time:  1303.347764968872
user  800 time:  1483.460860967636
user  900 time:  1673.064772605896
user  1000 time:  1852.2533648014069
user  1100 time:  2035.2506039142609
user  1200 time:  2235.5358440876007
user  1300 time:  2436.318598508835
user  1400 time:  2636.0688576698303
user  1500 time:  2830.6225233078003
user  1600 time:  3031.899243116379
user  1700 time:  3231.7745237350464
user  1800 time:  3432.8804185390472
user  1900 time:  3630.261771917343
start training item-item instance self attention module...
user  0 time:  7.3909759521484375e-06
user  100 time:  79.60292267799377
user  200 time:  143.17050623893738
user  300 time:  208.55728554725647
user  400 time:  287.3316652774811
user  500 time:  353.8991963863373
user  600 time:  426.0398817062378
user  700 time:  496.7455198764801
user  800 time:  574.3503868579865
user  900 time:  646.3371379375458
user  1000 time:  711.4900901317596
user  1100 time:  777.4805369377136
user  1200 time:  865.4726812839508
user  1300 time:  957.7750086784363
user  1400 time:  1041.0251200199127
user  1500 time:  1121.2581722736359
user  1600 time:  1200.280611038208
user  1700 time:  1285.1786365509033
user  1800 time:  1362.750759601593
user  1900 time:  1447.8108010292053
start updating user and item embedding...
user_name:2000
user  0 time:  1.1920928955078125e-05
user  100 time:  21.186639547348022
user  200 time:  42.08374285697937
user  300 time:  63.095202684402466
user  400 time:  84.05111384391785
user  500 time:  104.96624684333801
user  600 time:  126.16597723960876
user  700 time:  147.1376314163208
user  800 time:  168.38165068626404
user  900 time:  189.35991787910461
user  1000 time:  210.11938786506653
user  1100 time:  231.1583559513092
user  1200 time:  252.32839107513428
user  1300 time:  273.4357135295868
user  1400 time:  294.32383704185486
user  1500 time:  315.3405227661133
user  1600 time:  336.19375586509705
user  1700 time:  357.22510600090027
user  1800 time:  378.20742297172546
user  1900 time:  399.0634169578552
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 278.8052458719467, train time: 85.61407256126404
epoch: 1, training loss: 207.93129169917665, train time: 85.88848757743835
epoch: 2, training loss: 194.54278324573534, train time: 85.75807404518127
epoch: 3, training loss: 185.71426081872778, train time: 85.60209894180298
epoch: 4, training loss: 179.71109678721405, train time: 85.82359457015991
epoch: 5, training loss: 174.98456707687, train time: 85.70590448379517
epoch: 6, training loss: 170.93706374819158, train time: 85.71816635131836
epoch: 7, training loss: 168.3043400015158, train time: 85.76419591903687
epoch: 8, training loss: 166.99903362095938, train time: 85.74593091011047
epoch: 9, training loss: 164.22218734081252, train time: 85.6213915348053
epoch: 10, training loss: 161.2521164615464, train time: 85.40250420570374
epoch: 11, training loss: 159.66321294748923, train time: 85.82833552360535
epoch: 12, training loss: 159.51436574285617, train time: 85.67566967010498
epoch: 13, training loss: 156.54078643320827, train time: 85.5024151802063
epoch: 14, training loss: 155.85556211872608, train time: 85.80501961708069
epoch: 15, training loss: 154.28543116542278, train time: 85.9475564956665
epoch: 16, training loss: 155.4133777146344, train time: 85.74693536758423
epoch: 17, training loss: 153.66524576712982, train time: 85.5731430053711
epoch: 18, training loss: 152.52118453779258, train time: 85.4464225769043
epoch: 19, training loss: 151.01553522163886, train time: 85.66861414909363
epoch: 20, training loss: 152.05973670003004, train time: 85.68683981895447
epoch: 21, training loss: 151.5139616265078, train time: 85.45052528381348
epoch: 22, training loss: 150.61117487875163, train time: 85.47855567932129
epoch: 23, training loss: 149.08835607449873, train time: 85.5573980808258
epoch: 24, training loss: 147.83103694347665, train time: 85.67427062988281
epoch: 25, training loss: 148.45094769389834, train time: 82.638267993927
epoch: 26, training loss: 149.53554159709893, train time: 80.58063244819641
epoch: 27, training loss: 148.14848116433132, train time: 77.75538921356201
epoch: 28, training loss: 150.9194587637612, train time: 75.66436243057251
epoch: 29, training loss: 148.93707289648592, train time: 79.23588132858276
epo:29 | HR@5:0.7215 | HR@10:0.7630 | HR@20:0.8126 | NDCG@5:0.3544 | NDCG@10:0.3991 | NDCG@20:0.4554 | recall@5:0.5137 | recall@10:0.6091 | recall@20:0.6607 | precision@5:0.6165 | precision@10:0.3654 | precision@20:0.1982 | best_HR@5:0.7215 | best_HR@10:0.7630 | best_HR@20:0.8126 | best_NDCG@5:0.3544 | best_NDCG@10:0.3991 | best_NDCG@20:0.4554 | best_recall@5:0.5137 | best_recall@10:0.6091 | best_recall@20:0.6607 | best_precision@5:0.6165 | best_precision@10:0.3654 | best_precision@20:0.1982 | 
epoch: 30, training loss: 148.31632993850508, train time: 85.29427528381348
epoch: 31, training loss: 150.38175339512236, train time: 85.26677083969116
epoch: 32, training loss: 147.82910123346664, train time: 85.32144451141357
epoch: 33, training loss: 149.19049181899754, train time: 85.37238788604736
epoch: 34, training loss: 147.49451019536355, train time: 85.57353472709656
epoch: 35, training loss: 148.65234061237425, train time: 85.35371780395508
epoch: 36, training loss: 148.11524060287047, train time: 85.7873101234436
epoch: 37, training loss: 145.98812810261734, train time: 106.18676352500916
epoch: 38, training loss: 148.3946228820423, train time: 109.50974130630493
epoch: 39, training loss: 149.79936574328167, train time: 109.96811723709106
epoch: 40, training loss: 149.85146116669057, train time: 110.0818259716034
epoch: 41, training loss: 151.05682209791848, train time: 110.72247457504272
epoch: 42, training loss: 148.5539273135073, train time: 110.5457558631897
epoch: 43, training loss: 150.87042936772923, train time: 110.0204644203186
epoch: 44, training loss: 147.59704368117673, train time: 110.09091520309448
epoch: 45, training loss: 147.94118086568778, train time: 110.36956191062927
epoch: 46, training loss: 149.59087803667353, train time: 110.3512794971466
epoch: 47, training loss: 147.1846116248198, train time: 109.8582067489624
epoch: 48, training loss: 151.6100754338113, train time: 110.0263020992279
epoch: 49, training loss: 146.18045014649397, train time: 109.93411493301392
epoch: 50, training loss: 146.2274364157347, train time: 110.11689782142639
epoch: 51, training loss: 149.1308765861031, train time: 110.10254263877869
epoch: 52, training loss: 148.13591664054547, train time: 109.91628432273865
epoch: 53, training loss: 149.83228900050744, train time: 109.91177153587341
epoch: 54, training loss: 151.33193065992964, train time: 110.27048754692078
epoch: 55, training loss: 149.10734219840379, train time: 110.86324143409729
epoch: 56, training loss: 149.41429835509916, train time: 110.56791019439697
epoch: 57, training loss: 152.19026382468292, train time: 110.65476679801941
epoch: 58, training loss: 150.54238915455062, train time: 109.67385625839233
epoch: 59, training loss: 151.46133541644667, train time: 110.31272673606873
epo:59 | HR@5:0.7074 | HR@10:0.7421 | HR@20:0.7965 | NDCG@5:0.3469 | NDCG@10:0.3917 | NDCG@20:0.4484 | recall@5:0.5062 | recall@10:0.5990 | recall@20:0.6459 | precision@5:0.6074 | precision@10:0.3594 | precision@20:0.1938 | best_HR@5:0.7215 | best_HR@10:0.7630 | best_HR@20:0.8126 | best_NDCG@5:0.3544 | best_NDCG@10:0.3991 | best_NDCG@20:0.4554 | best_recall@5:0.5137 | best_recall@10:0.6091 | best_recall@20:0.6607 | best_precision@5:0.6165 | best_precision@10:0.3654 | best_precision@20:0.1982 | 
epoch: 60, training loss: 152.76919690224167, train time: 97.97222852706909
epoch: 61, training loss: 151.71018155192723, train time: 103.39411950111389
epoch: 62, training loss: 150.10842574673006, train time: 94.95410394668579
epoch: 63, training loss: 151.1100396597467, train time: 96.77563738822937
epoch: 64, training loss: 148.69398731342517, train time: 98.2192599773407
epoch: 65, training loss: 149.7881807618396, train time: 107.34120607376099
epoch: 66, training loss: 148.41585555986967, train time: 98.08354711532593
epoch: 67, training loss: 149.6583758929628, train time: 97.92483639717102
epoch: 68, training loss: 149.69938583399926, train time: 97.74139785766602
epoch: 69, training loss: 150.73609186176327, train time: 95.71363496780396
epoch: 70, training loss: 149.32004811395018, train time: 94.22323608398438
epoch: 71, training loss: 151.8302011496562, train time: 98.77741932868958
epoch: 72, training loss: 150.7475319783407, train time: 95.22541761398315
epoch: 73, training loss: 152.97206869901856, train time: 102.54975128173828
epoch: 74, training loss: 149.95549281779677, train time: 110.16222381591797
epoch: 75, training loss: 152.20631841613795, train time: 109.48031330108643
epoch: 76, training loss: 152.0600058316777, train time: 110.42737197875977
epoch: 77, training loss: 155.476765558662, train time: 110.1094286441803
epoch: 78, training loss: 153.3740049180924, train time: 110.34031939506531
epoch: 79, training loss: 155.6009733115061, train time: 109.85800647735596
epoch: 80, training loss: 153.4426815591578, train time: 109.80965232849121
epoch: 81, training loss: 154.37841228282196, train time: 110.3108184337616
epoch: 82, training loss: 158.09835668097367, train time: 110.24064350128174
epoch: 83, training loss: 155.12252460990567, train time: 110.40570306777954
epoch: 84, training loss: 154.92144575039856, train time: 110.11812233924866
epoch: 85, training loss: 152.4468341540196, train time: 109.99177193641663
epoch: 86, training loss: 156.98333234421443, train time: 110.11622214317322
epoch: 87, training loss: 154.30140733103326, train time: 110.1308069229126
epoch: 88, training loss: 156.53246428945567, train time: 110.29949879646301
epoch: 89, training loss: 156.41468816777342, train time: 110.16328001022339
epo:89 | HR@5:0.6876 | HR@10:0.7275 | HR@20:0.7818 | NDCG@5:0.3203 | NDCG@10:0.3659 | NDCG@20:0.4234 | recall@5:0.4985 | recall@10:0.5838 | recall@20:0.6358 | precision@5:0.5982 | precision@10:0.3502 | precision@20:0.1907 | best_HR@5:0.7215 | best_HR@10:0.7630 | best_HR@20:0.8126 | best_NDCG@5:0.3544 | best_NDCG@10:0.3991 | best_NDCG@20:0.4554 | best_recall@5:0.5137 | best_recall@10:0.6091 | best_recall@20:0.6607 | best_precision@5:0.6165 | best_precision@10:0.3654 | best_precision@20:0.1982 | 
epoch: 90, training loss: 157.15384703865857, train time: 110.54769587516785
epoch: 91, training loss: 159.2722784942307, train time: 110.35039639472961
epoch: 92, training loss: 159.1018350358936, train time: 110.55679750442505
epoch: 93, training loss: 158.04186515239417, train time: 110.01887345314026
epoch: 94, training loss: 157.09451476793038, train time: 109.62367558479309
epoch: 95, training loss: 160.77895432474907, train time: 110.00138854980469
epoch: 96, training loss: 157.13256047278992, train time: 109.18016004562378
epoch: 97, training loss: 157.59566206284217, train time: 109.09609460830688
epoch: 98, training loss: 158.28489019547123, train time: 93.96667528152466
epoch: 99, training loss: 160.25416710568243, train time: 96.81435322761536
epoch: 100, training loss: 161.275751491572, train time: 95.53551173210144
epoch: 101, training loss: 157.64591728238156, train time: 94.79427862167358
epoch: 102, training loss: 156.9760971236683, train time: 100.90318393707275
epoch: 103, training loss: 158.51187841646606, train time: 88.13087511062622
epoch: 104, training loss: 157.7948863156489, train time: 83.08344960212708
epoch: 105, training loss: 158.57215158984764, train time: 85.28642272949219
epoch: 106, training loss: 157.42480931521277, train time: 92.38998103141785
epoch: 107, training loss: 156.70870360336266, train time: 93.61506724357605
epoch: 108, training loss: 155.74114474916132, train time: 100.06994867324829
epoch: 109, training loss: 156.99522504294873, train time: 93.25171303749084
epoch: 110, training loss: 158.94198096898617, train time: 101.95075821876526
epoch: 111, training loss: 160.39845071655873, train time: 106.84360384941101
epoch: 112, training loss: 158.74020938042668, train time: 110.93484902381897
epoch: 113, training loss: 162.35263522689638, train time: 109.47409152984619
epoch: 114, training loss: 162.30400005268166, train time: 109.67054104804993
epoch: 115, training loss: 161.27018454950303, train time: 109.32325100898743
epoch: 116, training loss: 160.09546693006996, train time: 109.77329802513123
epoch: 117, training loss: 158.5198648925434, train time: 109.67054176330566
epoch: 118, training loss: 157.1988547638175, train time: 109.97311615943909
epoch: 119, training loss: 159.53830082013155, train time: 110.06480407714844
epo:119 | HR@5:0.6963 | HR@10:0.7323 | HR@20:0.7819 | NDCG@5:0.3111 | NDCG@10:0.3558 | NDCG@20:0.4130 | recall@5:0.5042 | recall@10:0.5903 | recall@20:0.6355 | precision@5:0.6050 | precision@10:0.3542 | precision@20:0.1906 | best_HR@5:0.7215 | best_HR@10:0.7630 | best_HR@20:0.8126 | best_NDCG@5:0.3544 | best_NDCG@10:0.3991 | best_NDCG@20:0.4554 | best_recall@5:0.5137 | best_recall@10:0.6091 | best_recall@20:0.6607 | best_precision@5:0.6165 | best_precision@10:0.3654 | best_precision@20:0.1982 | 
epoch: 120, training loss: 159.5009007449844, train time: 110.38588857650757
epoch: 121, training loss: 158.93863845482701, train time: 109.83867359161377
epoch: 122, training loss: 156.8226735246135, train time: 109.36355710029602
epoch: 123, training loss: 157.93836135457968, train time: 109.55130386352539
epoch: 124, training loss: 159.93041716053267, train time: 113.76042056083679
epoch: 125, training loss: 159.7790515050292, train time: 180.00321578979492
epoch: 126, training loss: 158.0488771352684, train time: 218.53430199623108
epoch: 127, training loss: 159.09588735713623, train time: 218.29260325431824
epoch: 128, training loss: 159.04121614307223, train time: 217.8716995716095
epoch: 129, training loss: 160.2327486956783, train time: 217.81359958648682
epoch: 130, training loss: 159.67443980963435, train time: 217.66983938217163
epoch: 131, training loss: 158.68600463974872, train time: 217.0966763496399
epoch: 132, training loss: 157.9848967799917, train time: 217.5814824104309
epoch: 133, training loss: 156.7965199736209, train time: 218.31765246391296
epoch: 134, training loss: 155.3393753648561, train time: 217.38372492790222
epoch: 135, training loss: 156.05553517551743, train time: 218.03907871246338
epoch: 136, training loss: 156.80032542347908, train time: 217.85913753509521
epoch: 137, training loss: 158.4123552615929, train time: 217.2277421951294
epoch: 138, training loss: 158.02122748512193, train time: 214.94552397727966
epoch: 139, training loss: 161.67755560702062, train time: 209.8030457496643
epoch: 140, training loss: 162.16643886166275, train time: 209.34041619300842
epoch: 141, training loss: 162.83802727545844, train time: 207.8367600440979
epoch: 142, training loss: 160.99512723226508, train time: 195.97635293006897
epoch: 143, training loss: 159.88395546919492, train time: 200.15563297271729
epoch: 144, training loss: 163.45788244428695, train time: 207.80105829238892
epoch: 145, training loss: 160.86187388413236, train time: 208.29176020622253
epoch: 146, training loss: 160.34648625415866, train time: 208.53677368164062
epoch: 147, training loss: 160.61509838647908, train time: 218.31399941444397
epoch: 148, training loss: 160.18216225336073, train time: 217.79413604736328
epoch: 149, training loss: 162.08670068102947, train time: 217.54726362228394
epo:149 | HR@5:0.6412 | HR@10:0.6777 | HR@20:0.7358 | NDCG@5:0.3031 | NDCG@10:0.3451 | NDCG@20:0.3986 | recall@5:0.4762 | recall@10:0.5433 | recall@20:0.5959 | precision@5:0.5715 | precision@10:0.3260 | precision@20:0.1788 | best_HR@5:0.7215 | best_HR@10:0.7630 | best_HR@20:0.8126 | best_NDCG@5:0.3544 | best_NDCG@10:0.3991 | best_NDCG@20:0.4554 | best_recall@5:0.5137 | best_recall@10:0.6091 | best_recall@20:0.6607 | best_precision@5:0.6165 | best_precision@10:0.3654 | best_precision@20:0.1982 | 
training finish
