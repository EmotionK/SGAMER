nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  8.58306884765625e-06
user  100 time:  421.6508438587189
user  200 time:  847.6547203063965
user  300 time:  1275.6119289398193
user  400 time:  1711.1569769382477
user  500 time:  2148.0117049217224
user  600 time:  2581.3387472629547
user  700 time:  3017.6137475967407
user  800 time:  3454.0561928749084
user  900 time:  3893.09592628479
user  1000 time:  4330.0275847911835
user  1100 time:  4764.62468457222
user  1200 time:  5207.253698348999
user  1300 time:  5644.75815486908
user  1400 time:  6084.809771060944
start training item-item instance self attention module...
user  0 time:  5.4836273193359375e-06
user  100 time:  317.65533542633057
user  200 time:  635.235345363617
user  300 time:  956.005651473999
user  400 time:  1280.4456963539124
user  500 time:  1596.9240186214447
user  600 time:  1891.334153175354
user  700 time:  2216.9444460868835
user  800 time:  2519.8003792762756
user  900 time:  2831.0376131534576
user  1000 time:  3121.827726125717
user  1100 time:  3428.3061332702637
user  1200 time:  3749.448529481888
user  1300 time:  4078.364842891693
user  1400 time:  4398.042123794556
start updating user and item embedding...
user_name:1450
user  0 time:  1.3589859008789062e-05
user  100 time:  23.98886489868164
user  200 time:  48.148926973342896
user  300 time:  71.91641640663147
user  400 time:  95.74248886108398
user  500 time:  119.71711492538452
user  600 time:  143.5902771949768
user  700 time:  167.04113125801086
user  800 time:  190.71542978286743
user  900 time:  214.40473556518555
user  1000 time:  238.03905725479126
user  1100 time:  261.6382794380188
user  1200 time:  285.8639352321625
user  1300 time:  309.85076570510864
user  1400 time:  333.2715194225311
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 149.83608049503528, train time: 38.52024865150452
epoch: 1, training loss: 87.64361889936845, train time: 38.699549436569214
epoch: 2, training loss: 73.87201644832385, train time: 38.61280059814453
epoch: 3, training loss: 66.24725860543549, train time: 38.58672881126404
epoch: 4, training loss: 61.18971901808982, train time: 38.06869578361511
epoch: 5, training loss: 57.65818872566888, train time: 38.404865980148315
epoch: 6, training loss: 53.61115436190448, train time: 38.535884141922
epoch: 7, training loss: 52.070437433590996, train time: 38.44795060157776
epoch: 8, training loss: 48.32510290778009, train time: 38.514708518981934
epoch: 9, training loss: 45.927068843870074, train time: 38.589702129364014
epoch: 10, training loss: 43.62148558232002, train time: 38.552526235580444
epoch: 11, training loss: 41.1233584487054, train time: 38.52474856376648
epoch: 12, training loss: 40.09338197758552, train time: 38.55137014389038
epoch: 13, training loss: 38.35727961035445, train time: 38.50712752342224
epoch: 14, training loss: 36.550574292781675, train time: 38.57633376121521
epoch: 15, training loss: 35.42608369320442, train time: 38.609010219573975
epoch: 16, training loss: 33.05418482864843, train time: 38.448928356170654
epoch: 17, training loss: 31.58723288956753, train time: 38.60694193840027
epoch: 18, training loss: 31.16183327246108, train time: 38.37258577346802
epoch: 19, training loss: 28.728381300032197, train time: 38.62159538269043
epoch: 20, training loss: 27.416764720124775, train time: 38.64656090736389
epoch: 21, training loss: 26.325667141114536, train time: 38.662039279937744
epoch: 22, training loss: 25.50582360331282, train time: 38.596022844314575
epoch: 23, training loss: 24.448488164713126, train time: 38.405253648757935
epoch: 24, training loss: 22.563181627174345, train time: 38.74398398399353
epoch: 25, training loss: 22.754712940042737, train time: 38.61047649383545
epoch: 26, training loss: 22.2445705028631, train time: 38.789438247680664
epoch: 27, training loss: 20.208381439746518, train time: 38.989293813705444
epoch: 28, training loss: 20.54702723313312, train time: 38.84467577934265
epoch: 29, training loss: 19.244085479938803, train time: 38.68601107597351
epo:29 | HR@5:0.7952 | HR@10:0.8348 | HR@20:0.8792 | NDCG@5:0.4547 | NDCG@10:0.4939 | NDCG@20:0.5431 | recall@5:0.5630 | recall@10:0.6672 | recall@20:0.7141 | precision@5:0.6756 | precision@10:0.4003 | precision@20:0.2142 | best_HR@5:0.7952 | best_HR@10:0.8348 | best_HR@20:0.8792 | best_NDCG@5:0.4547 | best_NDCG@10:0.4939 | best_NDCG@20:0.5431 | best_recall@5:0.5630 | best_recall@10:0.6672 | best_recall@20:0.7141 | best_precision@5:0.6756 | best_precision@10:0.4003 | best_precision@20:0.2142 | 
epoch: 30, training loss: 18.869430320176434, train time: 38.732651710510254
epoch: 31, training loss: 18.258461848255138, train time: 38.541070222854614
epoch: 32, training loss: 17.160581558001468, train time: 38.44983410835266
epoch: 33, training loss: 17.800532727212158, train time: 38.77727150917053
epoch: 34, training loss: 16.998805861118853, train time: 38.67855167388916
epoch: 35, training loss: 15.957956317286516, train time: 38.70822811126709
epoch: 36, training loss: 16.11561302648738, train time: 38.4832181930542
epoch: 37, training loss: 15.557995912166461, train time: 38.613104581832886
epoch: 38, training loss: 15.685437963167715, train time: 38.57893085479736
epoch: 39, training loss: 15.501216031974081, train time: 38.6839644908905
epoch: 40, training loss: 14.554850924031598, train time: 38.64692211151123
epoch: 41, training loss: 15.0012248792018, train time: 38.57182741165161
epoch: 42, training loss: 13.93102759087401, train time: 38.48080587387085
epoch: 43, training loss: 14.203760314276224, train time: 38.54876184463501
epoch: 44, training loss: 12.93988162186406, train time: 38.60000967979431
epoch: 45, training loss: 14.441227870170678, train time: 38.567779541015625
epoch: 46, training loss: 14.152362018671738, train time: 38.53769636154175
epoch: 47, training loss: 14.262300272665925, train time: 38.56800580024719
epoch: 48, training loss: 12.176670542012062, train time: 38.48718595504761
epoch: 49, training loss: 13.708530853258253, train time: 38.542872190475464
epoch: 50, training loss: 12.377849336756071, train time: 38.517783641815186
epoch: 51, training loss: 12.597804729251493, train time: 38.66132307052612
epoch: 52, training loss: 12.766760246232366, train time: 38.41199064254761
epoch: 53, training loss: 12.718594087577912, train time: 38.798508644104004
epoch: 54, training loss: 13.694983686583782, train time: 38.656426191329956
epoch: 55, training loss: 11.717800576574746, train time: 38.343393325805664
epoch: 56, training loss: 13.295032919748337, train time: 38.23226594924927
epoch: 57, training loss: 12.682788363142436, train time: 38.405766010284424
epoch: 58, training loss: 11.751781355138519, train time: 38.45480036735535
epoch: 59, training loss: 12.099083803898566, train time: 38.30302619934082
epo:59 | HR@5:0.7641 | HR@10:0.8037 | HR@20:0.8547 | NDCG@5:0.4765 | NDCG@10:0.5131 | NDCG@20:0.5598 | recall@5:0.5448 | recall@10:0.6445 | recall@20:0.6946 | precision@5:0.6538 | precision@10:0.3867 | precision@20:0.2084 | best_HR@5:0.7952 | best_HR@10:0.8348 | best_HR@20:0.8792 | best_NDCG@5:0.4765 | best_NDCG@10:0.5131 | best_NDCG@20:0.5598 | best_recall@5:0.5630 | best_recall@10:0.6672 | best_recall@20:0.7141 | best_precision@5:0.6756 | best_precision@10:0.4003 | best_precision@20:0.2142 | 
epoch: 60, training loss: 12.592922248932723, train time: 38.2475790977478
epoch: 61, training loss: 10.899479055574034, train time: 38.311131954193115
epoch: 62, training loss: 11.619091099478055, train time: 38.23633885383606
epoch: 63, training loss: 11.937571598458021, train time: 38.543785095214844
epoch: 64, training loss: 11.810014533571916, train time: 38.561002016067505
epoch: 65, training loss: 11.868763615140438, train time: 38.56398916244507
epoch: 66, training loss: 11.084054490967674, train time: 38.71900820732117
epoch: 67, training loss: 12.005902368008947, train time: 38.541611433029175
epoch: 68, training loss: 11.95107134232444, train time: 38.57460689544678
epoch: 69, training loss: 11.323397730264787, train time: 38.74950909614563
epoch: 70, training loss: 11.922530716947335, train time: 38.64286947250366
epoch: 71, training loss: 10.604939029212744, train time: 38.55504369735718
epoch: 72, training loss: 11.16308794744873, train time: 38.54571843147278
epoch: 73, training loss: 11.14913948518415, train time: 38.654868364334106
epoch: 74, training loss: 10.063234548552828, train time: 38.69842052459717
epoch: 75, training loss: 11.265317411392743, train time: 38.632750034332275
epoch: 76, training loss: 10.883128264638117, train time: 38.666070222854614
epoch: 77, training loss: 11.252914174623129, train time: 38.524977684020996
epoch: 78, training loss: 9.707936285475398, train time: 38.71751570701599
epoch: 79, training loss: 10.691212560555641, train time: 38.59756374359131
epoch: 80, training loss: 11.425956901370967, train time: 38.59585475921631
epoch: 81, training loss: 10.591098087126852, train time: 38.595505237579346
epoch: 82, training loss: 10.413959767386586, train time: 38.67613244056702
epoch: 83, training loss: 9.674949968824649, train time: 38.80447959899902
epoch: 84, training loss: 9.642067559744646, train time: 38.21169376373291
epoch: 85, training loss: 10.476823548291577, train time: 37.99122428894043
epoch: 86, training loss: 10.101346546540412, train time: 38.41468644142151
epoch: 87, training loss: 9.904045288898658, train time: 38.579023599624634
epoch: 88, training loss: 10.780642449872232, train time: 38.83495497703552
epoch: 89, training loss: 9.726126647482602, train time: 38.509225368499756
epo:89 | HR@5:0.7494 | HR@10:0.7889 | HR@20:0.8369 | NDCG@5:0.4757 | NDCG@10:0.5125 | NDCG@20:0.5595 | recall@5:0.5407 | recall@10:0.6315 | recall@20:0.6818 | precision@5:0.6488 | precision@10:0.3789 | precision@20:0.2046 | best_HR@5:0.7952 | best_HR@10:0.8348 | best_HR@20:0.8792 | best_NDCG@5:0.4765 | best_NDCG@10:0.5131 | best_NDCG@20:0.5598 | best_recall@5:0.5630 | best_recall@10:0.6672 | best_recall@20:0.7141 | best_precision@5:0.6756 | best_precision@10:0.4003 | best_precision@20:0.2142 | 
epoch: 90, training loss: 9.38542516715097, train time: 38.90231275558472
epoch: 91, training loss: 10.869589358863436, train time: 38.91265249252319
epoch: 92, training loss: 9.787453785700677, train time: 38.903823137283325
epoch: 93, training loss: 10.102056920244195, train time: 38.50702929496765
epoch: 94, training loss: 9.284432473308414, train time: 38.525827169418335
epoch: 95, training loss: 10.838602190334768, train time: 38.66496229171753
epoch: 96, training loss: 9.829440456681482, train time: 38.55101156234741
epoch: 97, training loss: 10.162119010024298, train time: 38.5311017036438
epoch: 98, training loss: 8.542936180698405, train time: 38.59816336631775
epoch: 99, training loss: 10.379923182374625, train time: 38.46957039833069
epoch: 100, training loss: 9.905130247586271, train time: 38.574806451797485
epoch: 101, training loss: 8.853615095938153, train time: 38.457199811935425
epoch: 102, training loss: 9.298033458803502, train time: 38.62357139587402
epoch: 103, training loss: 10.30028758683983, train time: 38.568241119384766
epoch: 104, training loss: 9.638351284037299, train time: 38.34503722190857
epoch: 105, training loss: 8.120543825064033, train time: 38.60664916038513
epoch: 106, training loss: 9.784702280496163, train time: 38.63021421432495
epoch: 107, training loss: 8.836708128568262, train time: 38.55479145050049
epoch: 108, training loss: 9.398223133543638, train time: 38.57007670402527
epoch: 109, training loss: 8.483576976948711, train time: 38.63944983482361
epoch: 110, training loss: 8.494910739857914, train time: 38.59889268875122
epoch: 111, training loss: 10.04379669800852, train time: 38.656553506851196
epoch: 112, training loss: 8.420392916932087, train time: 38.5249080657959
epoch: 113, training loss: 9.734487518706374, train time: 39.0266649723053
epoch: 114, training loss: 9.487810787412172, train time: 38.87396001815796
epoch: 115, training loss: 9.47031419456323, train time: 39.083874464035034
epoch: 116, training loss: 8.72716099220213, train time: 39.13060164451599
epoch: 117, training loss: 8.454731722190218, train time: 38.915181159973145
epoch: 118, training loss: 8.490993172939682, train time: 38.62273812294006
epoch: 119, training loss: 9.693470549820518, train time: 38.49640917778015
epo:119 | HR@5:0.7502 | HR@10:0.7883 | HR@20:0.8336 | NDCG@5:0.4850 | NDCG@10:0.5212 | NDCG@20:0.5669 | recall@5:0.5405 | recall@10:0.6292 | recall@20:0.6766 | precision@5:0.6486 | precision@10:0.3775 | precision@20:0.2030 | best_HR@5:0.7952 | best_HR@10:0.8348 | best_HR@20:0.8792 | best_NDCG@5:0.4850 | best_NDCG@10:0.5212 | best_NDCG@20:0.5669 | best_recall@5:0.5630 | best_recall@10:0.6672 | best_recall@20:0.7141 | best_precision@5:0.6756 | best_precision@10:0.4003 | best_precision@20:0.2142 | 
epoch: 120, training loss: 8.146868798871239, train time: 38.6055428981781
epoch: 121, training loss: 8.29115399023135, train time: 39.15196251869202
epoch: 122, training loss: 8.412383491572143, train time: 38.51008367538452
epoch: 123, training loss: 8.33985874032669, train time: 38.52489924430847
epoch: 124, training loss: 9.453337593107165, train time: 38.52278208732605
epoch: 125, training loss: 8.74131465121934, train time: 38.48663306236267
epoch: 126, training loss: 8.546323544375952, train time: 38.54722857475281
epoch: 127, training loss: 8.550928095790198, train time: 38.4271981716156
epoch: 128, training loss: 7.778065156783896, train time: 38.756864070892334
epoch: 129, training loss: 8.215140441269284, train time: 38.49918484687805
epoch: 130, training loss: 8.02903465891211, train time: 38.56399750709534
epoch: 131, training loss: 8.572033540751193, train time: 38.60253858566284
epoch: 132, training loss: 8.887454874341984, train time: 38.71304106712341
epoch: 133, training loss: 9.08348984947392, train time: 38.6321005821228
epoch: 134, training loss: 8.527530007584403, train time: 38.61278295516968
epoch: 135, training loss: 8.958522068122079, train time: 38.53763699531555
epoch: 136, training loss: 8.273367312828896, train time: 38.53245711326599
epoch: 137, training loss: 8.388829813240022, train time: 38.71068811416626
epoch: 138, training loss: 8.008215900131404, train time: 38.53899693489075
epoch: 139, training loss: 9.159772281735513, train time: 38.68992829322815
epoch: 140, training loss: 8.316680281471804, train time: 38.64461874961853
epoch: 141, training loss: 8.771523109181885, train time: 38.6997172832489
epoch: 142, training loss: 8.56404381094336, train time: 39.02203965187073
epoch: 143, training loss: 9.034145538968488, train time: 39.012964963912964
epoch: 144, training loss: 9.18654692834599, train time: 38.951072692871094
epoch: 145, training loss: 8.570287222297566, train time: 38.95762801170349
epoch: 146, training loss: 8.298566618057976, train time: 38.95277786254883
epoch: 147, training loss: 8.767190729503852, train time: 38.94362235069275
epoch: 148, training loss: 7.915554336358696, train time: 39.041764974594116
epoch: 149, training loss: 8.586032452025734, train time: 38.563870668411255
epo:149 | HR@5:0.7547 | HR@10:0.7886 | HR@20:0.8376 | NDCG@5:0.4786 | NDCG@10:0.5152 | NDCG@20:0.5616 | recall@5:0.5410 | recall@10:0.6307 | recall@20:0.6806 | precision@5:0.6492 | precision@10:0.3784 | precision@20:0.2042 | best_HR@5:0.7952 | best_HR@10:0.8348 | best_HR@20:0.8792 | best_NDCG@5:0.4850 | best_NDCG@10:0.5212 | best_NDCG@20:0.5669 | best_recall@5:0.5630 | best_recall@10:0.6672 | best_recall@20:0.7141 | best_precision@5:0.6756 | best_precision@10:0.4003 | best_precision@20:0.2142 | 
training finish
