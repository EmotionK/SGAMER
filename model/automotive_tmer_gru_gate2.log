nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.814697265625e-06
user  100 time:  227.28722977638245
user  200 time:  455.15774488449097
user  300 time:  684.4395356178284
user  400 time:  915.1006095409393
user  500 time:  1146.1998915672302
user  600 time:  1379.2618217468262
user  700 time:  1656.4713377952576
user  800 time:  1927.9513335227966
user  900 time:  2200.042952775955
user  1000 time:  2475.99005484581
user  1100 time:  2754.482173681259
user  1200 time:  3030.3702404499054
user  1300 time:  3311.4049830436707
user  1400 time:  3588.929449558258
user  1500 time:  3873.733602285385
user  1600 time:  4161.115086078644
user  1700 time:  4507.233179569244
user  1800 time:  4858.656187295914
user  1900 time:  5206.558988332748
user  2000 time:  5537.578288078308
user  2100 time:  5864.332305431366
user  2200 time:  6193.136602878571
user  2300 time:  6530.809422492981
user  2400 time:  6846.089395999908
user  2500 time:  7172.393227815628
user  2600 time:  7494.2291967868805
user  2700 time:  7821.554802656174
user  2800 time:  8164.183364868164
user  2900 time:  8501.399981975555
user  3000 time:  8831.084211111069
user  3100 time:  9098.941538572311
user  3200 time:  9362.837800264359
user  3300 time:  9637.365041732788
user  3400 time:  9932.23732471466
user  3500 time:  10203.843748807907
user  3600 time:  10469.007448673248
user  3700 time:  10735.32546544075
user  3800 time:  11006.655380249023
user  3900 time:  11289.529492378235
user  4000 time:  11561.245478391647
user  4100 time:  11837.906576633453
user  4200 time:  12092.274983644485
user  4300 time:  12349.789340019226
user  4400 time:  12606.082890748978
user  4500 time:  12861.04370355606
start training item-item instance self attention module...
user  0 time:  4.76837158203125e-06
user  100 time:  195.08750748634338
user  200 time:  391.88476610183716
user  300 time:  584.5307023525238
user  400 time:  778.0575885772705
user  500 time:  962.8811450004578
user  600 time:  1150.9615557193756
user  700 time:  1344.4060740470886
user  800 time:  1533.476814031601
user  900 time:  1725.1104459762573
user  1000 time:  1923.2621245384216
user  1100 time:  2118.380816936493
user  1200 time:  2302.062072277069
user  1300 time:  2491.783764362335
user  1400 time:  2670.994792699814
user  1500 time:  2865.65118765831
user  1600 time:  3054.8048136234283
user  1700 time:  3237.645196199417
user  1800 time:  3435.0312972068787
user  1900 time:  3642.441599369049
user  2000 time:  3837.566059112549
user  2100 time:  4010.5806930065155
user  2200 time:  4171.048650741577
user  2300 time:  4329.692705154419
user  2400 time:  4496.046725988388
user  2500 time:  4660.881647348404
user  2600 time:  4831.982722759247
user  2700 time:  5009.008599758148
user  2800 time:  5188.62925863266
user  2900 time:  5355.647560119629
user  3000 time:  5523.258408308029
user  3100 time:  5696.844974994659
user  3200 time:  5857.331500291824
user  3300 time:  6027.166688203812
user  3400 time:  6199.973170995712
user  3500 time:  6369.3250205516815
user  3600 time:  6551.264167070389
user  3700 time:  6721.851824045181
user  3800 time:  6898.543941736221
user  3900 time:  7053.946980237961
user  4000 time:  7235.528968334198
user  4100 time:  7416.725813150406
user  4200 time:  7590.9654014110565
user  4300 time:  7763.30343079567
user  4400 time:  7936.659329652786
user  4500 time:  8101.065665960312
start updating user and item embedding...
user_name:4600
user  0 time:  1.049041748046875e-05
user  100 time:  64.93031287193298
user  200 time:  129.11650943756104
user  300 time:  192.48765325546265
user  400 time:  257.0890369415283
user  500 time:  321.1961886882782
user  600 time:  385.932368516922
user  700 time:  451.02177238464355
user  800 time:  514.9144361019135
user  900 time:  579.7669172286987
user  1000 time:  644.4040412902832
user  1100 time:  709.0635025501251
user  1200 time:  773.3550419807434
user  1300 time:  837.2482600212097
user  1400 time:  901.4783990383148
user  1500 time:  966.320796251297
user  1600 time:  1030.9813680648804
user  1700 time:  1096.1622624397278
user  1800 time:  1159.09459400177
user  1900 time:  1223.1531250476837
user  2000 time:  1287.6354897022247
user  2100 time:  1351.9443082809448
user  2200 time:  1416.2427198886871
user  2300 time:  1480.3656814098358
user  2400 time:  1543.9891505241394
user  2500 time:  1608.837664604187
user  2600 time:  1673.5875499248505
user  2700 time:  1738.411957502365
user  2800 time:  1802.3875257968903
user  2900 time:  1867.0023353099823
user  3000 time:  1931.2819738388062
user  3100 time:  1995.0696675777435
user  3200 time:  2059.3039712905884
user  3300 time:  2123.850997209549
user  3400 time:  2188.514588356018
user  3500 time:  2252.2960600852966
user  3600 time:  2316.286345243454
user  3700 time:  2380.337893009186
user  3800 time:  2444.017504930496
user  3900 time:  2508.633777141571
user  4000 time:  2573.028584957123
user  4100 time:  2638.1194648742676
user  4200 time:  2701.0831937789917
user  4300 time:  2764.700902223587
user  4400 time:  2828.763607263565
user  4500 time:  2893.038022518158
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 287.7393685943389, train time: 333.13186621665955
epoch: 1, training loss: 182.36180404530023, train time: 335.3838038444519
epoch: 2, training loss: 168.51911571179517, train time: 332.27617502212524
epoch: 3, training loss: 161.43659404888604, train time: 333.96872878074646
epoch: 4, training loss: 155.72087203408591, train time: 333.60907316207886
epoch: 5, training loss: 150.21792443482263, train time: 332.7366352081299
epoch: 6, training loss: 147.1633566767705, train time: 332.43979048728943
epoch: 7, training loss: 142.94774786927155, train time: 334.1202087402344
epoch: 8, training loss: 138.9404276486457, train time: 334.1505219936371
epoch: 9, training loss: 135.4095064889334, train time: 336.3907480239868
epoch: 10, training loss: 132.17589585253154, train time: 334.660281419754
epoch: 11, training loss: 129.99395373908192, train time: 333.65308594703674
epoch: 12, training loss: 128.73691350373701, train time: 334.5757851600647
epoch: 13, training loss: 124.30739802928292, train time: 334.4518942832947
epoch: 14, training loss: 123.43666917095834, train time: 335.6238148212433
epoch: 15, training loss: 120.57249560728087, train time: 333.1546161174774
epoch: 16, training loss: 119.03557140803605, train time: 335.004860162735
epoch: 17, training loss: 116.22055452966015, train time: 334.4514226913452
epoch: 18, training loss: 115.54600598606339, train time: 334.20252680778503
epoch: 19, training loss: 114.73801537181134, train time: 332.74917340278625
epoch: 20, training loss: 114.80131718727353, train time: 334.7482523918152
epoch: 21, training loss: 113.98528513853671, train time: 333.4152600765228
epoch: 22, training loss: 113.68798945598246, train time: 334.4731159210205
epoch: 23, training loss: 111.0177816183932, train time: 334.97016286849976
epoch: 24, training loss: 110.2602291554067, train time: 334.35369062423706
epoch: 25, training loss: 110.17624354996951, train time: 335.1668245792389
epoch: 26, training loss: 109.69252486184268, train time: 332.5916335582733
epoch: 27, training loss: 109.02407985094032, train time: 335.0626356601715
epoch: 28, training loss: 106.90030326896522, train time: 335.08665132522583
epoch: 29, training loss: 107.34465257699776, train time: 334.0756757259369
epo:29 | HR@5:0.9074 | HR@10:0.9226 | HR@20:0.9417 | NDCG@5:0.4276 | NDCG@10:0.4721 | NDCG@20:0.5262 | recall@5:0.6416 | recall@10:0.7580 | recall@20:0.7774 | precision@5:0.7699 | precision@10:0.4548 | precision@20:0.2332 | best_HR@5:0.9074 | best_HR@10:0.9226 | best_HR@20:0.9417 | best_NDCG@5:0.4276 | best_NDCG@10:0.4721 | best_NDCG@20:0.5262 | best_recall@5:0.6416 | best_recall@10:0.7580 | best_recall@20:0.7774 | best_precision@5:0.7699 | best_precision@10:0.4548 | best_precision@20:0.2332 | 
epoch: 30, training loss: 106.75916014048562, train time: 330.8196032047272
epoch: 31, training loss: 105.47306013855268, train time: 333.5780518054962
epoch: 32, training loss: 105.32801119663782, train time: 333.5126984119415
epoch: 33, training loss: 104.3895373480118, train time: 335.07382011413574
epoch: 34, training loss: 104.78039556381555, train time: 334.5935468673706
epoch: 35, training loss: 104.95848803512854, train time: 333.9843146800995
epoch: 36, training loss: 103.46883616462583, train time: 333.2598948478699
epoch: 37, training loss: 105.30331742452108, train time: 332.9889192581177
epoch: 38, training loss: 104.83877645768371, train time: 334.8210530281067
epoch: 39, training loss: 102.31057287489966, train time: 335.0491404533386
epoch: 40, training loss: 103.9394435904178, train time: 333.10197377204895
epoch: 41, training loss: 104.0112209156614, train time: 333.8656988143921
epoch: 42, training loss: 102.82634455999505, train time: 333.73510456085205
epoch: 43, training loss: 103.81071875433554, train time: 334.5529205799103
epoch: 44, training loss: 102.30766740038234, train time: 335.1580288410187
epoch: 45, training loss: 104.40913627840928, train time: 333.735538482666
epoch: 46, training loss: 103.07227073198737, train time: 335.71409726142883
epoch: 47, training loss: 103.53576350319054, train time: 334.12893176078796
epoch: 48, training loss: 104.09318048225396, train time: 334.8692317008972
epoch: 49, training loss: 103.62259327564243, train time: 333.77002787590027
epoch: 50, training loss: 102.48724357716128, train time: 332.6979010105133
epoch: 51, training loss: 103.02084901001217, train time: 333.9728515148163
epoch: 52, training loss: 100.96564626300096, train time: 334.9379267692566
epoch: 53, training loss: 101.34712332815252, train time: 333.94269466400146
epoch: 54, training loss: 101.15359166264534, train time: 333.14273476600647
epoch: 55, training loss: 100.20210799706547, train time: 332.99849104881287
epoch: 56, training loss: 100.17008051346784, train time: 333.95865178108215
epoch: 57, training loss: 100.49039484852256, train time: 334.94510793685913
epoch: 58, training loss: 101.13704338008392, train time: 335.6440329551697
epoch: 59, training loss: 98.62236954647233, train time: 335.1258714199066
epo:59 | HR@5:0.8713 | HR@10:0.8902 | HR@20:0.9143 | NDCG@5:0.4511 | NDCG@10:0.4923 | NDCG@20:0.5432 | recall@5:0.6303 | recall@10:0.7267 | recall@20:0.7534 | precision@5:0.7563 | precision@10:0.4360 | precision@20:0.2260 | best_HR@5:0.9074 | best_HR@10:0.9226 | best_HR@20:0.9417 | best_NDCG@5:0.4511 | best_NDCG@10:0.4923 | best_NDCG@20:0.5432 | best_recall@5:0.6416 | best_recall@10:0.7580 | best_recall@20:0.7774 | best_precision@5:0.7699 | best_precision@10:0.4548 | best_precision@20:0.2332 | 
epoch: 60, training loss: 100.38551356791868, train time: 339.6530616283417
epoch: 61, training loss: 99.7600237197039, train time: 335.1413986682892
epoch: 62, training loss: 100.25897638194874, train time: 335.286336183548
epoch: 63, training loss: 98.49015071697795, train time: 339.8006663322449
epoch: 64, training loss: 100.13369264770881, train time: 339.8009030818939
epoch: 65, training loss: 99.09962893744523, train time: 334.54002833366394
epoch: 66, training loss: 98.66560609344015, train time: 333.28038477897644
epoch: 67, training loss: 99.10056016944145, train time: 337.03850054740906
epoch: 68, training loss: 98.72987553226267, train time: 340.80212593078613
epoch: 69, training loss: 97.47776060373872, train time: 339.99804162979126
epoch: 70, training loss: 98.72007873094844, train time: 344.856858253479
epoch: 71, training loss: 98.2776326887979, train time: 365.11623454093933
epoch: 72, training loss: 96.92171868213336, train time: 421.6652498245239
epoch: 73, training loss: 96.96787213924108, train time: 435.06199645996094
epoch: 74, training loss: 95.97605856259179, train time: 435.2726218700409
epoch: 75, training loss: 96.3020513086667, train time: 435.4903564453125
epoch: 76, training loss: 94.73602274362929, train time: 435.4107015132904
epoch: 77, training loss: 95.4552524741739, train time: 434.43026852607727
epoch: 78, training loss: 94.13948711557168, train time: 435.84689021110535
epoch: 79, training loss: 93.50062947317929, train time: 435.5211977958679
epoch: 80, training loss: 94.43390848833951, train time: 427.8671655654907
epoch: 81, training loss: 93.61404995855264, train time: 428.75084686279297
epoch: 82, training loss: 92.81210456050758, train time: 428.5431635379791
epoch: 83, training loss: 92.93861137989734, train time: 429.9701769351959
epoch: 84, training loss: 93.91716301202541, train time: 428.2504897117615
epoch: 85, training loss: 93.1776076935057, train time: 428.48209142684937
epoch: 86, training loss: 93.63601468248089, train time: 427.94936299324036
epoch: 87, training loss: 92.74665982754959, train time: 428.3799786567688
epoch: 88, training loss: 92.01537446532166, train time: 427.63628935813904
epoch: 89, training loss: 91.88239265649463, train time: 424.67960143089294
epo:89 | HR@5:0.8728 | HR@10:0.8886 | HR@20:0.9113 | NDCG@5:0.4423 | NDCG@10:0.4863 | NDCG@20:0.5392 | recall@5:0.6295 | recall@10:0.7268 | recall@20:0.7500 | precision@5:0.7553 | precision@10:0.4361 | precision@20:0.2250 | best_HR@5:0.9074 | best_HR@10:0.9226 | best_HR@20:0.9417 | best_NDCG@5:0.4511 | best_NDCG@10:0.4923 | best_NDCG@20:0.5432 | best_recall@5:0.6416 | best_recall@10:0.7580 | best_recall@20:0.7774 | best_precision@5:0.7699 | best_precision@10:0.4548 | best_precision@20:0.2332 | 
epoch: 90, training loss: 92.45790653787844, train time: 375.9692711830139
epoch: 91, training loss: 92.34588788165638, train time: 378.1223714351654
epoch: 92, training loss: 91.77509510061645, train time: 379.893191576004
epoch: 93, training loss: 92.18633337250503, train time: 377.49075078964233
epoch: 94, training loss: 92.6002142765501, train time: 379.87660336494446
epoch: 95, training loss: 91.99928696600546, train time: 377.67379999160767
epoch: 96, training loss: 94.0031610679871, train time: 378.85427069664
epoch: 97, training loss: 92.8603832377703, train time: 377.4987874031067
epoch: 98, training loss: 92.84999115825485, train time: 346.89093136787415
epoch: 99, training loss: 93.25448593473993, train time: 344.3823776245117
epoch: 100, training loss: 93.76504579669563, train time: 344.1035461425781
epoch: 101, training loss: 92.55177735447069, train time: 344.2252745628357
epoch: 102, training loss: 93.46127085569606, train time: 345.4905216693878
epoch: 103, training loss: 91.38627538312358, train time: 343.97593235969543
epoch: 104, training loss: 92.7868512484929, train time: 344.10888671875
epoch: 105, training loss: 92.28050166433968, train time: 347.46894669532776
epoch: 106, training loss: 92.30341270437202, train time: 345.4550874233246
epoch: 107, training loss: 93.5051710852349, train time: 345.36826372146606
epoch: 108, training loss: 92.9365518350969, train time: 344.53056025505066
epoch: 109, training loss: 92.88242689282197, train time: 345.94589281082153
epoch: 110, training loss: 92.496007335365, train time: 345.9318778514862
epoch: 111, training loss: 92.26361860566249, train time: 343.9964265823364
epoch: 112, training loss: 92.7426971234745, train time: 343.4658625125885
epoch: 113, training loss: 91.23804416852363, train time: 343.82687497138977
epoch: 114, training loss: 92.90706862809748, train time: 369.9956154823303
epoch: 115, training loss: 91.35070985607308, train time: 374.8519768714905
epoch: 116, training loss: 92.46976100831671, train time: 374.77338314056396
epoch: 117, training loss: 92.14632387180609, train time: 374.01067066192627
epoch: 118, training loss: 93.41328862368391, train time: 373.7068681716919
epoch: 119, training loss: 93.46147433629085, train time: 373.5671932697296
epo:119 | HR@5:0.8676 | HR@10:0.8845 | HR@20:0.9064 | NDCG@5:0.4300 | NDCG@10:0.4742 | NDCG@20:0.5277 | recall@5:0.6279 | recall@10:0.7248 | recall@20:0.7467 | precision@5:0.7535 | precision@10:0.4349 | precision@20:0.2240 | best_HR@5:0.9074 | best_HR@10:0.9226 | best_HR@20:0.9417 | best_NDCG@5:0.4511 | best_NDCG@10:0.4923 | best_NDCG@20:0.5432 | best_recall@5:0.6416 | best_recall@10:0.7580 | best_recall@20:0.7774 | best_precision@5:0.7699 | best_precision@10:0.4548 | best_precision@20:0.2332 | 
epoch: 120, training loss: 92.47758315842293, train time: 369.0761082172394
epoch: 121, training loss: 94.61640086247644, train time: 374.8778238296509
epoch: 122, training loss: 95.8034818165761, train time: 375.80746626853943
epoch: 123, training loss: 92.28764823748497, train time: 376.1675908565521
epoch: 124, training loss: 93.72022180418571, train time: 375.9207684993744
epoch: 125, training loss: 93.91468813773099, train time: 375.4912190437317
epoch: 126, training loss: 93.81302758829406, train time: 379.91194248199463
epoch: 127, training loss: 93.36284539793996, train time: 415.53171849250793
epoch: 128, training loss: 93.28814149077516, train time: 437.4266881942749
epoch: 129, training loss: 94.80901168585115, train time: 451.06359028816223
epoch: 130, training loss: 93.92884147613222, train time: 460.04797983169556
epoch: 131, training loss: 93.80861441901652, train time: 465.2904760837555
epoch: 132, training loss: 94.89530119125266, train time: 467.2807025909424
epoch: 133, training loss: 93.80500922640931, train time: 463.3013997077942
epoch: 134, training loss: 94.02958918233344, train time: 463.84536504745483
epoch: 135, training loss: 92.49511010485003, train time: 466.64801263809204
epoch: 136, training loss: 94.5023673796095, train time: 465.9741790294647
epoch: 137, training loss: 94.18059238425485, train time: 463.645450592041
epoch: 138, training loss: 94.20721250315546, train time: 465.3336477279663
epoch: 139, training loss: 95.08725075887924, train time: 466.20129799842834
epoch: 140, training loss: 95.05743000146322, train time: 468.6097855567932
epoch: 141, training loss: 95.20026117553061, train time: 470.05820083618164
epoch: 142, training loss: 95.78072243812494, train time: 469.2938394546509
epoch: 143, training loss: 96.89374044695433, train time: 470.35754108428955
epoch: 144, training loss: 96.01243776950287, train time: 466.26955485343933
epoch: 145, training loss: 97.11851200084493, train time: 468.815349817276
epoch: 146, training loss: 96.88165483265038, train time: 469.43461656570435
epoch: 147, training loss: 96.95821380636335, train time: 469.20105957984924
epoch: 148, training loss: 98.38662674811349, train time: 465.7977237701416
epoch: 149, training loss: 98.36660171967378, train time: 471.30935621261597
epo:149 | HR@5:0.8480 | HR@10:0.8662 | HR@20:0.8909 | NDCG@5:0.4258 | NDCG@10:0.4700 | NDCG@20:0.5237 | recall@5:0.6207 | recall@10:0.7077 | recall@20:0.7314 | precision@5:0.7449 | precision@10:0.4246 | precision@20:0.2194 | best_HR@5:0.9074 | best_HR@10:0.9226 | best_HR@20:0.9417 | best_NDCG@5:0.4511 | best_NDCG@10:0.4923 | best_NDCG@20:0.5432 | best_recall@5:0.6416 | best_recall@10:0.7580 | best_recall@20:0.7774 | best_precision@5:0.7699 | best_precision@10:0.4548 | best_precision@20:0.2332 | 
training finish
