nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  1.0728836059570312e-05
user  100 time:  334.32089161872864
user  200 time:  667.6982612609863
user  300 time:  1005.1048502922058
user  400 time:  1343.914677143097
user  500 time:  1679.674681186676
user  600 time:  2021.3350493907928
user  700 time:  2364.431843996048
user  800 time:  2702.8184173107147
user  900 time:  3044.089514017105
user  1000 time:  3384.504350423813
user  1100 time:  3727.6173293590546
user  1200 time:  4069.364666223526
user  1300 time:  4414.606995820999
user  1400 time:  4754.617110013962
user  1500 time:  5096.373579263687
user  1600 time:  5434.709468841553
user  1700 time:  5774.24688744545
user  1800 time:  6118.043972969055
user  1900 time:  6460.844331979752
user  2000 time:  6801.246609926224
user  2100 time:  7138.966859102249
user  2200 time:  7480.306133985519
user  2300 time:  7826.712125301361
user  2400 time:  8167.609471082687
user  2500 time:  8510.50821852684
user  2600 time:  8854.197234392166
user  2700 time:  9196.014709711075
user  2800 time:  9542.89947271347
user  2900 time:  9886.009851694107
user  3000 time:  10227.990612983704
user  3100 time:  10571.353060722351
user  3200 time:  10912.235852956772
user  3300 time:  11256.245806455612
user  3400 time:  11601.464163541794
user  3500 time:  11942.706684827805
user  3600 time:  12288.591274499893
user  3700 time:  12629.510177135468
user  3800 time:  12968.062628269196
user  3900 time:  13310.136490106583
user  4000 time:  13652.701666593552
user  4100 time:  13998.090940475464
user  4200 time:  14337.447301864624
user  4300 time:  14680.80289888382
user  4400 time:  15026.385419130325
user  4500 time:  15366.270615816116
start training item-item instance self attention module...
user  0 time:  5.9604644775390625e-06
user  100 time:  233.36734580993652
user  200 time:  468.4507305622101
user  300 time:  696.9461915493011
user  400 time:  926.126544713974
user  500 time:  1149.4899470806122
user  600 time:  1374.7807207107544
user  700 time:  1605.3012444972992
user  800 time:  1834.087120771408
user  900 time:  2066.41543340683
user  1000 time:  2305.823404073715
user  1100 time:  2538.806430578232
user  1200 time:  2762.6709163188934
user  1300 time:  2993.796096086502
user  1400 time:  3206.857528924942
user  1500 time:  3438.108845472336
user  1600 time:  3662.2089824676514
user  1700 time:  3875.682723760605
user  1800 time:  4106.152876853943
user  1900 time:  4348.4215965271
user  2000 time:  4576.361605644226
user  2100 time:  4781.938439369202
user  2200 time:  4989.686183452606
user  2300 time:  5196.30955696106
user  2400 time:  5413.170156478882
user  2500 time:  5629.888339519501
user  2600 time:  5854.706828594208
user  2700 time:  6084.260185480118
user  2800 time:  6319.6817417144775
user  2900 time:  6538.443793296814
user  3000 time:  6757.037662982941
user  3100 time:  6982.79975104332
user  3200 time:  7189.867039680481
user  3300 time:  7409.419342517853
user  3400 time:  7633.258288145065
user  3500 time:  7853.897926807404
user  3600 time:  8091.180272817612
user  3700 time:  8312.26070857048
user  3800 time:  8539.952484607697
user  3900 time:  8741.303896665573
user  4000 time:  8977.317957639694
user  4100 time:  9214.400464057922
user  4200 time:  9438.868733406067
user  4300 time:  9661.233829975128
user  4400 time:  9888.569580316544
user  4500 time:  10101.687258720398
start updating user and item embedding...
user_name:4600
user  0 time:  1.0013580322265625e-05
user  100 time:  15.629644870758057
user  200 time:  31.254900217056274
user  300 time:  46.67548179626465
user  400 time:  62.170408487319946
user  500 time:  77.6927227973938
user  600 time:  93.2363178730011
user  700 time:  108.87166786193848
user  800 time:  124.34138989448547
user  900 time:  140.0295021533966
user  1000 time:  156.00855684280396
user  1100 time:  171.56204867362976
user  1200 time:  187.31201338768005
user  1300 time:  202.9529972076416
user  1400 time:  218.31253695487976
user  1500 time:  233.80589723587036
user  1600 time:  249.3167600631714
user  1700 time:  264.92837357521057
user  1800 time:  280.5783863067627
user  1900 time:  296.1393678188324
user  2000 time:  311.689977645874
user  2100 time:  327.13236021995544
user  2200 time:  342.86833691596985
user  2300 time:  358.3696436882019
user  2400 time:  373.8539433479309
user  2500 time:  389.3794856071472
user  2600 time:  404.8771460056305
user  2700 time:  420.6764831542969
user  2800 time:  436.1808822154999
user  2900 time:  451.4676377773285
user  3000 time:  467.0358610153198
user  3100 time:  482.47140741348267
user  3200 time:  497.84420585632324
user  3300 time:  513.2403597831726
user  3400 time:  528.5132369995117
user  3500 time:  543.7304539680481
user  3600 time:  559.2016565799713
user  3700 time:  574.7242612838745
user  3800 time:  590.1540291309357
user  3900 time:  605.3783967494965
user  4000 time:  620.8686556816101
user  4100 time:  636.2906658649445
user  4200 time:  651.7312443256378
user  4300 time:  667.2125549316406
user  4400 time:  682.6746487617493
user  4500 time:  698.1674399375916
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 289.645490909199, train time: 245.32871532440186
epoch: 1, training loss: 181.492916786774, train time: 246.00990390777588
epoch: 2, training loss: 168.3856510083133, train time: 245.8069863319397
epoch: 3, training loss: 161.98098245979054, train time: 245.77649116516113
epoch: 4, training loss: 155.4911296999053, train time: 245.71680426597595
epoch: 5, training loss: 150.51478913385654, train time: 245.08615112304688
epoch: 6, training loss: 146.48064839128347, train time: 245.19767665863037
epoch: 7, training loss: 142.34668857677025, train time: 245.8462097644806
epoch: 8, training loss: 139.21685886932391, train time: 245.54294323921204
epoch: 9, training loss: 135.9974654304242, train time: 245.4104540348053
epoch: 10, training loss: 133.4690219606564, train time: 245.73025727272034
epoch: 11, training loss: 130.01192896986322, train time: 245.6584300994873
epoch: 12, training loss: 126.88771042228473, train time: 245.7779085636139
epoch: 13, training loss: 124.7966723184145, train time: 245.42764973640442
epoch: 14, training loss: 123.3185223358887, train time: 245.69977474212646
epoch: 15, training loss: 120.91176525349147, train time: 245.22438144683838
epoch: 16, training loss: 119.14243257103954, train time: 245.79463601112366
epoch: 17, training loss: 120.52463261690718, train time: 245.29207277297974
epoch: 18, training loss: 117.23049185905984, train time: 245.71084642410278
epoch: 19, training loss: 117.23668688305042, train time: 245.39734768867493
epoch: 20, training loss: 115.94229281139269, train time: 245.51236128807068
epoch: 21, training loss: 113.60618340139627, train time: 245.65802907943726
epoch: 22, training loss: 113.49517760313756, train time: 245.49344420433044
epoch: 23, training loss: 111.29827346254024, train time: 245.5783154964447
epoch: 24, training loss: 110.7117303225823, train time: 244.781147480011
epoch: 25, training loss: 110.18013787906239, train time: 245.90552020072937
epoch: 26, training loss: 110.57653364098951, train time: 245.79970026016235
epoch: 27, training loss: 109.1552544329752, train time: 245.60577535629272
epoch: 28, training loss: 107.37251406215, train time: 245.60226106643677
epoch: 29, training loss: 107.90208346825239, train time: 245.21194005012512
epo:29 | HR@5:0.9069 | HR@10:0.9229 | HR@20:0.9407 | NDCG@5:0.4239 | NDCG@10:0.4680 | NDCG@20:0.5220 | recall@5:0.6445 | recall@10:0.7568 | recall@20:0.7775 | precision@5:0.7734 | precision@10:0.4541 | precision@20:0.2333 | best_HR@5:0.9069 | best_HR@10:0.9229 | best_HR@20:0.9407 | best_NDCG@5:0.4239 | best_NDCG@10:0.4680 | best_NDCG@20:0.5220 | best_recall@5:0.6445 | best_recall@10:0.7568 | best_recall@20:0.7775 | best_precision@5:0.7734 | best_precision@10:0.4541 | best_precision@20:0.2333 | 
epoch: 30, training loss: 106.95794681855477, train time: 246.18824934959412
epoch: 31, training loss: 107.08254848927754, train time: 245.87254977226257
epoch: 32, training loss: 105.25166698372777, train time: 245.60591530799866
epoch: 33, training loss: 104.84909412798879, train time: 245.09748148918152
epoch: 34, training loss: 105.31258224991325, train time: 245.84483575820923
epoch: 35, training loss: 107.23696729796939, train time: 246.03705596923828
epoch: 36, training loss: 104.02469277053024, train time: 245.6120481491089
epoch: 37, training loss: 103.43332971685959, train time: 245.8667209148407
epoch: 38, training loss: 102.84469964269374, train time: 245.52283549308777
epoch: 39, training loss: 103.36186939712934, train time: 245.70088720321655
epoch: 40, training loss: 102.10133774613496, train time: 245.84037160873413
epoch: 41, training loss: 103.90769510104292, train time: 246.3499412536621
epoch: 42, training loss: 102.70256949224131, train time: 245.80878281593323
epoch: 43, training loss: 101.54874131769611, train time: 246.1828649044037
epoch: 44, training loss: 100.49143847970845, train time: 245.77510571479797
epoch: 45, training loss: 99.07676424038073, train time: 246.44139409065247
epoch: 46, training loss: 98.01903525410307, train time: 246.1327929496765
epoch: 47, training loss: 98.96537380674272, train time: 245.31883454322815
epoch: 48, training loss: 98.52995237901632, train time: 245.21556067466736
epoch: 49, training loss: 96.66592432954349, train time: 245.79705476760864
epoch: 50, training loss: 97.19840094798565, train time: 246.23786234855652
epoch: 51, training loss: 96.78951587761549, train time: 245.49846482276917
epoch: 52, training loss: 97.44585337323952, train time: 245.6261100769043
epoch: 53, training loss: 98.10683188067196, train time: 245.204852104187
epoch: 54, training loss: 96.74426847180439, train time: 246.07125663757324
epoch: 55, training loss: 95.47637326045515, train time: 246.26345109939575
epoch: 56, training loss: 95.25785705260932, train time: 246.03809356689453
epoch: 57, training loss: 96.04033378370514, train time: 245.9400839805603
epoch: 58, training loss: 95.90148091853189, train time: 245.87449049949646
epoch: 59, training loss: 95.1939251243748, train time: 245.1578664779663
epo:59 | HR@5:0.8727 | HR@10:0.8931 | HR@20:0.9174 | NDCG@5:0.4400 | NDCG@10:0.4825 | NDCG@20:0.5349 | recall@5:0.6311 | recall@10:0.7295 | recall@20:0.7560 | precision@5:0.7573 | precision@10:0.4377 | precision@20:0.2268 | best_HR@5:0.9069 | best_HR@10:0.9229 | best_HR@20:0.9407 | best_NDCG@5:0.4400 | best_NDCG@10:0.4825 | best_NDCG@20:0.5349 | best_recall@5:0.6445 | best_recall@10:0.7568 | best_recall@20:0.7775 | best_precision@5:0.7734 | best_precision@10:0.4541 | best_precision@20:0.2333 | 
epoch: 60, training loss: 95.67637984081375, train time: 245.5917932987213
epoch: 61, training loss: 95.40257868303888, train time: 245.9303629398346
epoch: 62, training loss: 94.102603432053, train time: 245.72087502479553
epoch: 63, training loss: 92.32426181996925, train time: 246.1150643825531
epoch: 64, training loss: 94.1411883153778, train time: 245.66361451148987
epoch: 65, training loss: 92.96207101774053, train time: 246.0035264492035
epoch: 66, training loss: 93.1470510705476, train time: 245.69635248184204
epoch: 67, training loss: 93.54914837186516, train time: 245.33624935150146
epoch: 68, training loss: 95.4411582070461, train time: 245.5682029724121
epoch: 69, training loss: 95.86855338267924, train time: 246.0713393688202
epoch: 70, training loss: 94.65274364123616, train time: 245.94078016281128
epoch: 71, training loss: 95.1964036009449, train time: 245.93838691711426
epoch: 72, training loss: 94.25511729568461, train time: 245.73694109916687
epoch: 73, training loss: 94.39736869086482, train time: 246.10361099243164
epoch: 74, training loss: 93.74513534885045, train time: 246.33074188232422
epoch: 75, training loss: 94.64020575200993, train time: 245.82311010360718
epoch: 76, training loss: 93.6323502658488, train time: 245.41057348251343
epoch: 77, training loss: 93.20007463719958, train time: 245.75033855438232
epoch: 78, training loss: 93.44826939725317, train time: 244.9765055179596
epoch: 79, training loss: 92.16529315293155, train time: 245.11790132522583
epoch: 80, training loss: 93.38162418058346, train time: 245.19564056396484
epoch: 81, training loss: 94.26870702239103, train time: 246.01476120948792
epoch: 82, training loss: 94.35826748097315, train time: 245.0596604347229
epoch: 83, training loss: 93.33829211659031, train time: 246.1547815799713
epoch: 84, training loss: 92.27269983060978, train time: 246.03411650657654
epoch: 85, training loss: 92.28477970932727, train time: 245.87202310562134
epoch: 86, training loss: 92.29751739444328, train time: 245.85890746116638
epoch: 87, training loss: 94.08684753387934, train time: 246.23225688934326
epoch: 88, training loss: 92.86796282396972, train time: 245.80141353607178
epoch: 89, training loss: 94.224051973928, train time: 245.71268892288208
epo:89 | HR@5:0.8725 | HR@10:0.8908 | HR@20:0.9110 | NDCG@5:0.4244 | NDCG@10:0.4692 | NDCG@20:0.5234 | recall@5:0.6282 | recall@10:0.7290 | recall@20:0.7526 | precision@5:0.7539 | precision@10:0.4374 | precision@20:0.2258 | best_HR@5:0.9069 | best_HR@10:0.9229 | best_HR@20:0.9407 | best_NDCG@5:0.4400 | best_NDCG@10:0.4825 | best_NDCG@20:0.5349 | best_recall@5:0.6445 | best_recall@10:0.7568 | best_recall@20:0.7775 | best_precision@5:0.7734 | best_precision@10:0.4541 | best_precision@20:0.2333 | 
epoch: 90, training loss: 92.93162679991656, train time: 245.84065127372742
epoch: 91, training loss: 92.97137660070439, train time: 245.2905077934265
epoch: 92, training loss: 92.8887224226637, train time: 245.67874336242676
epoch: 93, training loss: 93.14128796282603, train time: 245.5946924686432
epoch: 94, training loss: 92.14027021844959, train time: 245.62701773643494
epoch: 95, training loss: 92.75162546619686, train time: 245.33426189422607
epoch: 96, training loss: 92.0650657642691, train time: 245.41107559204102
epoch: 97, training loss: 91.15975892407005, train time: 245.47890520095825
epoch: 98, training loss: 91.44548608746118, train time: 245.40733313560486
epoch: 99, training loss: 92.84961300112627, train time: 245.84029841423035
epoch: 100, training loss: 93.36457237535069, train time: 245.437997341156
epoch: 101, training loss: 92.13396458641364, train time: 245.4114203453064
epoch: 102, training loss: 92.39419290594378, train time: 245.3596978187561
epoch: 103, training loss: 93.02632459984306, train time: 245.53874969482422
epoch: 104, training loss: 93.94039788689406, train time: 245.73766803741455
epoch: 105, training loss: 92.52553165964491, train time: 245.48435139656067
epoch: 106, training loss: 91.80806639722141, train time: 245.64445066452026
epoch: 107, training loss: 91.6829455776533, train time: 245.50995874404907
epoch: 108, training loss: 93.44026653475157, train time: 245.4358057975769
epoch: 109, training loss: 93.42067253651476, train time: 246.09285879135132
epoch: 110, training loss: 93.09396663882217, train time: 245.47020363807678
epoch: 111, training loss: 93.65527072377154, train time: 245.39935755729675
epoch: 112, training loss: 93.56346262320585, train time: 245.9760103225708
epoch: 113, training loss: 93.95064670727879, train time: 245.86823177337646
epoch: 114, training loss: 95.28226209490094, train time: 245.43099236488342
epoch: 115, training loss: 93.23973953659151, train time: 245.81371879577637
epoch: 116, training loss: 95.09144540828856, train time: 245.07923197746277
epoch: 117, training loss: 94.2361268671666, train time: 245.72011494636536
epoch: 118, training loss: 94.16875058485311, train time: 245.46678495407104
epoch: 119, training loss: 93.25259019355872, train time: 246.0095181465149
epo:119 | HR@5:0.8497 | HR@10:0.8675 | HR@20:0.8928 | NDCG@5:0.4337 | NDCG@10:0.4770 | NDCG@20:0.5298 | recall@5:0.6226 | recall@10:0.7100 | recall@20:0.7344 | precision@5:0.7472 | precision@10:0.4260 | precision@20:0.2203 | best_HR@5:0.9069 | best_HR@10:0.9229 | best_HR@20:0.9407 | best_NDCG@5:0.4400 | best_NDCG@10:0.4825 | best_NDCG@20:0.5349 | best_recall@5:0.6445 | best_recall@10:0.7568 | best_recall@20:0.7775 | best_precision@5:0.7734 | best_precision@10:0.4541 | best_precision@20:0.2333 | 
epoch: 120, training loss: 95.26227746508084, train time: 245.6329426765442
epoch: 121, training loss: 96.25405752023653, train time: 251.89562559127808
epoch: 122, training loss: 95.50155856274068, train time: 246.4501473903656
epoch: 123, training loss: 94.89685084555094, train time: 245.87703704833984
epoch: 124, training loss: 93.76365049293963, train time: 245.1192696094513
epoch: 125, training loss: 95.33648490480118, train time: 245.85027170181274
epoch: 126, training loss: 95.69213576117909, train time: 245.86206984519958
epoch: 127, training loss: 93.23658249301661, train time: 245.32530999183655
epoch: 128, training loss: 94.7700643961798, train time: 244.95520520210266
epoch: 129, training loss: 94.65197597124643, train time: 245.5961184501648
epoch: 130, training loss: 96.03525203015306, train time: 245.73438954353333
epoch: 131, training loss: 97.69952354896668, train time: 245.6504578590393
epoch: 132, training loss: 97.5472520695912, train time: 245.38453221321106
epoch: 133, training loss: 95.59075597713672, train time: 245.8671441078186
epoch: 134, training loss: 97.48907269605115, train time: 245.69882035255432
epoch: 135, training loss: 95.39337490012258, train time: 245.55890583992004
epoch: 136, training loss: 96.73177964250499, train time: 245.4205071926117
epoch: 137, training loss: 96.82775846448203, train time: 245.5800004005432
epoch: 138, training loss: 97.62904720912775, train time: 245.58285975456238
epoch: 139, training loss: 95.7416663234253, train time: 245.61031579971313
epoch: 140, training loss: 95.68123471880244, train time: 245.9294409751892
epoch: 141, training loss: 95.95119601663464, train time: 245.99409556388855
epoch: 142, training loss: 97.1269359865837, train time: 245.77478003501892
epoch: 143, training loss: 95.51568174340355, train time: 245.71986484527588
epoch: 144, training loss: 96.62464484631346, train time: 246.22558164596558
epoch: 145, training loss: 95.66594827517838, train time: 245.47925472259521
epoch: 146, training loss: 98.24585036872304, train time: 245.6063892841339
epoch: 147, training loss: 96.33197925746208, train time: 245.71686172485352
epoch: 148, training loss: 97.69689549284521, train time: 246.28149938583374
epoch: 149, training loss: 98.17505861524842, train time: 245.6946315765381
epo:149 | HR@5:0.8526 | HR@10:0.8705 | HR@20:0.8937 | NDCG@5:0.4244 | NDCG@10:0.4684 | NDCG@20:0.5222 | recall@5:0.6218 | recall@10:0.7129 | recall@20:0.7371 | precision@5:0.7462 | precision@10:0.4277 | precision@20:0.2211 | best_HR@5:0.9069 | best_HR@10:0.9229 | best_HR@20:0.9407 | best_NDCG@5:0.4400 | best_NDCG@10:0.4825 | best_NDCG@20:0.5349 | best_recall@5:0.6445 | best_recall@10:0.7568 | best_recall@20:0.7775 | best_precision@5:0.7734 | best_precision@10:0.4541 | best_precision@20:0.2333 | 
training finish
