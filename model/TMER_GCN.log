nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.0994415283203125e-06
user  100 time:  237.75131011009216
user  200 time:  477.98977541923523
user  300 time:  718.8295414447784
user  400 time:  959.7381265163422
user  500 time:  1199.5680420398712
user  600 time:  1441.0670585632324
user  700 time:  1680.1157779693604
user  800 time:  1913.8862161636353
user  900 time:  2152.7551679611206
user  1000 time:  2393.6160192489624
user  1100 time:  2633.6576890945435
user  1200 time:  2876.6904509067535
user  1300 time:  3119.9131648540497
user  1400 time:  3362.7475924491882
start training item-item instance self attention module...
user  0 time:  1.0728836059570312e-05
user  100 time:  205.45623207092285
user  200 time:  405.12969875335693
user  300 time:  615.9016845226288
user  400 time:  822.7528698444366
user  500 time:  1027.4231305122375
user  600 time:  1232.8300788402557
user  700 time:  1438.6192636489868
user  800 time:  1641.046915769577
user  900 time:  1844.3512871265411
user  1000 time:  2045.9311215877533
user  1100 time:  2246.3019590377808
user  1200 time:  2443.494942188263
user  1300 time:  2653.44131398201
user  1400 time:  2853.2193319797516
start updating user and item embedding...
user_name:1450
user  0 time:  1.430511474609375e-05
user  100 time:  25.91550922393799
user  200 time:  51.704415798187256
user  300 time:  77.65488934516907
user  400 time:  103.54578709602356
user  500 time:  129.54510879516602
user  600 time:  155.68341517448425
user  700 time:  181.85772275924683
user  800 time:  207.97683119773865
user  900 time:  233.95810437202454
user  1000 time:  259.90980553627014
user  1100 time:  286.0555238723755
user  1200 time:  312.01326608657837
user  1300 time:  338.1668255329132
user  1400 time:  364.3441812992096
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:56: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 340.4311787757906, train time: 23.187482357025146
epoch: 1, training loss: 338.0265842729714, train time: 23.429702520370483
epoch: 2, training loss: 336.80867788149044, train time: 23.30032467842102
epoch: 3, training loss: 337.81045453832485, train time: 23.385475158691406
epoch: 4, training loss: 337.6302754423814, train time: 22.36941957473755
epoch: 5, training loss: 337.2809811369516, train time: 23.231781721115112
epoch: 6, training loss: 336.22984052845277, train time: 22.83297824859619
epoch: 7, training loss: 337.6044370054733, train time: 23.23599886894226
epoch: 8, training loss: 336.58740220498294, train time: 22.394938230514526
epoch: 9, training loss: 337.13442783313803, train time: 22.990966320037842
epoch: 10, training loss: 338.43901929515414, train time: 23.089930057525635
epoch: 11, training loss: 335.8194095401559, train time: 23.28106951713562
epoch: 12, training loss: 336.56930819968693, train time: 23.27491044998169
epoch: 13, training loss: 336.29864021996036, train time: 22.792158126831055
epoch: 14, training loss: 337.6049592040945, train time: 23.520874738693237
epoch: 15, training loss: 338.08564155269414, train time: 23.36220669746399
epoch: 16, training loss: 338.1032736687921, train time: 22.850342988967896
epoch: 17, training loss: 336.61042439495213, train time: 23.334816932678223
epoch: 18, training loss: 337.1221063330304, train time: 23.696842670440674
epoch: 19, training loss: 338.3037136089988, train time: 22.92099380493164
epoch: 20, training loss: 338.96740469313227, train time: 23.084222555160522
epoch: 21, training loss: 336.542830305174, train time: 23.072461128234863
epoch: 22, training loss: 336.1724653551355, train time: 22.7899751663208
epoch: 23, training loss: 337.7005852381699, train time: 23.50063443183899
epoch: 24, training loss: 337.24728686665185, train time: 23.207271099090576
epoch: 25, training loss: 336.97019712626934, train time: 23.353786945343018
epoch: 26, training loss: 338.15623717661947, train time: 23.12075924873352
epoch: 27, training loss: 337.47068872954696, train time: 23.225666761398315
epoch: 28, training loss: 336.727279051207, train time: 22.91409683227539
epoch: 29, training loss: 336.8655738676898, train time: 22.96221423149109
epoch: 30, training loss: 334.7499769083224, train time: 23.295433044433594
epoch: 31, training loss: 338.83665899257176, train time: 23.0613853931427
epoch: 32, training loss: 337.6733563262969, train time: 23.180540084838867
epoch: 33, training loss: 338.1559204533696, train time: 22.89598512649536
epoch: 34, training loss: 336.6999313526321, train time: 23.208324432373047
epoch: 35, training loss: 336.13242365489714, train time: 23.01522970199585
epoch: 36, training loss: 338.18511604866944, train time: 22.861282110214233
epoch: 37, training loss: 336.1448616858106, train time: 23.27017569541931
epoch: 38, training loss: 336.32516172900796, train time: 22.67214298248291
epoch: 39, training loss: 338.00666720978916, train time: 23.138529539108276
epoch: 40, training loss: 336.964889731491, train time: 23.256091594696045
epoch: 41, training loss: 337.4255053016823, train time: 22.32426953315735
epoch: 42, training loss: 337.05667278310284, train time: 22.934328079223633
epoch: 43, training loss: 337.4147728269454, train time: 23.107704877853394
epoch: 44, training loss: 336.84526175423525, train time: 23.270256280899048
epoch: 45, training loss: 336.47078577335924, train time: 22.955493450164795
epoch: 46, training loss: 338.35721143661067, train time: 23.076395750045776
epoch: 47, training loss: 337.21512660733424, train time: 23.358823776245117
epoch: 48, training loss: 337.81147627253085, train time: 23.307288646697998
epoch: 49, training loss: 337.75543077196926, train time: 23.464844465255737
epo:49|HR@1:0.0533 | HR@5:0.1364 | HR@10:0.2170 | HR@20:0.3394 | HR@50:0.6148 | NDCG@1:0.4252 | NDCG@5:0.4693 | NDCG@10:0.4965| NDCG@20:0.5374| NDCG@50:0.6361| best_HR@1:0.0533 | best_HR@5:0.1364 | best_HR@10:0.2170 | best_HR@20:0.3394 | best_HR@50:0.6148 | best_NDCG@1:0.4252 | best_NDCG@5:0.4693 | best_NDCG@10:0.4965 | best_NDCG@20:0.5374 | best_NDCG@50:0.6361 | train_time:23.46 | test_time:408.91
epoch: 50, training loss: 338.541665175464, train time: 23.101135730743408
epoch: 51, training loss: 338.5660124721471, train time: 23.314090490341187
epoch: 52, training loss: 337.90436038468033, train time: 23.31383490562439
epoch: 53, training loss: 339.6906953349244, train time: 22.759633779525757
epoch: 54, training loss: 338.8258960349485, train time: 23.534581184387207
epoch: 55, training loss: 338.3429980566725, train time: 23.365935802459717
epoch: 56, training loss: 338.7336136989761, train time: 23.240541219711304
epoch: 57, training loss: 338.23557312833145, train time: 22.723835945129395
epoch: 58, training loss: 337.2770596863702, train time: 22.784453868865967
epoch: 59, training loss: 338.6880679577589, train time: 22.756796836853027
epoch: 60, training loss: 337.93989901058376, train time: 23.521201133728027
epoch: 61, training loss: 337.5068738798145, train time: 22.784425020217896
epoch: 62, training loss: 336.0793114416301, train time: 23.413440942764282
epoch: 63, training loss: 336.8601137427613, train time: 22.95436930656433
epoch: 64, training loss: 337.10822821129113, train time: 22.7939190864563
epoch: 65, training loss: 339.962209302932, train time: 22.888836145401
epoch: 66, training loss: 338.56755176302977, train time: 23.120237588882446
epoch: 67, training loss: 338.6659089976456, train time: 22.934419870376587
epoch: 68, training loss: 339.4379237566609, train time: 22.513896703720093
epoch: 69, training loss: 338.04872388672084, train time: 22.801862955093384
epoch: 70, training loss: 339.23018120927736, train time: 23.272639751434326
epoch: 71, training loss: 338.25290831224993, train time: 22.9306583404541
epoch: 72, training loss: 337.5925591336563, train time: 22.805927991867065
epoch: 73, training loss: 338.3617747807875, train time: 22.78692364692688
epoch: 74, training loss: 337.62645642412826, train time: 22.698240041732788
epoch: 75, training loss: 336.7555888975039, train time: 23.21845531463623
epoch: 76, training loss: 336.45683960127644, train time: 23.18720555305481
epoch: 77, training loss: 336.79556523193605, train time: 22.741918325424194
epoch: 78, training loss: 336.55090186046436, train time: 22.889677047729492
epoch: 79, training loss: 337.2373212033417, train time: 22.787298917770386
epoch: 80, training loss: 337.50511821592227, train time: 22.835683822631836
epoch: 81, training loss: 337.56846789876, train time: 23.25664234161377
epoch: 82, training loss: 336.19292676541954, train time: 22.869508743286133
epoch: 83, training loss: 336.28205176396295, train time: 23.30743169784546
epoch: 84, training loss: 336.49150832602754, train time: 23.048009157180786
epoch: 85, training loss: 337.46707290620543, train time: 22.990813970565796
epoch: 86, training loss: 337.22494076471776, train time: 22.704487085342407
epoch: 87, training loss: 337.7177469613962, train time: 22.830900192260742
epoch: 88, training loss: 336.351942503592, train time: 22.5973002910614
epoch: 89, training loss: 336.4051903532818, train time: 28.00300908088684
epoch: 90, training loss: 337.216944551561, train time: 28.872273445129395
epoch: 91, training loss: 338.36158702359535, train time: 29.567911863327026
epoch: 92, training loss: 338.3837914210744, train time: 27.837779760360718
epoch: 93, training loss: 339.2162868496962, train time: 32.140207290649414
epoch: 94, training loss: 339.36179310665466, train time: 35.395081996917725
epoch: 95, training loss: 337.6245144989807, train time: 37.32189583778381
epoch: 96, training loss: 338.20081419171765, train time: 36.219504833221436
epoch: 97, training loss: 339.3059039111249, train time: 36.73910641670227
epoch: 98, training loss: 338.7611944354139, train time: 38.18407893180847
epoch: 99, training loss: 339.4801472024992, train time: 37.01216006278992
epo:99|HR@1:0.0345 | HR@5:0.1040 | HR@10:0.1682 | HR@20:0.2801 | HR@50:0.5695 | NDCG@1:0.4990 | NDCG@5:0.5442 | NDCG@10:0.5722| NDCG@20:0.6121| NDCG@50:0.7033| best_HR@1:0.0533 | best_HR@5:0.1364 | best_HR@10:0.2170 | best_HR@20:0.3394 | best_HR@50:0.6148 | best_NDCG@1:0.4990 | best_NDCG@5:0.5442 | best_NDCG@10:0.5722 | best_NDCG@20:0.6121 | best_NDCG@50:0.7033 | train_time:37.01 | test_time:408.02
training finish
