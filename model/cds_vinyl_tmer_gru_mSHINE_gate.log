nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_CDs_Vinyl......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  9.775161743164062e-06
user  100 time:  16.34941792488098
user  200 time:  16.801759958267212
user  300 time:  17.698063373565674
user  400 time:  18.56212615966797
user  500 time:  18.564330577850342
user  600 time:  23.269446849822998
user  700 time:  24.537819385528564
user  800 time:  24.540417194366455
user  900 time:  30.036285877227783
user  1000 time:  30.516831159591675
user  1100 time:  35.30831575393677
user  1200 time:  36.648733615875244
user  1300 time:  47.16987085342407
user  1400 time:  53.04115271568298
user  1500 time:  67.9966893196106
user  1600 time:  72.84149837493896
user  1700 time:  72.84367418289185
user  1800 time:  76.3604519367218
user  1900 time:  77.71525454521179
start training item-item instance self attention module...
user  0 time:  4.5299530029296875e-06
user  100 time:  90.92613363265991
user  200 time:  188.49195837974548
user  300 time:  277.7621102333069
user  400 time:  361.48018431663513
user  500 time:  443.49536418914795
user  600 time:  528.5188851356506
user  700 time:  610.0908319950104
user  800 time:  696.5357046127319
user  900 time:  795.2942192554474
user  1000 time:  884.9626231193542
user  1100 time:  973.1125519275665
user  1200 time:  1057.6140539646149
user  1300 time:  1142.8225843906403
user  1400 time:  1222.951593875885
user  1500 time:  1313.2601027488708
user  1600 time:  1393.1430370807648
user  1700 time:  1482.447467327118
user  1800 time:  1575.510198354721
user  1900 time:  1670.3124663829803
start updating user and item embedding...
user_name:2000
user  0 time:  8.106231689453125e-06
user  100 time:  19.068434238433838
user  200 time:  39.05101537704468
user  300 time:  58.77246451377869
user  400 time:  78.68223595619202
user  500 time:  98.35753631591797
user  600 time:  117.74503874778748
user  700 time:  137.38854694366455
user  800 time:  157.37291169166565
user  900 time:  176.81002926826477
user  1000 time:  195.72489404678345
user  1100 time:  215.36936235427856
user  1200 time:  234.06304788589478
user  1300 time:  253.30609393119812
user  1400 time:  272.28638648986816
user  1500 time:  291.57908844947815
user  1600 time:  310.2166039943695
user  1700 time:  329.95189094543457
user  1800 time:  349.31547498703003
user  1900 time:  368.0744068622589
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 360.0523796309717, train time: 325.32216596603394
epoch: 1, training loss: 326.33701520692557, train time: 324.94165205955505
epoch: 2, training loss: 232.30654922290705, train time: 324.29016613960266
epoch: 3, training loss: 212.1980294607347, train time: 323.79197216033936
epoch: 4, training loss: 201.36056949570775, train time: 324.88940691947937
epoch: 5, training loss: 194.03598240064457, train time: 325.1278133392334
epoch: 6, training loss: 188.47083316219505, train time: 322.8801152706146
epoch: 7, training loss: 182.78537372295978, train time: 325.21361327171326
epoch: 8, training loss: 178.17888346704422, train time: 322.7511143684387
epoch: 9, training loss: 174.21418123028707, train time: 321.81840324401855
epoch: 10, training loss: 170.12518106820062, train time: 324.70279145240784
epoch: 11, training loss: 166.53712675935822, train time: 323.9814238548279
epoch: 12, training loss: 163.32193070440553, train time: 322.5155875682831
epoch: 13, training loss: 160.3311901887646, train time: 323.18297958374023
epoch: 14, training loss: 157.02797227055999, train time: 321.92056035995483
epoch: 15, training loss: 154.37025763705606, train time: 325.17710852622986
epoch: 16, training loss: 151.03243004280375, train time: 322.7247471809387
epoch: 17, training loss: 148.53793817420956, train time: 323.745361328125
epoch: 18, training loss: 145.97851115034427, train time: 322.7473945617676
epoch: 19, training loss: 143.96708743216004, train time: 326.58306217193604
epoch: 20, training loss: 141.98516125854803, train time: 325.380966424942
epoch: 21, training loss: 139.26412230805727, train time: 324.32780957221985
epoch: 22, training loss: 137.54057578160428, train time: 325.6780574321747
epoch: 23, training loss: 135.64565276080975, train time: 323.3945848941803
epoch: 24, training loss: 133.8062906039995, train time: 321.89031624794006
epoch: 25, training loss: 131.967573535454, train time: 326.17896008491516
epoch: 26, training loss: 130.71913742111064, train time: 325.72192668914795
epoch: 27, training loss: 129.02521739157964, train time: 325.1194272041321
epoch: 28, training loss: 127.20293894023052, train time: 325.4029655456543
epoch: 29, training loss: 125.32852558462764, train time: 321.8234295845032
epo:29 | HR@5:0.8870 | HR@10:0.9603 | HR@20:0.9890 | NDCG@5:0.4673 | NDCG@10:0.4933 | NDCG@20:0.5338 | recall@5:0.5443 | recall@10:0.7539 | recall@20:0.8180 | precision@5:0.6532 | precision@10:0.4523 | precision@20:0.2454 | best_HR@5:0.8870 | best_HR@10:0.9603 | best_HR@20:0.9890 | best_NDCG@5:0.4673 | best_NDCG@10:0.4933 | best_NDCG@20:0.5338 | best_recall@5:0.5443 | best_recall@10:0.7539 | best_recall@20:0.8180 | best_precision@5:0.6532 | best_precision@10:0.4523 | best_precision@20:0.2454 | 
epoch: 30, training loss: 124.15539617507602, train time: 332.47546792030334
epoch: 31, training loss: 122.37165285929223, train time: 336.8009717464447
epoch: 32, training loss: 121.19516772311181, train time: 337.22183632850647
epoch: 33, training loss: 119.72030176705448, train time: 338.8878698348999
epoch: 34, training loss: 118.52214762258518, train time: 342.3430416584015
epoch: 35, training loss: 117.57922498518019, train time: 327.8013336658478
epoch: 36, training loss: 115.95284031325718, train time: 301.9633414745331
epoch: 37, training loss: 114.96303568757139, train time: 294.77297592163086
epoch: 38, training loss: 114.09911145779188, train time: 302.7118673324585
epoch: 39, training loss: 112.56460295483703, train time: 334.6361858844757
epoch: 40, training loss: 111.5665619315696, train time: 337.6581778526306
epoch: 41, training loss: 110.63633402527194, train time: 337.07632398605347
epoch: 42, training loss: 109.5495874126209, train time: 338.56693744659424
epoch: 43, training loss: 108.45630382050877, train time: 337.81841564178467
epoch: 44, training loss: 107.31726178161625, train time: 316.14484667778015
epoch: 45, training loss: 106.57638375857641, train time: 310.72661995887756
epoch: 46, training loss: 105.64586996569415, train time: 315.71938037872314
epoch: 47, training loss: 104.68898139379598, train time: 337.9495339393616
epoch: 48, training loss: 104.04168834735174, train time: 337.86359190940857
epoch: 49, training loss: 102.47788030121592, train time: 337.5406777858734
epoch: 50, training loss: 101.70891763922555, train time: 337.6870627403259
epoch: 51, training loss: 101.18978491451708, train time: 337.2782874107361
epoch: 52, training loss: 100.00013070917339, train time: 315.3007278442383
epoch: 53, training loss: 98.88721992071805, train time: 309.6035614013672
epoch: 54, training loss: 97.98315304152493, train time: 312.7089374065399
epoch: 55, training loss: 97.14687195116858, train time: 336.87169766426086
epoch: 56, training loss: 95.9824415528783, train time: 339.3221986293793
epoch: 57, training loss: 95.1634553891563, train time: 338.2982015609741
epoch: 58, training loss: 93.87475572129551, train time: 338.4523205757141
epoch: 59, training loss: 92.7274254670483, train time: 338.48376727104187
epo:59 | HR@5:0.8820 | HR@10:0.9563 | HR@20:0.9898 | NDCG@5:0.4739 | NDCG@10:0.4997 | NDCG@20:0.5401 | recall@5:0.5423 | recall@10:0.7516 | recall@20:0.8174 | precision@5:0.6508 | precision@10:0.4509 | precision@20:0.2452 | best_HR@5:0.8870 | best_HR@10:0.9603 | best_HR@20:0.9898 | best_NDCG@5:0.4739 | best_NDCG@10:0.4997 | best_NDCG@20:0.5401 | best_recall@5:0.5443 | best_recall@10:0.7539 | best_recall@20:0.8180 | best_precision@5:0.6532 | best_precision@10:0.4523 | best_precision@20:0.2454 | 
epoch: 60, training loss: 91.7095916500839, train time: 332.70658898353577
epoch: 61, training loss: 90.25269397833472, train time: 337.3878502845764
epoch: 62, training loss: 89.05436853989886, train time: 339.10621333122253
epoch: 63, training loss: 88.00011144125892, train time: 336.9195864200592
epoch: 64, training loss: 86.74345423439081, train time: 339.0808460712433
epoch: 65, training loss: 85.35816109118605, train time: 326.9962229728699
epoch: 66, training loss: 84.24794815852874, train time: 308.65166091918945
epoch: 67, training loss: 83.20535982731963, train time: 312.1919250488281
epoch: 68, training loss: 81.69798109282601, train time: 308.19158959388733
epoch: 69, training loss: 80.6002346213536, train time: 302.95228099823
epoch: 70, training loss: 78.89611237078861, train time: 320.06690645217896
epoch: 71, training loss: 77.70800802810845, train time: 319.1306314468384
epoch: 72, training loss: 75.72383029537559, train time: 325.97829389572144
epoch: 73, training loss: 74.60120035666841, train time: 326.3628444671631
epoch: 74, training loss: 73.34766584690533, train time: 326.34098768234253
epoch: 75, training loss: 72.26859426204078, train time: 326.612664937973
epoch: 76, training loss: 70.49762555476082, train time: 326.68819975852966
epoch: 77, training loss: 69.24571842114437, train time: 327.12085461616516
epoch: 78, training loss: 67.03352336725584, train time: 327.65426087379456
epoch: 79, training loss: 66.58845219423256, train time: 327.3807291984558
epoch: 80, training loss: 65.52730683740538, train time: 328.7258610725403
epoch: 81, training loss: 63.50431093880616, train time: 326.49737644195557
epoch: 82, training loss: 61.96863604280452, train time: 327.2655484676361
epoch: 83, training loss: 60.91011027795594, train time: 326.7884886264801
epoch: 84, training loss: 59.47486367428894, train time: 326.03469705581665
epoch: 85, training loss: 58.274796908248845, train time: 327.41602754592896
epoch: 86, training loss: 56.85303278762149, train time: 327.90924406051636
epoch: 87, training loss: 55.64994365310099, train time: 326.9292321205139
epoch: 88, training loss: 54.26364160240428, train time: 319.54690194129944
epoch: 89, training loss: 53.63034408124145, train time: 301.84979248046875
epo:89 | HR@5:0.8579 | HR@10:0.9454 | HR@20:0.9864 | NDCG@5:0.5060 | NDCG@10:0.5303 | NDCG@20:0.5687 | recall@5:0.5247 | recall@10:0.7331 | recall@20:0.8123 | precision@5:0.6297 | precision@10:0.4398 | precision@20:0.2437 | best_HR@5:0.8870 | best_HR@10:0.9603 | best_HR@20:0.9898 | best_NDCG@5:0.5060 | best_NDCG@10:0.5303 | best_NDCG@20:0.5687 | best_recall@5:0.5443 | best_recall@10:0.7539 | best_recall@20:0.8180 | best_precision@5:0.6532 | best_precision@10:0.4523 | best_precision@20:0.2454 | 
epoch: 90, training loss: 52.125886167766794, train time: 301.49540662765503
epoch: 91, training loss: 50.73269360549011, train time: 302.6162624359131
epoch: 92, training loss: 49.61083138394213, train time: 303.1799826622009
epoch: 93, training loss: 48.758836671789595, train time: 301.30502462387085
epoch: 94, training loss: 47.72305189205042, train time: 303.07228112220764
epoch: 95, training loss: 46.06744674750985, train time: 302.8446002006531
epoch: 96, training loss: 46.04379492343293, train time: 303.32252192497253
epoch: 97, training loss: 44.96233379029229, train time: 302.0389325618744
epoch: 98, training loss: 43.12578849593342, train time: 302.6851136684418
epoch: 99, training loss: 42.3957916406049, train time: 303.5986762046814
epoch: 100, training loss: 41.701334943535585, train time: 303.0643427371979
epoch: 101, training loss: 40.15978260637144, train time: 303.2066740989685
epoch: 102, training loss: 39.6407783979015, train time: 303.5131058692932
epoch: 103, training loss: 38.86349111562754, train time: 291.85941195487976
epoch: 104, training loss: 38.79384356850841, train time: 291.5491681098938
epoch: 105, training loss: 37.05524590836774, train time: 297.28021693229675
epoch: 106, training loss: 36.43191693566462, train time: 302.27783703804016
epoch: 107, training loss: 35.972892807030206, train time: 302.87689781188965
epoch: 108, training loss: 35.12434232646116, train time: 303.3708465099335
epoch: 109, training loss: 34.88274697673063, train time: 301.9281928539276
epoch: 110, training loss: 33.779791225677215, train time: 302.2758548259735
epoch: 111, training loss: 33.909560965639265, train time: 301.5728027820587
epoch: 112, training loss: 32.108254724518275, train time: 301.88361740112305
epoch: 113, training loss: 32.03450294643508, train time: 301.4394407272339
epoch: 114, training loss: 32.327861968998675, train time: 301.3982560634613
epoch: 115, training loss: 31.175961634347168, train time: 302.8334813117981
epoch: 116, training loss: 29.848867884751012, train time: 301.97512221336365
epoch: 117, training loss: 30.7014977400938, train time: 302.12671875953674
epoch: 118, training loss: 28.922164268294665, train time: 301.97192573547363
epoch: 119, training loss: 29.525063998450737, train time: 302.53353333473206
epo:119 | HR@5:0.8458 | HR@10:0.9413 | HR@20:0.9844 | NDCG@5:0.5079 | NDCG@10:0.5327 | NDCG@20:0.5714 | recall@5:0.5169 | recall@10:0.7277 | recall@20:0.8098 | precision@5:0.6203 | precision@10:0.4366 | precision@20:0.2429 | best_HR@5:0.8870 | best_HR@10:0.9603 | best_HR@20:0.9898 | best_NDCG@5:0.5079 | best_NDCG@10:0.5327 | best_NDCG@20:0.5714 | best_recall@5:0.5443 | best_recall@10:0.7539 | best_recall@20:0.8180 | best_precision@5:0.6532 | best_precision@10:0.4523 | best_precision@20:0.2454 | 
epoch: 120, training loss: 28.16524465823651, train time: 303.52015447616577
epoch: 121, training loss: 27.481048199060545, train time: 302.6362566947937
epoch: 122, training loss: 27.99632901765078, train time: 302.6603057384491
epoch: 123, training loss: 27.12873008792655, train time: 301.631489276886
epoch: 124, training loss: 26.885830323642303, train time: 301.7132694721222
epoch: 125, training loss: 26.398089157090844, train time: 300.7387623786926
epoch: 126, training loss: 25.85934946891166, train time: 302.20984649658203
epoch: 127, training loss: 25.46990289438981, train time: 301.7743000984192
epoch: 128, training loss: 25.62909132821362, train time: 303.0699598789215
epoch: 129, training loss: 24.966894069031227, train time: 302.68451738357544
epoch: 130, training loss: 24.780856247237, train time: 302.1940338611603
epoch: 131, training loss: 24.18600234917006, train time: 302.0486035346985
epoch: 132, training loss: 23.899448072993255, train time: 302.6820869445801
epoch: 133, training loss: 24.159456744024745, train time: 301.92133378982544
epoch: 134, training loss: 22.97385216275577, train time: 301.71105432510376
epoch: 135, training loss: 22.943660410368654, train time: 301.4106090068817
epoch: 136, training loss: 24.229920119819443, train time: 294.87759923934937
epoch: 137, training loss: 21.731620299629846, train time: 290.8000133037567
epoch: 138, training loss: 23.436990764901772, train time: 291.9021728038788
epoch: 139, training loss: 21.76922030635123, train time: 301.5528042316437
epoch: 140, training loss: 21.676548380547658, train time: 301.85273146629333
epoch: 141, training loss: 21.897912058801765, train time: 303.91018652915955
epoch: 142, training loss: 21.680134118556403, train time: 302.0523691177368
epoch: 143, training loss: 20.795313600039588, train time: 302.2156653404236
epoch: 144, training loss: 22.008040899210584, train time: 301.8400025367737
epoch: 145, training loss: 20.52248547996646, train time: 303.3473377227783
epoch: 146, training loss: 21.43193314057467, train time: 302.07694959640503
epoch: 147, training loss: 19.663949522977347, train time: 302.50261330604553
epoch: 148, training loss: 19.316698496031886, train time: 301.90395045280457
epoch: 149, training loss: 21.256916212985395, train time: 304.1727201938629
epo:149 | HR@5:0.8389 | HR@10:0.9333 | HR@20:0.9822 | NDCG@5:0.5056 | NDCG@10:0.5305 | NDCG@20:0.5695 | recall@5:0.5157 | recall@10:0.7224 | recall@20:0.8058 | precision@5:0.6189 | precision@10:0.4334 | precision@20:0.2417 | best_HR@5:0.8870 | best_HR@10:0.9603 | best_HR@20:0.9898 | best_NDCG@5:0.5079 | best_NDCG@10:0.5327 | best_NDCG@20:0.5714 | best_recall@5:0.5443 | best_recall@10:0.7539 | best_recall@20:0.8180 | best_precision@5:0.6532 | best_precision@10:0.4523 | best_precision@20:0.2454 | 
training finish
