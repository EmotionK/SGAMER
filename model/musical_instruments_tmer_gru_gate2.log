nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  7.62939453125e-06
user  100 time:  372.3237383365631
user  200 time:  745.5106251239777
user  300 time:  1123.481718301773
user  400 time:  1507.9480102062225
user  500 time:  1893.8483109474182
user  600 time:  2275.4976556301117
user  700 time:  2650.096209049225
user  800 time:  3028.616619348526
user  900 time:  3416.2452917099
user  1000 time:  3809.061061859131
user  1100 time:  4175.853539943695
user  1200 time:  4543.775534152985
user  1300 time:  4909.051336765289
user  1400 time:  5276.788353681564
start training item-item instance self attention module...
user  0 time:  5.0067901611328125e-06
user  100 time:  254.87229585647583
user  200 time:  508.44054317474365
user  300 time:  768.5404450893402
user  400 time:  1030.7788784503937
user  500 time:  1285.1357753276825
user  600 time:  1523.1288623809814
user  700 time:  1786.6953246593475
user  800 time:  2029.9296271800995
user  900 time:  2282.0962731838226
user  1000 time:  2517.2565348148346
user  1100 time:  2763.055813550949
user  1200 time:  3020.0147020816803
user  1300 time:  3283.397031068802
user  1400 time:  3540.2272152900696
start updating user and item embedding...
user_name:1450
user  0 time:  9.5367431640625e-06
user  100 time:  17.721194982528687
user  200 time:  35.78071904182434
user  300 time:  54.14058232307434
user  400 time:  71.91569113731384
user  500 time:  89.95611190795898
user  600 time:  108.13658475875854
user  700 time:  126.38790702819824
user  800 time:  143.78141832351685
user  900 time:  162.42191529273987
user  1000 time:  180.2666039466858
user  1100 time:  197.8025939464569
user  1200 time:  215.50960683822632
user  1300 time:  234.03277158737183
user  1400 time:  251.8341612815857
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 153.66965092212195, train time: 31.13059973716736
epoch: 1, training loss: 88.0038172879722, train time: 30.965086460113525
epoch: 2, training loss: 74.0928972199763, train time: 30.91674304008484
epoch: 3, training loss: 66.87440683241584, train time: 31.298214435577393
epoch: 4, training loss: 61.29691189360892, train time: 30.434370517730713
epoch: 5, training loss: 57.980373202546616, train time: 30.946749448776245
epoch: 6, training loss: 54.5795568577596, train time: 31.209024667739868
epoch: 7, training loss: 50.938094187760726, train time: 30.91580867767334
epoch: 8, training loss: 48.497930560704845, train time: 31.06423854827881
epoch: 9, training loss: 46.16341546204785, train time: 30.763927698135376
epoch: 10, training loss: 43.902625631177216, train time: 30.969404458999634
epoch: 11, training loss: 42.72106492376042, train time: 30.59522843360901
epoch: 12, training loss: 40.6782531360368, train time: 30.492364645004272
epoch: 13, training loss: 38.88717942014773, train time: 30.78166675567627
epoch: 14, training loss: 36.659595653109136, train time: 30.607368230819702
epoch: 15, training loss: 35.35532893992786, train time: 30.65094566345215
epoch: 16, training loss: 33.84744737318397, train time: 31.278986930847168
epoch: 17, training loss: 32.92247890732688, train time: 31.381377696990967
epoch: 18, training loss: 31.219857724005124, train time: 31.131557941436768
epoch: 19, training loss: 29.36415534835396, train time: 30.81485652923584
epoch: 20, training loss: 27.491682828065677, train time: 30.7481369972229
epoch: 21, training loss: 26.489933927648963, train time: 31.045170068740845
epoch: 22, training loss: 25.12006287829081, train time: 31.019850730895996
epoch: 23, training loss: 24.10283430283198, train time: 31.066807746887207
epoch: 24, training loss: 23.62928885110523, train time: 31.645358562469482
epoch: 25, training loss: 22.349772337089234, train time: 30.85263204574585
epoch: 26, training loss: 21.077685186678536, train time: 30.9807231426239
epoch: 27, training loss: 20.734007372954693, train time: 31.45668864250183
epoch: 28, training loss: 19.326845041597608, train time: 30.615240335464478
epoch: 29, training loss: 19.07969755602062, train time: 30.9512996673584
epo:29 | HR@5:0.7989 | HR@10:0.8409 | HR@20:0.8877 | NDCG@5:0.4408 | NDCG@10:0.4804 | NDCG@20:0.5312 | recall@5:0.5598 | recall@10:0.6728 | recall@20:0.7244 | precision@5:0.6717 | precision@10:0.4037 | precision@20:0.2173 | best_HR@5:0.7989 | best_HR@10:0.8409 | best_HR@20:0.8877 | best_NDCG@5:0.4408 | best_NDCG@10:0.4804 | best_NDCG@20:0.5312 | best_recall@5:0.5598 | best_recall@10:0.6728 | best_recall@20:0.7244 | best_precision@5:0.6717 | best_precision@10:0.4037 | best_precision@20:0.2173 | 
epoch: 30, training loss: 18.48117550770803, train time: 30.616904497146606
epoch: 31, training loss: 18.650445344921536, train time: 31.156129121780396
epoch: 32, training loss: 17.678039317630464, train time: 31.127342224121094
epoch: 33, training loss: 17.627676780980437, train time: 30.693028211593628
epoch: 34, training loss: 16.718912645289493, train time: 30.79653000831604
epoch: 35, training loss: 16.64481802788805, train time: 30.55940008163452
epoch: 36, training loss: 15.866445274262787, train time: 30.916057109832764
epoch: 37, training loss: 14.928912268233944, train time: 31.222398042678833
epoch: 38, training loss: 16.345252854673163, train time: 30.94531774520874
epoch: 39, training loss: 14.88301580395364, train time: 30.842236518859863
epoch: 40, training loss: 15.468756437353704, train time: 31.08139204978943
epoch: 41, training loss: 14.908573101870843, train time: 30.669819593429565
epoch: 42, training loss: 15.046327566623859, train time: 30.571315050125122
epoch: 43, training loss: 13.2092514436697, train time: 30.797137022018433
epoch: 44, training loss: 14.584693406589167, train time: 30.914668798446655
epoch: 45, training loss: 13.891348501854281, train time: 31.80640745162964
epoch: 46, training loss: 13.958845655074128, train time: 30.989041566848755
epoch: 47, training loss: 13.583370619860261, train time: 31.145621299743652
epoch: 48, training loss: 13.566301430040085, train time: 30.82990312576294
epoch: 49, training loss: 13.550567047340564, train time: 30.55834698677063
epoch: 50, training loss: 12.990453316243702, train time: 30.648379802703857
epoch: 51, training loss: 13.630414610382331, train time: 31.037394762039185
epoch: 52, training loss: 13.161851877284334, train time: 30.909262657165527
epoch: 53, training loss: 12.636092280338744, train time: 30.874653100967407
epoch: 54, training loss: 11.813884666290505, train time: 31.138041973114014
epoch: 55, training loss: 13.391540510746609, train time: 30.871840000152588
epoch: 56, training loss: 11.265462998516341, train time: 30.8009033203125
epoch: 57, training loss: 12.410990476017332, train time: 31.185688972473145
epoch: 58, training loss: 12.134971136770446, train time: 30.78666877746582
epoch: 59, training loss: 12.608071044738608, train time: 30.86512851715088
epo:59 | HR@5:0.7751 | HR@10:0.8160 | HR@20:0.8631 | NDCG@5:0.4705 | NDCG@10:0.5072 | NDCG@20:0.5546 | recall@5:0.5491 | recall@10:0.6539 | recall@20:0.7045 | precision@5:0.6589 | precision@10:0.3923 | precision@20:0.2113 | best_HR@5:0.7989 | best_HR@10:0.8409 | best_HR@20:0.8877 | best_NDCG@5:0.4705 | best_NDCG@10:0.5072 | best_NDCG@20:0.5546 | best_recall@5:0.5598 | best_recall@10:0.6728 | best_recall@20:0.7244 | best_precision@5:0.6717 | best_precision@10:0.4037 | best_precision@20:0.2173 | 
epoch: 60, training loss: 12.561149262198569, train time: 30.982434511184692
epoch: 61, training loss: 12.422001889790408, train time: 31.225901126861572
epoch: 62, training loss: 11.949085394815711, train time: 30.56218123435974
epoch: 63, training loss: 12.804679902201087, train time: 31.359124898910522
epoch: 64, training loss: 11.869934752701283, train time: 30.96878409385681
epoch: 65, training loss: 11.792123498939418, train time: 30.750611305236816
epoch: 66, training loss: 11.602530571794773, train time: 31.413556575775146
epoch: 67, training loss: 11.865190587512416, train time: 31.00642991065979
epoch: 68, training loss: 11.665113056360724, train time: 31.462257385253906
epoch: 69, training loss: 11.222436501596349, train time: 31.156655073165894
epoch: 70, training loss: 11.375006184891049, train time: 30.762139081954956
epoch: 71, training loss: 11.030842067685853, train time: 31.1421480178833
epoch: 72, training loss: 11.224879518457556, train time: 30.827835083007812
epoch: 73, training loss: 10.822452322254321, train time: 31.008469820022583
epoch: 74, training loss: 10.88152553390637, train time: 31.473485708236694
epoch: 75, training loss: 11.398988375353497, train time: 30.936108350753784
epoch: 76, training loss: 9.948750254066681, train time: 31.13948345184326
epoch: 77, training loss: 9.537170578163284, train time: 30.88500428199768
epoch: 78, training loss: 10.73059220151066, train time: 31.11081624031067
epoch: 79, training loss: 10.164153155938664, train time: 30.751745462417603
epoch: 80, training loss: 10.108889592621267, train time: 31.047796964645386
epoch: 81, training loss: 10.906398989449599, train time: 31.113508701324463
epoch: 82, training loss: 10.070871132972059, train time: 30.871373176574707
epoch: 83, training loss: 10.73558776994571, train time: 31.243565559387207
epoch: 84, training loss: 11.066234010480457, train time: 30.917515993118286
epoch: 85, training loss: 10.089250831935146, train time: 30.92970633506775
epoch: 86, training loss: 9.707308513817225, train time: 30.650192260742188
epoch: 87, training loss: 9.37842357030138, train time: 30.586164474487305
epoch: 88, training loss: 9.823959972428725, train time: 30.992597103118896
epoch: 89, training loss: 10.8157865218339, train time: 31.024425745010376
epo:89 | HR@5:0.7515 | HR@10:0.7905 | HR@20:0.8415 | NDCG@5:0.4778 | NDCG@10:0.5138 | NDCG@20:0.5606 | recall@5:0.5400 | recall@10:0.6359 | recall@20:0.6828 | precision@5:0.6480 | precision@10:0.3815 | precision@20:0.2048 | best_HR@5:0.7989 | best_HR@10:0.8409 | best_HR@20:0.8877 | best_NDCG@5:0.4778 | best_NDCG@10:0.5138 | best_NDCG@20:0.5606 | best_recall@5:0.5598 | best_recall@10:0.6728 | best_recall@20:0.7244 | best_precision@5:0.6717 | best_precision@10:0.4037 | best_precision@20:0.2173 | 
epoch: 90, training loss: 8.618863733464991, train time: 30.64205241203308
epoch: 91, training loss: 9.549157330148262, train time: 30.849127054214478
epoch: 92, training loss: 11.199714471111065, train time: 30.936738967895508
epoch: 93, training loss: 9.861020912608069, train time: 31.091206073760986
epoch: 94, training loss: 9.80758446293737, train time: 31.05273151397705
epoch: 95, training loss: 10.027814143088221, train time: 31.025480031967163
epoch: 96, training loss: 9.565201221753227, train time: 30.606751918792725
epoch: 97, training loss: 9.28709049644209, train time: 30.705998420715332
epoch: 98, training loss: 10.077590494310755, train time: 30.900941848754883
epoch: 99, training loss: 9.180453831873592, train time: 30.89272713661194
epoch: 100, training loss: 9.462485196576324, train time: 30.78406023979187
epoch: 101, training loss: 9.693110186943045, train time: 30.621001720428467
epoch: 102, training loss: 9.754601718647507, train time: 31.157837629318237
epoch: 103, training loss: 9.756432545044959, train time: 30.547873735427856
epoch: 104, training loss: 9.988002226103163, train time: 31.011938095092773
epoch: 105, training loss: 8.434412569431288, train time: 30.92820429801941
epoch: 106, training loss: 8.487640411808513, train time: 30.86618971824646
epoch: 107, training loss: 10.479912342405214, train time: 30.616467714309692
epoch: 108, training loss: 8.626398834938868, train time: 30.9319806098938
epoch: 109, training loss: 9.086136248203104, train time: 30.86101794242859
epoch: 110, training loss: 9.141416309425608, train time: 30.71644401550293
epoch: 111, training loss: 8.68848332313189, train time: 30.719409465789795
epoch: 112, training loss: 10.438112022620885, train time: 30.6141037940979
epoch: 113, training loss: 8.504033027199739, train time: 30.76800298690796
epoch: 114, training loss: 8.979354540819259, train time: 31.0064377784729
epoch: 115, training loss: 8.395755358106669, train time: 30.55595326423645
epoch: 116, training loss: 9.416794500650099, train time: 30.751769065856934
epoch: 117, training loss: 8.790649781453112, train time: 30.939701557159424
epoch: 118, training loss: 9.71167946622478, train time: 30.983017683029175
epoch: 119, training loss: 8.552058022210872, train time: 30.67533016204834
epo:119 | HR@5:0.7498 | HR@10:0.7918 | HR@20:0.8398 | NDCG@5:0.4729 | NDCG@10:0.5102 | NDCG@20:0.5576 | recall@5:0.5379 | recall@10:0.6330 | recall@20:0.6843 | precision@5:0.6455 | precision@10:0.3798 | precision@20:0.2053 | best_HR@5:0.7989 | best_HR@10:0.8409 | best_HR@20:0.8877 | best_NDCG@5:0.4778 | best_NDCG@10:0.5138 | best_NDCG@20:0.5606 | best_recall@5:0.5598 | best_recall@10:0.6728 | best_recall@20:0.7244 | best_precision@5:0.6717 | best_precision@10:0.4037 | best_precision@20:0.2173 | 
epoch: 120, training loss: 8.217248183732522, train time: 30.963245153427124
epoch: 121, training loss: 9.381718606244021, train time: 30.869410276412964
epoch: 122, training loss: 8.899039413742855, train time: 30.72094488143921
epoch: 123, training loss: 8.776586134023034, train time: 31.189127922058105
epoch: 124, training loss: 8.302287116293314, train time: 30.906769514083862
epoch: 125, training loss: 8.614076622700338, train time: 31.14427947998047
epoch: 126, training loss: 8.540720259405532, train time: 30.96873927116394
epoch: 127, training loss: 9.414682466651072, train time: 31.40447425842285
epoch: 128, training loss: 8.793006020668457, train time: 30.718196630477905
epoch: 129, training loss: 8.095600336558277, train time: 31.253718852996826
epoch: 130, training loss: 7.942292548448648, train time: 31.439841747283936
epoch: 131, training loss: 8.612726389643171, train time: 32.87909817695618
epoch: 132, training loss: 7.313840892121789, train time: 32.4460015296936
epoch: 133, training loss: 8.31175340220625, train time: 33.379112243652344
epoch: 134, training loss: 7.318239268231309, train time: 32.57953333854675
epoch: 135, training loss: 8.817961780739807, train time: 33.369521141052246
epoch: 136, training loss: 8.772427223851196, train time: 31.444749355316162
epoch: 137, training loss: 9.186168303818945, train time: 30.893792390823364
epoch: 138, training loss: 8.290761943659788, train time: 30.414163827896118
epoch: 139, training loss: 8.161281311922835, train time: 31.051090717315674
epoch: 140, training loss: 8.183180273299172, train time: 31.142754554748535
epoch: 141, training loss: 8.193724776763617, train time: 30.72644329071045
epoch: 142, training loss: 8.596563261188408, train time: 30.692659378051758
epoch: 143, training loss: 7.688105086928033, train time: 31.05640459060669
epoch: 144, training loss: 7.942107603552699, train time: 30.944358110427856
epoch: 145, training loss: 7.621465297431257, train time: 31.10844326019287
epoch: 146, training loss: 7.584819325825066, train time: 30.791486024856567
epoch: 147, training loss: 8.497284935901632, train time: 31.39784526824951
epoch: 148, training loss: 8.024038960271128, train time: 30.782326698303223
epoch: 149, training loss: 7.95332413826921, train time: 30.704960823059082
epo:149 | HR@5:0.7390 | HR@10:0.7784 | HR@20:0.8300 | NDCG@5:0.4862 | NDCG@10:0.5214 | NDCG@20:0.5673 | recall@5:0.5334 | recall@10:0.6220 | recall@20:0.6736 | precision@5:0.6401 | precision@10:0.3732 | precision@20:0.2021 | best_HR@5:0.7989 | best_HR@10:0.8409 | best_HR@20:0.8877 | best_NDCG@5:0.4862 | best_NDCG@10:0.5214 | best_NDCG@20:0.5673 | best_recall@5:0.5598 | best_recall@10:0.6728 | best_recall@20:0.7244 | best_precision@5:0.6717 | best_precision@10:0.4037 | best_precision@20:0.2173 | 
training finish
