nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.5762786865234375e-06
user  100 time:  228.41771006584167
user  200 time:  457.6427414417267
user  300 time:  686.6588232517242
user  400 time:  916.9515223503113
user  500 time:  1145.3784947395325
user  600 time:  1389.1664283275604
user  700 time:  1665.8882684707642
user  800 time:  1936.7075333595276
user  900 time:  2207.946517467499
user  1000 time:  2485.120221853256
user  1100 time:  2762.6578979492188
user  1200 time:  3038.8689398765564
user  1300 time:  3322.1596603393555
user  1400 time:  3600.2351770401
user  1500 time:  3883.06383395195
user  1600 time:  4179.345679998398
user  1700 time:  4529.721367359161
user  1800 time:  4882.989845991135
user  1900 time:  5227.044966936111
user  2000 time:  5554.505082368851
user  2100 time:  5885.090193748474
user  2200 time:  6207.82749247551
user  2300 time:  6545.136659383774
user  2400 time:  6866.5224277973175
user  2500 time:  7186.07709646225
user  2600 time:  7512.415965795517
user  2700 time:  7837.068528413773
user  2800 time:  8182.777859210968
user  2900 time:  8515.729037761688
user  3000 time:  8833.32207942009
user  3100 time:  9100.198649644852
user  3200 time:  9366.211581468582
user  3300 time:  9645.912609100342
user  3400 time:  9940.59555888176
user  3500 time:  10205.42351269722
user  3600 time:  10470.742253303528
user  3700 time:  10740.25562119484
user  3800 time:  11015.918225288391
user  3900 time:  11296.418304920197
user  4000 time:  11564.307309627533
user  4100 time:  11839.915884971619
user  4200 time:  12094.368977069855
user  4300 time:  12351.004366874695
user  4400 time:  12606.959864616394
user  4500 time:  12861.164295911789
start training item-item instance self attention module...
user  0 time:  5.9604644775390625e-06
user  100 time:  195.58152389526367
user  200 time:  392.42842841148376
user  300 time:  584.3616094589233
user  400 time:  776.8161242008209
user  500 time:  960.7417621612549
user  600 time:  1148.6162474155426
user  700 time:  1340.5939509868622
user  800 time:  1528.9102210998535
user  900 time:  1719.2047009468079
user  1000 time:  1919.24267911911
user  1100 time:  2112.5841825008392
user  1200 time:  2296.1493768692017
user  1300 time:  2485.4973559379578
user  1400 time:  2665.2287549972534
user  1500 time:  2860.034991979599
user  1600 time:  3049.3180119991302
user  1700 time:  3232.448124408722
user  1800 time:  3430.028817176819
user  1900 time:  3637.0449788570404
user  2000 time:  3831.781367778778
user  2100 time:  4000.162533044815
user  2200 time:  4157.520891427994
user  2300 time:  4318.015086650848
user  2400 time:  4484.499568462372
user  2500 time:  4650.91766834259
user  2600 time:  4825.460303068161
user  2700 time:  5004.523307323456
user  2800 time:  5185.455627441406
user  2900 time:  5353.49901342392
user  3000 time:  5523.012645959854
user  3100 time:  5699.376839637756
user  3200 time:  5859.082057237625
user  3300 time:  6027.52894616127
user  3400 time:  6200.473087072372
user  3500 time:  6369.4582142829895
user  3600 time:  6552.330082416534
user  3700 time:  6722.775950908661
user  3800 time:  6899.567729949951
user  3900 time:  7054.210574865341
user  4000 time:  7237.181678056717
user  4100 time:  7419.477895259857
user  4200 time:  7589.981998682022
user  4300 time:  7759.835085630417
user  4400 time:  7934.249380350113
user  4500 time:  8097.4238748550415
start updating user and item embedding...
user_name:4600
user  0 time:  1.1444091796875e-05
user  100 time:  64.67384386062622
user  200 time:  129.00405859947205
user  300 time:  193.433198928833
user  400 time:  258.5105061531067
user  500 time:  322.90704917907715
user  600 time:  386.8852882385254
user  700 time:  451.1396062374115
user  800 time:  515.4281516075134
user  900 time:  579.5034372806549
user  1000 time:  642.5844647884369
user  1100 time:  707.0286180973053
user  1200 time:  771.9425194263458
user  1300 time:  837.2654187679291
user  1400 time:  901.8514406681061
user  1500 time:  965.3850376605988
user  1600 time:  1029.2489109039307
user  1700 time:  1093.562305688858
user  1800 time:  1158.3765568733215
user  1900 time:  1222.8369324207306
user  2000 time:  1286.3959724903107
user  2100 time:  1350.0721971988678
user  2200 time:  1414.0435090065002
user  2300 time:  1477.5848956108093
user  2400 time:  1542.0053162574768
user  2500 time:  1605.6938807964325
user  2600 time:  1670.085606098175
user  2700 time:  1733.0355672836304
user  2800 time:  1798.0765421390533
user  2900 time:  1861.7591757774353
user  3000 time:  1926.8921828269958
user  3100 time:  1990.90904378891
user  3200 time:  2055.6340448856354
user  3300 time:  2120.13938498497
user  3400 time:  2184.616203069687
user  3500 time:  2248.7396097183228
user  3600 time:  2312.7814526557922
user  3700 time:  2377.200142621994
user  3800 time:  2441.500153541565
user  3900 time:  2506.0382380485535
user  4000 time:  2570.1646456718445
user  4100 time:  2634.138077735901
user  4200 time:  2698.9553787708282
user  4300 time:  2762.695692062378
user  4400 time:  2825.279683113098
user  4500 time:  2886.7950694561005
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 293.13458907819586, train time: 334.19045901298523
epoch: 1, training loss: 180.6249126418188, train time: 331.8839545249939
epoch: 2, training loss: 168.72752360565937, train time: 332.8698847293854
epoch: 3, training loss: 160.72296957997605, train time: 333.8866546154022
epoch: 4, training loss: 155.40879638928163, train time: 333.67711067199707
epoch: 5, training loss: 150.05469476005237, train time: 332.6188840866089
epoch: 6, training loss: 145.9277047939977, train time: 333.22252345085144
epoch: 7, training loss: 141.6646019666514, train time: 334.4348027706146
epoch: 8, training loss: 138.19388190694735, train time: 334.53222393989563
epoch: 9, training loss: 134.79603079235676, train time: 333.1615171432495
epoch: 10, training loss: 130.86491369240684, train time: 333.7686586380005
epoch: 11, training loss: 126.90016516985634, train time: 333.79966831207275
epoch: 12, training loss: 125.25137927870674, train time: 333.2098717689514
epoch: 13, training loss: 123.05890960153192, train time: 333.4762463569641
epoch: 14, training loss: 119.30348547830363, train time: 332.5028359889984
epoch: 15, training loss: 117.68258499670628, train time: 335.0083565711975
epoch: 16, training loss: 114.97901352068584, train time: 334.7423052787781
epoch: 17, training loss: 114.57925164281914, train time: 333.9091420173645
epoch: 18, training loss: 114.17365463214082, train time: 333.0857627391815
epoch: 19, training loss: 111.01983064475644, train time: 334.2756688594818
epoch: 20, training loss: 110.75848280391801, train time: 332.31459307670593
epoch: 21, training loss: 109.05395471182419, train time: 334.04999470710754
epoch: 22, training loss: 107.03476872279134, train time: 331.67702555656433
epoch: 23, training loss: 107.08130481538683, train time: 333.2607669830322
epoch: 24, training loss: 105.3666612543966, train time: 333.4990997314453
epoch: 25, training loss: 102.75707195389987, train time: 334.5916368961334
epoch: 26, training loss: 102.86208192140475, train time: 333.64522767066956
epoch: 27, training loss: 101.18680443213816, train time: 333.6341223716736
epoch: 28, training loss: 101.3968557166445, train time: 333.7280077934265
epoch: 29, training loss: 100.58794916726765, train time: 331.1011505126953
epo:29 | HR@5:0.9009 | HR@10:0.9174 | HR@20:0.9364 | NDCG@5:0.4459 | NDCG@10:0.4890 | NDCG@20:0.5414 | recall@5:0.6414 | recall@10:0.7514 | recall@20:0.7732 | precision@5:0.7697 | precision@10:0.4509 | precision@20:0.2320 | best_HR@5:0.9009 | best_HR@10:0.9174 | best_HR@20:0.9364 | best_NDCG@5:0.4459 | best_NDCG@10:0.4890 | best_NDCG@20:0.5414 | best_recall@5:0.6414 | best_recall@10:0.7514 | best_recall@20:0.7732 | best_precision@5:0.7697 | best_precision@10:0.4509 | best_precision@20:0.2320 | 
epoch: 30, training loss: 100.03531655913685, train time: 333.9128715991974
epoch: 31, training loss: 98.84730284771649, train time: 333.03415393829346
epoch: 32, training loss: 100.01505796008132, train time: 334.9425423145294
epoch: 33, training loss: 97.96962211605569, train time: 331.95600509643555
epoch: 34, training loss: 97.9179842216945, train time: 334.52960228919983
epoch: 35, training loss: 97.039392314211, train time: 333.73982071876526
epoch: 36, training loss: 98.66823569654662, train time: 334.3076937198639
epoch: 37, training loss: 96.35592875638758, train time: 334.3792324066162
epoch: 38, training loss: 94.95765571643278, train time: 332.9583270549774
epoch: 39, training loss: 95.5374154988931, train time: 333.37955141067505
epoch: 40, training loss: 95.55274731594545, train time: 333.3109624385834
epoch: 41, training loss: 94.43433226907655, train time: 334.47666478157043
epoch: 42, training loss: 94.7297272420983, train time: 333.2175974845886
epoch: 43, training loss: 94.08512219118347, train time: 333.92996430397034
epoch: 44, training loss: 93.38472650463518, train time: 334.306832075119
epoch: 45, training loss: 93.75162503851243, train time: 334.6566309928894
epoch: 46, training loss: 92.33871309562528, train time: 333.66037106513977
epoch: 47, training loss: 92.44218446602827, train time: 334.5547490119934
epoch: 48, training loss: 90.76511504430528, train time: 333.93275237083435
epoch: 49, training loss: 91.087102040241, train time: 334.92903780937195
epoch: 50, training loss: 91.31723504215552, train time: 334.2558100223541
epoch: 51, training loss: 89.81254122145037, train time: 334.57636880874634
epoch: 52, training loss: 90.605497916491, train time: 334.1992332935333
epoch: 53, training loss: 92.15528929602442, train time: 332.1095540523529
epoch: 54, training loss: 90.90715289221407, train time: 333.53806924819946
epoch: 55, training loss: 91.67303120098586, train time: 332.66340589523315
epoch: 56, training loss: 91.06105570451473, train time: 332.6522583961487
epoch: 57, training loss: 92.12876747813789, train time: 332.9209027290344
epoch: 58, training loss: 91.23466700750214, train time: 335.7339096069336
epoch: 59, training loss: 90.01686398574384, train time: 331.5120575428009
epo:59 | HR@5:0.8808 | HR@10:0.8986 | HR@20:0.9203 | NDCG@5:0.4530 | NDCG@10:0.4937 | NDCG@20:0.5442 | recall@5:0.6342 | recall@10:0.7356 | recall@20:0.7596 | precision@5:0.7611 | precision@10:0.4414 | precision@20:0.2279 | best_HR@5:0.9009 | best_HR@10:0.9174 | best_HR@20:0.9364 | best_NDCG@5:0.4530 | best_NDCG@10:0.4937 | best_NDCG@20:0.5442 | best_recall@5:0.6414 | best_recall@10:0.7514 | best_recall@20:0.7732 | best_precision@5:0.7697 | best_precision@10:0.4509 | best_precision@20:0.2320 | 
epoch: 60, training loss: 89.94864948895702, train time: 339.5358898639679
epoch: 61, training loss: 90.69346673955442, train time: 334.6258120536804
epoch: 62, training loss: 91.55701813757332, train time: 334.1636929512024
epoch: 63, training loss: 89.66904191077629, train time: 339.62751960754395
epoch: 64, training loss: 89.72860718260563, train time: 337.13882875442505
epoch: 65, training loss: 89.65677882199088, train time: 333.7478654384613
epoch: 66, training loss: 89.9666214408644, train time: 333.36673498153687
epoch: 67, training loss: 88.8237020192937, train time: 338.34095311164856
epoch: 68, training loss: 88.44274794520607, train time: 340.1577477455139
epoch: 69, training loss: 88.41779319397392, train time: 340.2546045780182
epoch: 70, training loss: 90.30389750351242, train time: 344.4031937122345
epoch: 71, training loss: 91.44059650476265, train time: 363.72148394584656
epoch: 72, training loss: 89.4003913726483, train time: 419.80563259124756
epoch: 73, training loss: 88.77399881204474, train time: 433.57282733917236
epoch: 74, training loss: 87.91239791636326, train time: 433.8116981983185
epoch: 75, training loss: 90.37562406365032, train time: 432.7383642196655
epoch: 76, training loss: 88.33724689709925, train time: 433.65314292907715
epoch: 77, training loss: 88.40416875977098, train time: 433.257705450058
epoch: 78, training loss: 87.72566653035756, train time: 434.1742670536041
epoch: 79, training loss: 88.61048550408304, train time: 434.5835416316986
epoch: 80, training loss: 86.2655122536089, train time: 426.6743161678314
epoch: 81, training loss: 87.26486343259967, train time: 426.56692600250244
epoch: 82, training loss: 88.00111168822696, train time: 426.7185664176941
epoch: 83, training loss: 87.75451123553648, train time: 427.69607305526733
epoch: 84, training loss: 87.7787115480096, train time: 426.5100255012512
epoch: 85, training loss: 88.35811694373842, train time: 426.1938445568085
epoch: 86, training loss: 88.9652177811804, train time: 426.2309875488281
epoch: 87, training loss: 88.13805139181932, train time: 425.8747777938843
epoch: 88, training loss: 89.59697612487071, train time: 426.6254367828369
epoch: 89, training loss: 90.31663256085812, train time: 424.3944697380066
epo:89 | HR@5:0.8686 | HR@10:0.8857 | HR@20:0.9088 | NDCG@5:0.4510 | NDCG@10:0.4932 | NDCG@20:0.5446 | recall@5:0.6294 | recall@10:0.7254 | recall@20:0.7497 | precision@5:0.7553 | precision@10:0.4353 | precision@20:0.2249 | best_HR@5:0.9009 | best_HR@10:0.9174 | best_HR@20:0.9364 | best_NDCG@5:0.4530 | best_NDCG@10:0.4937 | best_NDCG@20:0.5446 | best_recall@5:0.6414 | best_recall@10:0.7514 | best_recall@20:0.7732 | best_precision@5:0.7697 | best_precision@10:0.4509 | best_precision@20:0.2320 | 
epoch: 90, training loss: 89.50140937017568, train time: 375.21082520484924
epoch: 91, training loss: 90.77441825755523, train time: 378.2773506641388
epoch: 92, training loss: 91.70999833060341, train time: 376.8899915218353
epoch: 93, training loss: 92.0036172844193, train time: 376.8893632888794
epoch: 94, training loss: 89.91972892605554, train time: 377.6556339263916
epoch: 95, training loss: 89.86832366191811, train time: 376.7044005393982
epoch: 96, training loss: 90.77485079552571, train time: 376.9052448272705
epoch: 97, training loss: 89.22741496733579, train time: 377.5284616947174
epoch: 98, training loss: 90.73749380840673, train time: 346.0308208465576
epoch: 99, training loss: 91.21389573406486, train time: 342.67031478881836
epoch: 100, training loss: 90.32064103878656, train time: 342.7064108848572
epoch: 101, training loss: 90.58751169831521, train time: 343.31166982650757
epoch: 102, training loss: 92.36450495446843, train time: 345.0656704902649
epoch: 103, training loss: 89.11966455467336, train time: 345.7810559272766
epoch: 104, training loss: 91.16739177065028, train time: 345.5630624294281
epoch: 105, training loss: 91.06750756988913, train time: 344.305095911026
epoch: 106, training loss: 92.67029035583982, train time: 343.6817624568939
epoch: 107, training loss: 92.28295116525987, train time: 345.5508177280426
epoch: 108, training loss: 92.75670929621992, train time: 342.8108878135681
epoch: 109, training loss: 91.80238066795573, train time: 342.1544859409332
epoch: 110, training loss: 91.8498213783605, train time: 344.97528171539307
epoch: 111, training loss: 90.72635756406089, train time: 343.0810315608978
epoch: 112, training loss: 91.20601922070637, train time: 343.6805577278137
epoch: 113, training loss: 91.87266948541946, train time: 343.80390548706055
epoch: 114, training loss: 91.66449540809845, train time: 369.29458236694336
epoch: 115, training loss: 92.62918405779783, train time: 374.2858028411865
epoch: 116, training loss: 92.69460535647522, train time: 372.7797040939331
epoch: 117, training loss: 90.73427479599923, train time: 374.1245641708374
epoch: 118, training loss: 95.24455958235194, train time: 372.8973412513733
epoch: 119, training loss: 92.79215544516046, train time: 373.00252509117126
epo:119 | HR@5:0.8587 | HR@10:0.8755 | HR@20:0.9004 | NDCG@5:0.4427 | NDCG@10:0.4851 | NDCG@20:0.5373 | recall@5:0.6254 | recall@10:0.7170 | recall@20:0.7417 | precision@5:0.7504 | precision@10:0.4302 | precision@20:0.2225 | best_HR@5:0.9009 | best_HR@10:0.9174 | best_HR@20:0.9364 | best_NDCG@5:0.4530 | best_NDCG@10:0.4937 | best_NDCG@20:0.5446 | best_recall@5:0.6414 | best_recall@10:0.7514 | best_recall@20:0.7732 | best_precision@5:0.7697 | best_precision@10:0.4509 | best_precision@20:0.2320 | 
epoch: 120, training loss: 91.64282724751683, train time: 369.4479112625122
epoch: 121, training loss: 91.97444157009159, train time: 373.7071764469147
epoch: 122, training loss: 92.83807201204036, train time: 373.79404282569885
epoch: 123, training loss: 92.72376074967906, train time: 375.38867020606995
epoch: 124, training loss: 91.86160286666563, train time: 375.35089778900146
epoch: 125, training loss: 92.8799837398401, train time: 374.83022809028625
epoch: 126, training loss: 92.7465939620015, train time: 377.13668179512024
epoch: 127, training loss: 91.04800953784434, train time: 407.43174743652344
epoch: 128, training loss: 91.58157687130733, train time: 436.21806740760803
epoch: 129, training loss: 92.67978929216042, train time: 447.9452803134918
epoch: 130, training loss: 91.99010726137931, train time: 458.8969795703888
epoch: 131, training loss: 91.12704003903491, train time: 463.94919896125793
epoch: 132, training loss: 91.32797814084915, train time: 466.52738976478577
epoch: 133, training loss: 91.16560699330876, train time: 463.42348170280457
epoch: 134, training loss: 91.13464562519948, train time: 464.0255448818207
epoch: 135, training loss: 90.24297002749518, train time: 464.50436329841614
epoch: 136, training loss: 91.63494173068466, train time: 463.994163274765
epoch: 137, training loss: 90.6352837982995, train time: 461.782963514328
epoch: 138, training loss: 92.15051375454641, train time: 463.4409108161926
epoch: 139, training loss: 91.68046224920545, train time: 464.24324345588684
epoch: 140, training loss: 90.91832341019472, train time: 468.82654428482056
epoch: 141, training loss: 91.52648424599465, train time: 470.48971128463745
epoch: 142, training loss: 92.49086090813944, train time: 468.886741399765
epoch: 143, training loss: 92.11461307587888, train time: 469.1199300289154
epoch: 144, training loss: 91.41594318709394, train time: 467.95886993408203
epoch: 145, training loss: 94.01612091330753, train time: 468.40478706359863
epoch: 146, training loss: 92.70412361396302, train time: 469.17469239234924
epoch: 147, training loss: 92.08681124977011, train time: 469.9037148952484
epoch: 148, training loss: 92.4065222099307, train time: 467.25348806381226
epoch: 149, training loss: 91.74778183136732, train time: 468.2890045642853
epo:149 | HR@5:0.8563 | HR@10:0.8738 | HR@20:0.8981 | NDCG@5:0.4408 | NDCG@10:0.4837 | NDCG@20:0.5359 | recall@5:0.6229 | recall@10:0.7153 | recall@20:0.7389 | precision@5:0.7475 | precision@10:0.4292 | precision@20:0.2217 | best_HR@5:0.9009 | best_HR@10:0.9174 | best_HR@20:0.9364 | best_NDCG@5:0.4530 | best_NDCG@10:0.4937 | best_NDCG@20:0.5446 | best_recall@5:0.6414 | best_recall@10:0.7514 | best_recall@20:0.7732 | best_precision@5:0.7697 | best_precision@10:0.4509 | best_precision@20:0.2320 | 
training finish
