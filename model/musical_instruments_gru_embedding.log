nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.814697265625e-06
user  100 time:  26.857484579086304
user  200 time:  42.497307777404785
user  300 time:  55.56577014923096
user  400 time:  66.71472263336182
user  500 time:  96.9021565914154
user  600 time:  114.24794268608093
user  700 time:  143.71801042556763
user  800 time:  171.57079243659973
user  900 time:  184.34044432640076
user  1000 time:  205.50131344795227
user  1100 time:  220.96793580055237
user  1200 time:  240.08364486694336
user  1300 time:  259.6205279827118
user  1400 time:  268.3726553916931
start training item-item instance self attention module...
user  0 time:  1.2636184692382812e-05
user  100 time:  175.23674035072327
user  200 time:  357.95267510414124
user  300 time:  544.993093252182
user  400 time:  728.5598113536835
user  500 time:  913.617105960846
user  600 time:  1094.5090234279633
user  700 time:  1279.5906884670258
user  800 time:  1466.107192516327
user  900 time:  1646.5825848579407
user  1000 time:  1830.5938668251038
user  1100 time:  2006.830013513565
user  1200 time:  2191.4788875579834
user  1300 time:  2367.9775223731995
user  1400 time:  2548.1793506145477
start updating user and item embedding...
user_name:1450
user  0 time:  1.0967254638671875e-05
user  100 time:  15.3453528881073
user  200 time:  31.098714590072632
user  300 time:  46.74088096618652
user  400 time:  62.46357798576355
user  500 time:  78.0497350692749
user  600 time:  93.47073125839233
user  700 time:  109.00013756752014
user  800 time:  124.6767988204956
user  900 time:  140.16307950019836
user  1000 time:  155.66775226593018
user  1100 time:  171.09322333335876
user  1200 time:  186.76049208641052
user  1300 time:  202.24171137809753
user  1400 time:  217.536194562912
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 290.1563722026767, train time: 22.905568599700928
epoch: 1, training loss: 230.02607927797362, train time: 22.530488967895508
epoch: 2, training loss: 218.37609329726547, train time: 22.709100008010864
epoch: 3, training loss: 212.80900288693374, train time: 23.028390884399414
epoch: 4, training loss: 207.2475809836178, train time: 22.651243925094604
epoch: 5, training loss: 203.79553976556053, train time: 23.094191312789917
epoch: 6, training loss: 200.13106636010343, train time: 22.531843423843384
epoch: 7, training loss: 197.94350493198726, train time: 22.82893395423889
epoch: 8, training loss: 194.8166762003093, train time: 22.471526384353638
epoch: 9, training loss: 192.9441374373273, train time: 22.985880136489868
epoch: 10, training loss: 191.08355812670197, train time: 23.157705545425415
epoch: 11, training loss: 189.68631061475025, train time: 23.061803579330444
epoch: 12, training loss: 189.01547038275748, train time: 22.46917462348938
epoch: 13, training loss: 187.7477780848276, train time: 22.873958110809326
epoch: 14, training loss: 186.13723112567095, train time: 22.815129041671753
epoch: 15, training loss: 184.8370340007532, train time: 22.549492597579956
epoch: 16, training loss: 184.10379031143384, train time: 22.68367028236389
epoch: 17, training loss: 182.36863259066013, train time: 22.398931741714478
epoch: 18, training loss: 182.6085427144426, train time: 23.072712898254395
epoch: 19, training loss: 180.97656850761268, train time: 22.765226364135742
epoch: 20, training loss: 180.74849457171513, train time: 22.677772045135498
epoch: 21, training loss: 181.18198494659737, train time: 23.096533060073853
epoch: 22, training loss: 180.48473044802085, train time: 22.509623050689697
epoch: 23, training loss: 179.65750607807422, train time: 22.73952603340149
epoch: 24, training loss: 180.3729571782751, train time: 22.561179637908936
epoch: 25, training loss: 178.99922407476697, train time: 22.69285774230957
epoch: 26, training loss: 177.70708588932757, train time: 23.349722623825073
epoch: 27, training loss: 176.92279273079475, train time: 23.01433777809143
epoch: 28, training loss: 176.45922567270463, train time: 22.82004404067993
epoch: 29, training loss: 175.53782976948423, train time: 23.453117847442627
epoch: 30, training loss: 176.1816497957334, train time: 22.776797771453857
epoch: 31, training loss: 174.2941453034291, train time: 23.169867038726807
epoch: 32, training loss: 176.38647281946032, train time: 22.36824917793274
epoch: 33, training loss: 175.72543636069167, train time: 22.679039478302002
epoch: 34, training loss: 174.12422288116068, train time: 22.676450490951538
epoch: 35, training loss: 174.48080729757203, train time: 34.37370824813843
epoch: 36, training loss: 173.5679623002361, train time: 34.3195424079895
epoch: 37, training loss: 174.3831843388616, train time: 37.45140242576599
epoch: 38, training loss: 174.33276117849164, train time: 36.52319574356079
epoch: 39, training loss: 173.00166662834818, train time: 36.97460412979126
epoch: 40, training loss: 171.98787789503695, train time: 37.74729800224304
epoch: 41, training loss: 172.49680612963857, train time: 35.85240173339844
epoch: 42, training loss: 171.94322111926158, train time: 34.80306553840637
epoch: 43, training loss: 172.00770817292505, train time: 33.32618260383606
epoch: 44, training loss: 172.07984056300484, train time: 30.629841327667236
epoch: 45, training loss: 170.12711007357575, train time: 30.31722927093506
epoch: 46, training loss: 170.7411944826017, train time: 30.042853116989136
epoch: 47, training loss: 170.7992825714755, train time: 31.57342004776001
epoch: 48, training loss: 171.2368836045498, train time: 30.22246551513672
epoch: 49, training loss: 170.66948413537466, train time: 30.934062719345093
epo:49|HR@1:0.3093 | HR@5:0.6978 | HR@10:0.8357 | HR@20:0.9256 | HR@50:0.9815 | NDCG@1:0.2833 | NDCG@5:0.3180 | NDCG@10:0.3489| NDCG@20:0.4021| NDCG@50:0.5286| best_HR@1:0.3093 | best_HR@5:0.6978 | best_HR@10:0.8357 | best_HR@20:0.9256 | best_HR@50:0.9815 | best_NDCG@1:0.2833 | best_NDCG@5:0.3180 | best_NDCG@10:0.3489 | best_NDCG@20:0.4021 | best_NDCG@50:0.5286 | train_time:30.93 | test_time:377.88
epoch: 50, training loss: 170.2987231715815, train time: 34.29224991798401
epoch: 51, training loss: 169.79133344831644, train time: 36.80640172958374
epoch: 52, training loss: 169.22880725786672, train time: 37.842331647872925
epoch: 53, training loss: 169.29109806919587, train time: 38.057865619659424
epoch: 54, training loss: 168.58859314903384, train time: 36.672455072402954
epoch: 55, training loss: 167.822725162172, train time: 35.99413537979126
epoch: 56, training loss: 168.5592737508705, train time: 37.454665184020996
epoch: 57, training loss: 167.8478692920762, train time: 31.13282608985901
epoch: 58, training loss: 166.94434477353934, train time: 32.95769667625427
epoch: 59, training loss: 168.31787464988884, train time: 31.887612342834473
epoch: 60, training loss: 168.6679300122487, train time: 30.377803087234497
epoch: 61, training loss: 167.879446190811, train time: 34.90472865104675
epoch: 62, training loss: 167.27542728494154, train time: 36.836803913116455
epoch: 63, training loss: 167.33817598226597, train time: 37.28337335586548
epoch: 64, training loss: 168.66770663234638, train time: 38.152684926986694
epoch: 65, training loss: 168.5927502263512, train time: 37.25369882583618
epoch: 66, training loss: 166.11047277644684, train time: 30.713248252868652
epoch: 67, training loss: 166.87410988868214, train time: 33.59987258911133
epoch: 68, training loss: 166.07709170648013, train time: 33.9833402633667
epoch: 69, training loss: 167.3834752564435, train time: 33.04273343086243
epoch: 70, training loss: 167.0338574804773, train time: 33.36458611488342
epoch: 71, training loss: 166.1052539917, train time: 33.47182846069336
epoch: 72, training loss: 166.2825875042472, train time: 35.800729751586914
epoch: 73, training loss: 165.49839206897013, train time: 30.808935165405273
epoch: 74, training loss: 165.184196866001, train time: 30.92179822921753
epoch: 75, training loss: 165.85296600160655, train time: 30.74072504043579
epoch: 76, training loss: 166.5752741361066, train time: 30.47103714942932
epoch: 77, training loss: 165.57924717079732, train time: 22.607611417770386
epoch: 78, training loss: 164.15001033492445, train time: 22.755379915237427
epoch: 79, training loss: 164.62808374952874, train time: 22.48902416229248
epoch: 80, training loss: 164.5404449122725, train time: 22.718448162078857
epoch: 81, training loss: 164.32584914335166, train time: 23.24645233154297
epoch: 82, training loss: 163.47132260291255, train time: 22.56368613243103
epoch: 83, training loss: 164.4154251771397, train time: 22.331940174102783
epoch: 84, training loss: 163.49331592879025, train time: 22.564532995224
epoch: 85, training loss: 164.24782578204758, train time: 32.156195402145386
epoch: 86, training loss: 164.27021456083457, train time: 32.02111458778381
epoch: 87, training loss: 163.94843242684146, train time: 26.236364603042603
epoch: 88, training loss: 164.556475356163, train time: 28.266754150390625
epoch: 89, training loss: 164.07466043770546, train time: 32.08264708518982
epoch: 90, training loss: 162.4316495648527, train time: 32.194706201553345
epoch: 91, training loss: 162.74298292963067, train time: 32.20477366447449
epoch: 92, training loss: 162.9071913032094, train time: 32.18674302101135
epoch: 93, training loss: 162.95646687263798, train time: 32.28271818161011
epoch: 94, training loss: 162.07901649028645, train time: 32.16848659515381
epoch: 95, training loss: 161.58689881938335, train time: 32.21970987319946
epoch: 96, training loss: 162.90679162560264, train time: 32.24185252189636
epoch: 97, training loss: 162.92705756041687, train time: 32.23472857475281
epoch: 98, training loss: 161.64957417928963, train time: 32.2470965385437
epoch: 99, training loss: 162.0165431300993, train time: 32.2250394821167
epo:99|HR@1:0.3334 | HR@5:0.7054 | HR@10:0.8378 | HR@20:0.9201 | HR@50:0.9792 | NDCG@1:0.2727 | NDCG@5:0.3137 | NDCG@10:0.3487| NDCG@20:0.4064| NDCG@50:0.5337| best_HR@1:0.3334 | best_HR@5:0.7054 | best_HR@10:0.8378 | best_HR@20:0.9256 | best_HR@50:0.9815 | best_NDCG@1:0.2833 | best_NDCG@5:0.3180 | best_NDCG@10:0.3489 | best_NDCG@20:0.4064 | best_NDCG@50:0.5337 | train_time:32.23 | test_time:599.76
training finish
