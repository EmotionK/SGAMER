nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Grocery_Gourmet_Food......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  9.059906005859375e-06
user  100 time:  5.751415491104126
user  200 time:  11.149133205413818
user  300 time:  19.318026781082153
user  400 time:  22.02727437019348
user  500 time:  23.037717580795288
user  600 time:  23.898625373840332
user  700 time:  23.901138067245483
user  800 time:  29.95641040802002
user  900 time:  33.49641728401184
user  1000 time:  35.914745569229126
user  1100 time:  42.60205340385437
user  1200 time:  46.917582750320435
user  1300 time:  50.784119844436646
user  1400 time:  55.20129370689392
user  1500 time:  61.197590827941895
user  1600 time:  64.16311931610107
user  1700 time:  69.80421662330627
user  1800 time:  71.35614681243896
user  1900 time:  73.6573326587677
start training item-item instance self attention module...
user  0 time:  4.291534423828125e-06
user  100 time:  200.67721509933472
user  200 time:  368.41821360588074
user  300 time:  542.0753021240234
user  400 time:  725.4234907627106
user  500 time:  917.5975298881531
user  600 time:  1086.5557157993317
user  700 time:  1284.974580526352
user  800 time:  1471.632215976715
user  900 time:  1658.972490787506
user  1000 time:  1844.2044200897217
user  1100 time:  2029.401599407196
user  1200 time:  2207.2946729660034
user  1300 time:  2421.143785715103
user  1400 time:  2618.6923863887787
user  1500 time:  2816.2951233386993
user  1600 time:  3002.6811003684998
user  1700 time:  3206.2633295059204
user  1800 time:  3396.829564332962
user  1900 time:  3595.6201033592224
start updating user and item embedding...
user_name:2000
user  0 time:  9.298324584960938e-06
user  100 time:  19.15132451057434
user  200 time:  38.50868773460388
user  300 time:  57.89122676849365
user  400 time:  76.9601514339447
user  500 time:  96.39868998527527
user  600 time:  115.52465224266052
user  700 time:  135.0650918483734
user  800 time:  153.99902510643005
user  900 time:  173.3490982055664
user  1000 time:  192.88774728775024
user  1100 time:  212.3945610523224
user  1200 time:  231.47954845428467
user  1300 time:  250.6597182750702
user  1400 time:  269.69483971595764
user  1500 time:  288.5378351211548
user  1600 time:  307.41732478141785
user  1700 time:  326.70212960243225
user  1800 time:  345.3716838359833
user  1900 time:  364.32519340515137
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 344.66302812844515, train time: 359.36984038352966
epoch: 1, training loss: 259.4610037889797, train time: 358.30510997772217
epoch: 2, training loss: 208.14842879597563, train time: 360.01344108581543
epoch: 3, training loss: 192.10818388313055, train time: 359.92481303215027
epoch: 4, training loss: 182.30098550731782, train time: 360.6632881164551
epoch: 5, training loss: 175.12271854479332, train time: 358.0075087547302
epoch: 6, training loss: 169.5055525235366, train time: 361.74285650253296
epoch: 7, training loss: 164.63324089284288, train time: 361.2681157588959
epoch: 8, training loss: 160.3825768061797, train time: 361.3704640865326
epoch: 9, training loss: 156.85173343017232, train time: 360.24020552635193
epoch: 10, training loss: 153.4429787787376, train time: 361.1473181247711
epoch: 11, training loss: 149.7088096998632, train time: 362.8962330818176
epoch: 12, training loss: 147.0276084219804, train time: 364.81590843200684
epoch: 13, training loss: 144.08781458990416, train time: 361.228675365448
epoch: 14, training loss: 141.24996943195583, train time: 361.88740038871765
epoch: 15, training loss: 138.46466642594896, train time: 361.0079176425934
epoch: 16, training loss: 135.99908100627363, train time: 360.8044409751892
epoch: 17, training loss: 133.30333058553515, train time: 362.6490526199341
epoch: 18, training loss: 130.8944062081282, train time: 361.23845410346985
epoch: 19, training loss: 128.48701098875608, train time: 362.0998418331146
epoch: 20, training loss: 126.49872693431098, train time: 360.6805784702301
epoch: 21, training loss: 123.93797290962539, train time: 355.93216824531555
epoch: 22, training loss: 122.39036949898582, train time: 348.04687762260437
epoch: 23, training loss: 120.19469518691767, train time: 350.06015825271606
epoch: 24, training loss: 118.45389925711788, train time: 364.34437680244446
epoch: 25, training loss: 117.02530510156066, train time: 375.53154039382935
epoch: 26, training loss: 115.19133631882141, train time: 377.1412241458893
epoch: 27, training loss: 113.5060333222209, train time: 376.88559889793396
epoch: 28, training loss: 111.83648045815062, train time: 380.8993902206421
epoch: 29, training loss: 110.08223885731422, train time: 356.58942341804504
epo:29 | HR@5:0.8483 | HR@10:0.9300 | HR@20:0.9794 | NDCG@5:0.4485 | NDCG@10:0.4764 | NDCG@20:0.5186 | recall@5:0.5209 | recall@10:0.7250 | recall@20:0.8056 | precision@5:0.6251 | precision@10:0.4350 | precision@20:0.2417 | best_HR@5:0.8483 | best_HR@10:0.9300 | best_HR@20:0.9794 | best_NDCG@5:0.4485 | best_NDCG@10:0.4764 | best_NDCG@20:0.5186 | best_recall@5:0.5209 | best_recall@10:0.7250 | best_recall@20:0.8056 | best_precision@5:0.6251 | best_precision@10:0.4350 | best_precision@20:0.2417 | 
epoch: 30, training loss: 108.90186585334595, train time: 376.8343586921692
epoch: 31, training loss: 107.39091319098952, train time: 376.1163983345032
epoch: 32, training loss: 105.88079634327732, train time: 378.9989309310913
epoch: 33, training loss: 104.56565568722726, train time: 374.2069249153137
epoch: 34, training loss: 102.87285507872002, train time: 353.51839327812195
epoch: 35, training loss: 101.44691156159388, train time: 347.87494230270386
epoch: 36, training loss: 100.52251682385395, train time: 359.31064105033875
epoch: 37, training loss: 98.92487336766499, train time: 376.95799255371094
epoch: 38, training loss: 98.07984885259066, train time: 377.56711053848267
epoch: 39, training loss: 96.74612456210889, train time: 376.0934238433838
epoch: 40, training loss: 95.43325222926796, train time: 377.2493586540222
epoch: 41, training loss: 93.93791412076098, train time: 358.2443149089813
epoch: 42, training loss: 93.22838249749475, train time: 345.38070607185364
epoch: 43, training loss: 91.69394789978833, train time: 352.90496730804443
epoch: 44, training loss: 89.91105331789731, train time: 378.32030391693115
epoch: 45, training loss: 89.00441837207472, train time: 375.1814603805542
epoch: 46, training loss: 87.15632030349661, train time: 376.6243064403534
epoch: 47, training loss: 85.66195577003236, train time: 375.80356454849243
epoch: 48, training loss: 84.18758774759044, train time: 359.58870553970337
epoch: 49, training loss: 82.76782345802712, train time: 338.7147915363312
epoch: 50, training loss: 81.26901956233633, train time: 329.9443359375
epoch: 51, training loss: 79.35490753804334, train time: 362.9447190761566
epoch: 52, training loss: 77.88695976703457, train time: 375.7371175289154
epoch: 53, training loss: 76.3834625175441, train time: 375.6743211746216
epoch: 54, training loss: 74.2359433529964, train time: 376.7076036930084
epoch: 55, training loss: 72.61681501675412, train time: 375.6144688129425
epoch: 56, training loss: 71.69527399009894, train time: 354.0156419277191
epoch: 57, training loss: 69.58815408811279, train time: 348.60713934898376
epoch: 58, training loss: 67.96988701459304, train time: 347.6614282131195
epoch: 59, training loss: 66.39672946649262, train time: 335.71757459640503
epo:59 | HR@5:0.8277 | HR@10:0.9187 | HR@20:0.9748 | NDCG@5:0.4894 | NDCG@10:0.5157 | NDCG@20:0.5558 | recall@5:0.5087 | recall@10:0.7102 | recall@20:0.7998 | precision@5:0.6104 | precision@10:0.4261 | precision@20:0.2399 | best_HR@5:0.8483 | best_HR@10:0.9300 | best_HR@20:0.9794 | best_NDCG@5:0.4894 | best_NDCG@10:0.5157 | best_NDCG@20:0.5558 | best_recall@5:0.5209 | best_recall@10:0.7250 | best_recall@20:0.8056 | best_precision@5:0.6251 | best_precision@10:0.4350 | best_precision@20:0.2417 | 
epoch: 60, training loss: 64.91950980614092, train time: 364.0663423538208
epoch: 61, training loss: 63.391082976548205, train time: 364.180401802063
epoch: 62, training loss: 61.81846485644655, train time: 362.73601174354553
epoch: 63, training loss: 59.81670964633781, train time: 364.2465958595276
epoch: 64, training loss: 59.04284123805974, train time: 363.8862609863281
epoch: 65, training loss: 57.42113324093498, train time: 364.93416595458984
epoch: 66, training loss: 56.18753906949678, train time: 363.78465819358826
epoch: 67, training loss: 54.45660564470563, train time: 365.9215340614319
epoch: 68, training loss: 53.14828953240249, train time: 363.1930470466614
epoch: 69, training loss: 52.27050073700684, train time: 363.69922709465027
epoch: 70, training loss: 51.141202860900194, train time: 363.66268968582153
epoch: 71, training loss: 49.671182309225514, train time: 363.3395230770111
epoch: 72, training loss: 47.880750612670454, train time: 364.4614779949188
epoch: 73, training loss: 47.05559873878519, train time: 364.485787153244
epoch: 74, training loss: 46.21962492727357, train time: 362.3012819290161
epoch: 75, training loss: 45.11556265042077, train time: 336.7861349582672
epoch: 76, training loss: 43.129491516026064, train time: 326.79675483703613
epoch: 77, training loss: 43.87774754282725, train time: 324.5984787940979
epoch: 78, training loss: 41.404541325030095, train time: 332.3818016052246
epoch: 79, training loss: 41.02270469760424, train time: 339.01924562454224
epoch: 80, training loss: 39.97768358125245, train time: 339.29061222076416
epoch: 81, training loss: 38.29556719303034, train time: 337.6375832557678
epoch: 82, training loss: 37.42475663098787, train time: 337.7845346927643
epoch: 83, training loss: 37.214292869794235, train time: 339.2583119869232
epoch: 84, training loss: 35.893649133818144, train time: 338.6705422401428
epoch: 85, training loss: 35.09614197122545, train time: 335.84034419059753
epoch: 86, training loss: 35.27531665888378, train time: 338.05955719947815
epoch: 87, training loss: 34.22969435811419, train time: 336.32066917419434
epoch: 88, training loss: 32.90086453629321, train time: 337.00610423088074
epoch: 89, training loss: 32.536928875320044, train time: 339.88239336013794
epo:89 | HR@5:0.8023 | HR@10:0.9034 | HR@20:0.9633 | NDCG@5:0.4956 | NDCG@10:0.5223 | NDCG@20:0.5627 | recall@5:0.4957 | recall@10:0.6917 | recall@20:0.7879 | precision@5:0.5949 | precision@10:0.4150 | precision@20:0.2364 | best_HR@5:0.8483 | best_HR@10:0.9300 | best_HR@20:0.9794 | best_NDCG@5:0.4956 | best_NDCG@10:0.5223 | best_NDCG@20:0.5627 | best_recall@5:0.5209 | best_recall@10:0.7250 | best_recall@20:0.8056 | best_precision@5:0.6251 | best_precision@10:0.4350 | best_precision@20:0.2417 | 
epoch: 90, training loss: 31.793675987650033, train time: 335.6848096847534
epoch: 91, training loss: 31.187480827719966, train time: 337.097083568573
epoch: 92, training loss: 31.76674386278212, train time: 336.2989399433136
epoch: 93, training loss: 29.814579340025148, train time: 336.509418964386
epoch: 94, training loss: 29.691039418944314, train time: 336.6670913696289
epoch: 95, training loss: 29.169259243811368, train time: 336.2051329612732
epoch: 96, training loss: 28.79814205834083, train time: 336.4492812156677
epoch: 97, training loss: 28.28757376513449, train time: 335.4014666080475
epoch: 98, training loss: 28.152523029511272, train time: 336.80864238739014
epoch: 99, training loss: 27.12164414722427, train time: 335.3612926006317
epoch: 100, training loss: 26.45795987074177, train time: 336.43869805336
epoch: 101, training loss: 27.244317306898047, train time: 335.4225239753723
epoch: 102, training loss: 26.594032110512963, train time: 336.8832485675812
epoch: 103, training loss: 25.070726588224034, train time: 325.2260262966156
epoch: 104, training loss: 25.575514615660694, train time: 324.52740931510925
epoch: 105, training loss: 24.59419608410351, train time: 333.6084280014038
epoch: 106, training loss: 24.442144259635935, train time: 336.784366607666
epoch: 107, training loss: 24.12078004082891, train time: 337.18178939819336
epoch: 108, training loss: 24.249569620411634, train time: 336.1640260219574
epoch: 109, training loss: 23.50899997674567, train time: 336.1334354877472
epoch: 110, training loss: 22.949180248757173, train time: 335.80464696884155
epoch: 111, training loss: 23.37824706346693, train time: 336.79056310653687
epoch: 112, training loss: 22.644161610251555, train time: 336.8768813610077
epoch: 113, training loss: 22.88853442368487, train time: 335.95214557647705
epoch: 114, training loss: 22.271891138538535, train time: 336.4971008300781
epoch: 115, training loss: 22.424965707683953, train time: 335.7235052585602
epoch: 116, training loss: 21.843301041400665, train time: 336.5808072090149
epoch: 117, training loss: 21.388173131201345, train time: 336.7634983062744
epoch: 118, training loss: 21.982288855989058, train time: 335.7115914821625
epoch: 119, training loss: 21.638118955872724, train time: 335.325820684433
epo:119 | HR@5:0.7904 | HR@10:0.8940 | HR@20:0.9600 | NDCG@5:0.4939 | NDCG@10:0.5209 | NDCG@20:0.5618 | recall@5:0.4888 | recall@10:0.6831 | recall@20:0.7822 | precision@5:0.5866 | precision@10:0.4098 | precision@20:0.2347 | best_HR@5:0.8483 | best_HR@10:0.9300 | best_HR@20:0.9794 | best_NDCG@5:0.4956 | best_NDCG@10:0.5223 | best_NDCG@20:0.5627 | best_recall@5:0.5209 | best_recall@10:0.7250 | best_recall@20:0.8056 | best_precision@5:0.6251 | best_precision@10:0.4350 | best_precision@20:0.2417 | 
epoch: 120, training loss: 21.871305623166307, train time: 336.00730180740356
epoch: 121, training loss: 21.189837640800462, train time: 336.8658790588379
epoch: 122, training loss: 20.884564604574763, train time: 337.1795425415039
epoch: 123, training loss: 20.142713555796945, train time: 336.36553263664246
epoch: 124, training loss: 20.75095928671888, train time: 336.1505825519562
epoch: 125, training loss: 20.08098479326386, train time: 336.7104995250702
epoch: 126, training loss: 20.491754790635873, train time: 336.7639117240906
epoch: 127, training loss: 21.12936168877974, train time: 336.0705931186676
epoch: 128, training loss: 19.419205940012972, train time: 336.17097449302673
epoch: 129, training loss: 20.270492034688722, train time: 336.9903573989868
epoch: 130, training loss: 19.499307156567497, train time: 324.2434198856354
epoch: 131, training loss: 19.148510069738137, train time: 324.6115176677704
epoch: 132, training loss: 19.1852013007745, train time: 320.81112909317017
epoch: 133, training loss: 19.693829712455923, train time: 318.8083727359772
epoch: 134, training loss: 19.337611080123597, train time: 318.76343536376953
epoch: 135, training loss: 19.227023991844057, train time: 318.5318033695221
epoch: 136, training loss: 18.163141956150533, train time: 318.3424928188324
epoch: 137, training loss: 18.76390009713941, train time: 318.4522612094879
epoch: 138, training loss: 19.88192395663919, train time: 318.44996190071106
epoch: 139, training loss: 17.5827703438092, train time: 318.7587080001831
epoch: 140, training loss: 18.450793015038986, train time: 318.770446062088
epoch: 141, training loss: 19.068117961089925, train time: 318.3498282432556
epoch: 142, training loss: 17.97433862267666, train time: 318.433069229126
epoch: 143, training loss: 18.05471175364001, train time: 319.84952569007874
epoch: 144, training loss: 18.10583455379402, train time: 318.813529253006
epoch: 145, training loss: 18.124354986972463, train time: 318.8739011287689
epoch: 146, training loss: 17.283926861764005, train time: 318.46714782714844
epoch: 147, training loss: 18.40048202525071, train time: 318.337904214859
epoch: 148, training loss: 17.500101527015275, train time: 318.38713026046753
epoch: 149, training loss: 16.978632417531117, train time: 318.55126786231995
epo:149 | HR@5:0.7866 | HR@10:0.8885 | HR@20:0.9593 | NDCG@5:0.4943 | NDCG@10:0.5210 | NDCG@20:0.5617 | recall@5:0.4883 | recall@10:0.6807 | recall@20:0.7819 | precision@5:0.5860 | precision@10:0.4084 | precision@20:0.2346 | best_HR@5:0.8483 | best_HR@10:0.9300 | best_HR@20:0.9794 | best_NDCG@5:0.4956 | best_NDCG@10:0.5223 | best_NDCG@20:0.5627 | best_recall@5:0.5209 | best_recall@10:0.7250 | best_recall@20:0.8056 | best_precision@5:0.6251 | best_precision@10:0.4350 | best_precision@20:0.2417 | 
training finish
