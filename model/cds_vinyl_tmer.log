nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_CDs_Vinyl......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  8.58306884765625e-06
user  100 time:  169.74003887176514
user  200 time:  337.4846136569977
user  300 time:  509.09475803375244
user  400 time:  679.0530297756195
user  500 time:  851.4114193916321
user  600 time:  1026.2006859779358
user  700 time:  1198.1955425739288
user  800 time:  1369.4246621131897
user  900 time:  1542.8855657577515
user  1000 time:  1714.569662809372
user  1100 time:  1886.9199569225311
user  1200 time:  2056.6015796661377
user  1300 time:  2229.696742773056
user  1400 time:  2401.591510772705
user  1500 time:  2577.844674348831
user  1600 time:  2750.855459690094
user  1700 time:  2925.5601201057434
user  1800 time:  3100.6192293167114
user  1900 time:  3273.1374299526215
start training item-item instance self attention module...
user  0 time:  6.67572021484375e-06
user  100 time:  30.78593158721924
user  200 time:  63.9260687828064
user  300 time:  93.48325824737549
user  400 time:  122.87308716773987
user  500 time:  151.38963770866394
user  600 time:  179.9815776348114
user  700 time:  207.682523727417
user  800 time:  236.87372159957886
user  900 time:  270.0839800834656
user  1000 time:  300.575275182724
user  1100 time:  329.9369239807129
user  1200 time:  358.16228890419006
user  1300 time:  387.24828147888184
user  1400 time:  413.9505181312561
user  1500 time:  444.8485162258148
user  1600 time:  472.17663645744324
user  1700 time:  502.4380781650543
user  1800 time:  533.5696914196014
user  1900 time:  565.5655786991119
start updating user and item embedding...
user_name:2000
user  0 time:  7.62939453125e-06
user  100 time:  15.579721689224243
user  200 time:  31.6824734210968
user  300 time:  47.574806213378906
user  400 time:  63.263686656951904
user  500 time:  78.70975828170776
user  600 time:  94.56719541549683
user  700 time:  110.52306485176086
user  800 time:  126.24876117706299
user  900 time:  141.98820543289185
user  1000 time:  157.63774490356445
user  1100 time:  173.30950284004211
user  1200 time:  188.807932138443
user  1300 time:  204.50816559791565
user  1400 time:  220.046484708786
user  1500 time:  235.56186294555664
user  1600 time:  251.090096950531
user  1700 time:  266.5940613746643
user  1800 time:  282.5151000022888
user  1900 time:  298.27972197532654
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 142.09321630207705, train time: 100.91480946540833
epoch: 1, training loss: 67.35868644947914, train time: 101.19414258003235
epoch: 2, training loss: 54.779282621664606, train time: 100.58503270149231
epoch: 3, training loss: 48.9531732243413, train time: 100.69292736053467
epoch: 4, training loss: 44.10915970204951, train time: 100.73859238624573
epoch: 5, training loss: 40.616067023031064, train time: 101.3631842136383
epoch: 6, training loss: 38.295527285115895, train time: 100.82208180427551
epoch: 7, training loss: 36.02467720104141, train time: 101.14371633529663
epoch: 8, training loss: 34.26804599319075, train time: 100.90238308906555
epoch: 9, training loss: 32.71522813554657, train time: 101.03148937225342
epoch: 10, training loss: 30.622911241927795, train time: 100.75302457809448
epoch: 11, training loss: 29.533467164275862, train time: 101.22426581382751
epoch: 12, training loss: 27.912939640493278, train time: 101.13817715644836
epoch: 13, training loss: 27.2454023603932, train time: 101.14301013946533
epoch: 14, training loss: 25.030676388705615, train time: 101.05493307113647
epoch: 15, training loss: 23.7795440930322, train time: 100.70983076095581
epoch: 16, training loss: 24.27764837673476, train time: 100.532301902771
epoch: 17, training loss: 22.924123032495118, train time: 100.80894470214844
epoch: 18, training loss: 22.23538056022062, train time: 101.00735306739807
epoch: 19, training loss: 21.284397650774736, train time: 101.4907546043396
epoch: 20, training loss: 20.983003837163324, train time: 100.68456220626831
epoch: 21, training loss: 19.86331412439904, train time: 100.89562845230103
epoch: 22, training loss: 19.835689743512376, train time: 100.21718573570251
epoch: 23, training loss: 18.93337548506497, train time: 100.8214201927185
epoch: 24, training loss: 18.371699630253715, train time: 100.87758135795593
epoch: 25, training loss: 17.68715605155012, train time: 100.71153044700623
epoch: 26, training loss: 17.154155388052914, train time: 101.19894790649414
epoch: 27, training loss: 17.069700345799447, train time: 100.53619456291199
epoch: 28, training loss: 16.861605526550647, train time: 100.70568227767944
epoch: 29, training loss: 16.51135866043569, train time: 100.53541994094849
epo:29 | HR@5:0.8985 | HR@10:0.9178 | HR@20:0.9385 | NDCG@5:0.4435 | NDCG@10:0.4851 | NDCG@20:0.5366 | recall@5:0.6429 | recall@10:0.7498 | recall@20:0.7737 | precision@5:0.7715 | precision@10:0.4499 | precision@20:0.2321 | best_HR@5:0.8985 | best_HR@10:0.9178 | best_HR@20:0.9385 | best_NDCG@5:0.4435 | best_NDCG@10:0.4851 | best_NDCG@20:0.5366 | best_recall@5:0.6429 | best_recall@10:0.7498 | best_recall@20:0.7737 | best_precision@5:0.7715 | best_precision@10:0.4499 | best_precision@20:0.2321 | 
epoch: 30, training loss: 16.579964321681018, train time: 100.5747618675232
epoch: 31, training loss: 14.779755611064047, train time: 100.71144366264343
epoch: 32, training loss: 15.694588147095601, train time: 100.40749096870422
epoch: 33, training loss: 16.119170594507978, train time: 100.35834312438965
epoch: 34, training loss: 15.35934085649842, train time: 100.15377759933472
epoch: 35, training loss: 14.586222503652152, train time: 97.22939848899841
epoch: 36, training loss: 15.483241401166197, train time: 97.37532925605774
epoch: 37, training loss: 14.597719221985244, train time: 99.55542516708374
epoch: 38, training loss: 13.566595804980352, train time: 100.81233167648315
epoch: 39, training loss: 15.060688305456097, train time: 97.51584529876709
epoch: 40, training loss: 14.141103177851846, train time: 98.87151193618774
epoch: 41, training loss: 14.11935053689399, train time: 100.75176930427551
epoch: 42, training loss: 12.76959491963953, train time: 100.57190895080566
epoch: 43, training loss: 13.021660561630142, train time: 100.1884093284607
epoch: 44, training loss: 13.579570865945016, train time: 100.854727268219
epoch: 45, training loss: 13.539222915195978, train time: 100.31594896316528
epoch: 46, training loss: 12.523973159972229, train time: 100.88282585144043
epoch: 47, training loss: 13.635788615686124, train time: 100.73301768302917
epoch: 48, training loss: 12.678477137128993, train time: 100.11984539031982
epoch: 49, training loss: 13.13062638817155, train time: 100.59277486801147
epoch: 50, training loss: 13.076328711181532, train time: 100.48712611198425
epoch: 51, training loss: 13.118112313001348, train time: 100.11798763275146
epoch: 52, training loss: 12.537976395576834, train time: 100.33856296539307
epoch: 53, training loss: 12.502288409803896, train time: 100.64859318733215
epoch: 54, training loss: 11.309387838731482, train time: 100.59510898590088
epoch: 55, training loss: 12.089620424984105, train time: 100.4572365283966
epoch: 56, training loss: 12.476294116816462, train time: 100.88943672180176
epoch: 57, training loss: 11.578038041021614, train time: 100.64730525016785
epoch: 58, training loss: 11.611716270971101, train time: 100.5776846408844
epoch: 59, training loss: 13.200759670988873, train time: 100.57785606384277
epo:59 | HR@5:0.8788 | HR@10:0.8959 | HR@20:0.9213 | NDCG@5:0.4602 | NDCG@10:0.5014 | NDCG@20:0.5519 | recall@5:0.6317 | recall@10:0.7339 | recall@20:0.7587 | precision@5:0.7580 | precision@10:0.4403 | precision@20:0.2276 | best_HR@5:0.8985 | best_HR@10:0.9178 | best_HR@20:0.9385 | best_NDCG@5:0.4602 | best_NDCG@10:0.5014 | best_NDCG@20:0.5519 | best_recall@5:0.6429 | best_recall@10:0.7498 | best_recall@20:0.7737 | best_precision@5:0.7715 | best_precision@10:0.4499 | best_precision@20:0.2321 | 
epoch: 60, training loss: 11.630286017919616, train time: 100.53943347930908
epoch: 61, training loss: 11.620277474909244, train time: 100.55115413665771
epoch: 62, training loss: 11.906956498593104, train time: 100.68531513214111
epoch: 63, training loss: 11.725743754308496, train time: 100.18165850639343
epoch: 64, training loss: 12.33632032235846, train time: 100.58021378517151
epoch: 65, training loss: 11.463228328106652, train time: 100.9098768234253
epoch: 66, training loss: 11.41127766149907, train time: 100.81484746932983
epoch: 67, training loss: 10.632075460386545, train time: 100.88363242149353
epoch: 68, training loss: 11.101442452951119, train time: 100.38273739814758
epoch: 69, training loss: 12.072749495319158, train time: 101.07901263237
epoch: 70, training loss: 11.299635465337701, train time: 100.55449271202087
epoch: 71, training loss: 10.758912084017311, train time: 100.87897324562073
epoch: 72, training loss: 11.552409203093475, train time: 100.67678117752075
epoch: 73, training loss: 10.837671412507461, train time: 100.8543438911438
epoch: 74, training loss: 11.000675273338175, train time: 100.47727751731873
epoch: 75, training loss: 10.927003401308639, train time: 100.42742037773132
epoch: 76, training loss: 11.687842282404517, train time: 100.8175835609436
epoch: 77, training loss: 11.024271797102756, train time: 101.4626681804657
epoch: 78, training loss: 10.890743625713185, train time: 100.33304929733276
epoch: 79, training loss: 10.909937079108431, train time: 101.30633020401001
epoch: 80, training loss: 11.047557381361457, train time: 100.99065947532654
epoch: 81, training loss: 9.697243441666842, train time: 101.07239770889282
epoch: 82, training loss: 9.951805256399439, train time: 100.9230546951294
epoch: 83, training loss: 10.863584182793602, train time: 100.70761060714722
epoch: 84, training loss: 11.378062466712436, train time: 101.04357767105103
epoch: 85, training loss: 11.147218845851512, train time: 100.89128255844116
epoch: 86, training loss: 10.904958276274101, train time: 101.19323778152466
epoch: 87, training loss: 10.795481118474981, train time: 101.2787082195282
epoch: 88, training loss: 10.15964451096636, train time: 100.48194670677185
epoch: 89, training loss: 9.976038747619441, train time: 101.09347033500671
epo:89 | HR@5:0.8646 | HR@10:0.8840 | HR@20:0.9073 | NDCG@5:0.4697 | NDCG@10:0.5107 | NDCG@20:0.5605 | recall@5:0.6308 | recall@10:0.7222 | recall@20:0.7475 | precision@5:0.7570 | precision@10:0.4333 | precision@20:0.2242 | best_HR@5:0.8985 | best_HR@10:0.9178 | best_HR@20:0.9385 | best_NDCG@5:0.4697 | best_NDCG@10:0.5107 | best_NDCG@20:0.5605 | best_recall@5:0.6429 | best_recall@10:0.7498 | best_recall@20:0.7737 | best_precision@5:0.7715 | best_precision@10:0.4499 | best_precision@20:0.2321 | 
epoch: 90, training loss: 10.19356570990169, train time: 101.00896096229553
epoch: 91, training loss: 10.352780000572125, train time: 100.9332947731018
epoch: 92, training loss: 10.77630717516854, train time: 100.95146226882935
epoch: 93, training loss: 11.093866035499445, train time: 101.05414724349976
epoch: 94, training loss: 11.00865407847823, train time: 101.27513527870178
epoch: 95, training loss: 9.735222452894504, train time: 101.24209547042847
epoch: 96, training loss: 10.205549891899068, train time: 101.07447171211243
epoch: 97, training loss: 10.272607788784399, train time: 100.7991075515747
epoch: 98, training loss: 10.088947615864527, train time: 101.01872563362122
epoch: 99, training loss: 9.872784419072332, train time: 101.42708325386047
epoch: 100, training loss: 9.649400457331126, train time: 100.93620657920837
epoch: 101, training loss: 10.5546218790887, train time: 101.15001249313354
epoch: 102, training loss: 9.812800849057794, train time: 100.82005023956299
epoch: 103, training loss: 10.313476281894168, train time: 101.03383612632751
epoch: 104, training loss: 9.550024763918259, train time: 100.5282392501831
epoch: 105, training loss: 9.207946289276151, train time: 100.59349393844604
epoch: 106, training loss: 9.407663510299471, train time: 101.08730244636536
epoch: 107, training loss: 10.191930336735481, train time: 101.01140999794006
epoch: 108, training loss: 9.547866640161601, train time: 101.0459954738617
epoch: 109, training loss: 9.730477175727515, train time: 100.85180878639221
epoch: 110, training loss: 9.921453500757934, train time: 100.68010091781616
epoch: 111, training loss: 8.706587720856191, train time: 100.91346454620361
epoch: 112, training loss: 8.672916472082534, train time: 101.42305898666382
epoch: 113, training loss: 9.685346008536271, train time: 100.9982259273529
epoch: 114, training loss: 9.432940645458075, train time: 101.34101152420044
epoch: 115, training loss: 9.738147221202667, train time: 101.00845956802368
epoch: 116, training loss: 9.219563509409227, train time: 100.60468459129333
epoch: 117, training loss: 8.901451659375994, train time: 100.93754053115845
epoch: 118, training loss: 8.952799765485338, train time: 101.01109671592712
epoch: 119, training loss: 9.752893696717251, train time: 100.9424500465393
epo:119 | HR@5:0.8568 | HR@10:0.8746 | HR@20:0.8996 | NDCG@5:0.4603 | NDCG@10:0.5023 | NDCG@20:0.5533 | recall@5:0.6234 | recall@10:0.7163 | recall@20:0.7405 | precision@5:0.7481 | precision@10:0.4298 | precision@20:0.2221 | best_HR@5:0.8985 | best_HR@10:0.9178 | best_HR@20:0.9385 | best_NDCG@5:0.4697 | best_NDCG@10:0.5107 | best_NDCG@20:0.5605 | best_recall@5:0.6429 | best_recall@10:0.7498 | best_recall@20:0.7737 | best_precision@5:0.7715 | best_precision@10:0.4499 | best_precision@20:0.2321 | 
epoch: 120, training loss: 9.46753996200141, train time: 98.32639741897583
epoch: 121, training loss: 8.846873816832328, train time: 98.20301008224487
epoch: 122, training loss: 9.119320620831786, train time: 97.95966625213623
epoch: 123, training loss: 9.119337831026883, train time: 98.18941164016724
epoch: 124, training loss: 9.666892272632083, train time: 101.09221482276917
epoch: 125, training loss: 10.606027782778938, train time: 101.41371822357178
epoch: 126, training loss: 10.210570392461818, train time: 101.21634221076965
epoch: 127, training loss: 10.367672030370613, train time: 100.73631024360657
epoch: 128, training loss: 10.294205201523482, train time: 100.89133095741272
epoch: 129, training loss: 8.999051363254239, train time: 101.19267416000366
epoch: 130, training loss: 9.994911889721834, train time: 101.31238603591919
epoch: 131, training loss: 10.095852201463117, train time: 101.40824484825134
epoch: 132, training loss: 10.243154344540358, train time: 101.10421466827393
epoch: 133, training loss: 10.721399565113188, train time: 101.2065966129303
epoch: 134, training loss: 10.66484713098248, train time: 101.06114888191223
epoch: 135, training loss: 10.462122679095273, train time: 101.65598440170288
epoch: 136, training loss: 9.19404908223953, train time: 101.26167416572571
epoch: 137, training loss: 9.327464176769581, train time: 115.79158520698547
epoch: 138, training loss: 9.496033517408819, train time: 125.09756469726562
epoch: 139, training loss: 9.838606981948942, train time: 125.00484156608582
epoch: 140, training loss: 9.751833080392771, train time: 125.00244188308716
epoch: 141, training loss: 9.932609898281953, train time: 125.24740624427795
epoch: 142, training loss: 10.077324627307462, train time: 125.06213474273682
epoch: 143, training loss: 9.927934791927555, train time: 126.01816391944885
epoch: 144, training loss: 9.026486348901472, train time: 125.35286569595337
epoch: 145, training loss: 8.907881051043091, train time: 125.69373512268066
epoch: 146, training loss: 9.470137300420674, train time: 125.29956197738647
epoch: 147, training loss: 8.788698009616155, train time: 125.24488377571106
epoch: 148, training loss: 9.00759101062522, train time: 124.70640134811401
epoch: 149, training loss: 9.266074874228934, train time: 124.32461500167847
epo:149 | HR@5:0.8477 | HR@10:0.8660 | HR@20:0.8929 | NDCG@5:0.4775 | NDCG@10:0.5176 | NDCG@20:0.5665 | recall@5:0.6217 | recall@10:0.7104 | recall@20:0.7358 | precision@5:0.7460 | precision@10:0.4262 | precision@20:0.2207 | best_HR@5:0.8985 | best_HR@10:0.9178 | best_HR@20:0.9385 | best_NDCG@5:0.4775 | best_NDCG@10:0.5176 | best_NDCG@20:0.5665 | best_recall@5:0.6429 | best_recall@10:0.7498 | best_recall@20:0.7737 | best_precision@5:0.7715 | best_precision@10:0.4499 | best_precision@20:0.2321 | 
training finish
