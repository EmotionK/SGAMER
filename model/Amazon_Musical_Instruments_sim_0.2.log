nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  9.5367431640625e-06
user  100 time:  194.59206128120422
user  200 time:  389.47796392440796
user  300 time:  592.9231033325195
user  400 time:  800.4412851333618
user  500 time:  1013.6676523685455
user  600 time:  1227.6005637645721
user  700 time:  1435.5473759174347
user  800 time:  1655.2118000984192
user  900 time:  1876.1030349731445
user  1000 time:  2096.8057103157043
user  1100 time:  2319.0709953308105
user  1200 time:  2539.116423368454
user  1300 time:  2762.2337458133698
user  1400 time:  2983.3530673980713
start training item-item instance self attention module...
user  0 time:  8.106231689453125e-06
user  100 time:  177.2311990261078
user  200 time:  341.9232497215271
user  300 time:  525.69695520401
user  400 time:  697.4745481014252
user  500 time:  865.078503370285
user  600 time:  1043.9710264205933
user  700 time:  1215.2666718959808
user  800 time:  1393.53124666214
user  900 time:  1563.4835879802704
user  1000 time:  1738.9389436244965
user  1100 time:  1919.286874294281
user  1200 time:  2090.856253862381
user  1300 time:  2274.4176557064056
user  1400 time:  2454.2696602344513
start updating user and item embedding...
user_name:1450
user  0 time:  1.1444091796875e-05
user  100 time:  72.5965929031372
user  200 time:  145.37824058532715
user  300 time:  218.64837002754211
user  400 time:  291.9088842868805
user  500 time:  372.14771366119385
user  600 time:  452.6099226474762
user  700 time:  532.5107958316803
user  800 time:  612.8806855678558
user  900 time:  693.4143707752228
user  1000 time:  773.8957877159119
user  1100 time:  854.1526656150818
user  1200 time:  934.7373886108398
user  1300 time:  1015.0065908432007
user  1400 time:  1095.8135607242584
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:67: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 152.1591564490227, train time: 61.48541831970215
epoch: 1, training loss: 86.9031427737209, train time: 61.172858238220215
epoch: 2, training loss: 73.75838741479674, train time: 60.97934556007385
epoch: 3, training loss: 65.65360964353022, train time: 61.41099429130554
epoch: 4, training loss: 60.000325220491504, train time: 61.560200929641724
epoch: 5, training loss: 55.7527618304448, train time: 61.43542242050171
epoch: 6, training loss: 52.53473632992245, train time: 61.49397587776184
epoch: 7, training loss: 49.221711667079944, train time: 61.51666021347046
epoch: 8, training loss: 46.5728000287927, train time: 61.48455619812012
epoch: 9, training loss: 44.61026880377176, train time: 61.75651812553406
epoch: 10, training loss: 41.9863571051319, train time: 61.678470849990845
epoch: 11, training loss: 40.063133312083664, train time: 61.486055850982666
epoch: 12, training loss: 38.72741882690025, train time: 63.528191566467285
epoch: 13, training loss: 36.23743151112285, train time: 63.32035565376282
epoch: 14, training loss: 35.236247326625744, train time: 63.44231200218201
epoch: 15, training loss: 33.64087396240575, train time: 63.16965651512146
epoch: 16, training loss: 31.456476390238095, train time: 65.720468044281
epoch: 17, training loss: 30.919085582485422, train time: 63.67377948760986
epoch: 18, training loss: 28.742363870169356, train time: 63.0052056312561
epoch: 19, training loss: 27.9059852719256, train time: 63.323251247406006
epoch: 20, training loss: 26.7711133541452, train time: 63.202205181121826
epoch: 21, training loss: 25.57671655682134, train time: 63.11872982978821
epoch: 22, training loss: 23.344192077503976, train time: 63.20200037956238
epoch: 23, training loss: 23.186225411518535, train time: 63.14736843109131
epoch: 24, training loss: 21.930155646098683, train time: 71.09383964538574
epoch: 25, training loss: 20.434397216722573, train time: 74.15236926078796
epoch: 26, training loss: 20.564700532830102, train time: 74.34974002838135
epoch: 27, training loss: 19.53911337964655, train time: 74.46488952636719
epoch: 28, training loss: 18.97433979618836, train time: 74.7300500869751
epoch: 29, training loss: 19.941916062281962, train time: 74.38694071769714
保存hit模型
保存ndcg模型
epo:29 | HR@5:0.8070 | HR@10:0.8497 | HR@20:0.8913 | NDCG@5:0.4525 | NDCG@10:0.4907 | NDCG@20:0.5399 | best_HR@5:0.8070 | best_HR@10:0.8497 | best_HR@20:0.8913 | best_NDCG@5:0.4525 | best_NDCG@10:0.4907 | best_NDCG@20:0.5399 | 
epoch: 30, training loss: 17.280280887108347, train time: 89.38923335075378
epoch: 31, training loss: 16.358824960627317, train time: 89.75412201881409
epoch: 32, training loss: 16.256083685932936, train time: 89.44031119346619
epoch: 33, training loss: 17.09353691025717, train time: 84.49206638336182
epoch: 34, training loss: 16.064044389604533, train time: 77.68339204788208
epoch: 35, training loss: 15.038298976075112, train time: 77.62500357627869
epoch: 36, training loss: 15.612406795803508, train time: 77.34663915634155
epoch: 37, training loss: 15.25277672186212, train time: 77.79628038406372
epoch: 38, training loss: 14.948499792433267, train time: 77.46176958084106
epoch: 39, training loss: 14.743851824958256, train time: 77.54260635375977
epoch: 40, training loss: 14.669916656520854, train time: 77.19036030769348
epoch: 41, training loss: 13.98855450420524, train time: 86.89961242675781
epoch: 42, training loss: 14.408550547970435, train time: 89.55423879623413
epoch: 43, training loss: 13.425132431668317, train time: 89.80096793174744
epoch: 44, training loss: 13.998732182205686, train time: 89.3562171459198
epoch: 45, training loss: 13.356497574570767, train time: 82.79255890846252
epoch: 46, training loss: 14.044715188893065, train time: 77.74337697029114
epoch: 47, training loss: 12.054834202251868, train time: 77.46033716201782
epoch: 48, training loss: 13.076561831706613, train time: 77.40608835220337
epoch: 49, training loss: 12.888390184483796, train time: 77.67400360107422
epoch: 50, training loss: 12.723348930947395, train time: 77.71130394935608
epoch: 51, training loss: 12.03237776899391, train time: 77.72189354896545
epoch: 52, training loss: 12.733784199160255, train time: 77.33939361572266
epoch: 53, training loss: 12.833115472371446, train time: 88.26955962181091
epoch: 54, training loss: 11.216619718963102, train time: 89.86936616897583
epoch: 55, training loss: 11.86552956099888, train time: 77.62331652641296
epoch: 56, training loss: 12.438289961996134, train time: 77.99498915672302
epoch: 57, training loss: 12.118342931171696, train time: 77.88796973228455
epoch: 58, training loss: 11.417648827198377, train time: 77.5639135837555
epoch: 59, training loss: 11.34371110190034, train time: 77.66904330253601
保存ndcg模型
epo:59 | HR@5:0.7647 | HR@10:0.8090 | HR@20:0.8549 | NDCG@5:0.4824 | NDCG@10:0.5175 | NDCG@20:0.5635 | best_HR@5:0.8070 | best_HR@10:0.8497 | best_HR@20:0.8913 | best_NDCG@5:0.4824 | best_NDCG@10:0.5175 | best_NDCG@20:0.5635 | 
epoch: 60, training loss: 11.423513094881628, train time: 90.52058362960815
epoch: 61, training loss: 11.199719358119864, train time: 89.97692584991455
epoch: 62, training loss: 11.574773762339646, train time: 78.87666654586792
epoch: 63, training loss: 12.00765003461845, train time: 77.25443267822266
epoch: 64, training loss: 10.457643529560357, train time: 77.60875225067139
epoch: 65, training loss: 10.92838819800727, train time: 77.861248254776
epoch: 66, training loss: 11.55097351439315, train time: 77.96596097946167
epoch: 67, training loss: 9.604614357887726, train time: 77.68481063842773
epoch: 68, training loss: 9.971209443833942, train time: 77.39861464500427
epoch: 69, training loss: 11.173072631128207, train time: 80.97863149642944
epoch: 70, training loss: 11.064374057065152, train time: 89.50320029258728
epoch: 71, training loss: 10.036693483288786, train time: 89.45976138114929
epoch: 72, training loss: 10.696706816266897, train time: 89.95840644836426
epoch: 73, training loss: 10.269181199079526, train time: 88.60663270950317
epoch: 74, training loss: 9.773482096309863, train time: 77.84138441085815
epoch: 75, training loss: 10.123434586963867, train time: 77.96429228782654
epoch: 76, training loss: 10.350381617735138, train time: 77.35796689987183
epoch: 77, training loss: 10.47933201990412, train time: 77.84666156768799
epoch: 78, training loss: 9.95304644486373, train time: 77.77594232559204
epoch: 79, training loss: 9.714340521073382, train time: 77.7898211479187
epoch: 80, training loss: 8.868374051855994, train time: 77.48154044151306
epoch: 81, training loss: 10.333994523991123, train time: 83.18800711631775
epoch: 82, training loss: 9.664803264045645, train time: 90.07032251358032
epoch: 83, training loss: 9.760079444330586, train time: 89.95903587341309
epoch: 84, training loss: 9.297574412875633, train time: 89.8513445854187
epoch: 85, training loss: 9.803158588711085, train time: 80.36944031715393
epoch: 86, training loss: 9.537194263874824, train time: 77.67756867408752
epoch: 87, training loss: 9.380653254073934, train time: 77.55442786216736
epoch: 88, training loss: 9.715533321468115, train time: 77.52430987358093
epoch: 89, training loss: 9.93459927143033, train time: 78.30443096160889
epo:89 | HR@5:0.7538 | HR@10:0.7975 | HR@20:0.8448 | NDCG@5:0.4806 | NDCG@10:0.5162 | NDCG@20:0.5623 | best_HR@5:0.8070 | best_HR@10:0.8497 | best_HR@20:0.8913 | best_NDCG@5:0.4824 | best_NDCG@10:0.5175 | best_NDCG@20:0.5635 | 
epoch: 90, training loss: 9.238776011554819, train time: 82.3374662399292
epoch: 91, training loss: 8.740823834052549, train time: 77.37343192100525
epoch: 92, training loss: 9.282677079593668, train time: 77.95254564285278
epoch: 93, training loss: 8.145348243810531, train time: 77.43464660644531
epoch: 94, training loss: 9.547817844987435, train time: 77.65745854377747
epoch: 95, training loss: 8.726221853673565, train time: 77.6710114479065
epoch: 96, training loss: 8.4117383018895, train time: 77.83282089233398
epoch: 97, training loss: 9.661531413436649, train time: 77.85937809944153
epoch: 98, training loss: 9.574611812989815, train time: 89.41240429878235
epoch: 99, training loss: 8.671809963245153, train time: 89.48873734474182
epoch: 100, training loss: 8.661904930505159, train time: 89.209796667099
epoch: 101, training loss: 9.217098385537156, train time: 89.4554283618927
epoch: 102, training loss: 9.001020899595346, train time: 80.63898181915283
epoch: 103, training loss: 8.65592077763904, train time: 77.38108015060425
epoch: 104, training loss: 8.566445690219382, train time: 77.81428670883179
epoch: 105, training loss: 8.992287621682124, train time: 78.89488673210144
epoch: 106, training loss: 8.90073557522777, train time: 77.54002141952515
epoch: 107, training loss: 9.725460815604322, train time: 77.50187563896179
epoch: 108, training loss: 7.5344889201338106, train time: 78.18328881263733
epoch: 109, training loss: 8.57788282144918, train time: 79.61927556991577
epoch: 110, training loss: 8.10503507685695, train time: 90.02639174461365
epoch: 111, training loss: 7.585061671957305, train time: 89.60554647445679
epoch: 112, training loss: 8.751033275339694, train time: 89.22461080551147
epoch: 113, training loss: 8.416737881051517, train time: 89.72896027565002
epoch: 114, training loss: 8.41546472462926, train time: 89.94958662986755
epoch: 115, training loss: 6.919118466955808, train time: 81.36786818504333
epoch: 116, training loss: 9.133909795793045, train time: 77.7381112575531
epoch: 117, training loss: 8.252013575981607, train time: 77.54977226257324
epoch: 118, training loss: 10.121790037720984, train time: 77.47733092308044
epoch: 119, training loss: 7.858237492660805, train time: 77.59310412406921
epo:119 | HR@5:0.7456 | HR@10:0.7860 | HR@20:0.8360 | NDCG@5:0.4779 | NDCG@10:0.5147 | NDCG@20:0.5614 | best_HR@5:0.8070 | best_HR@10:0.8497 | best_HR@20:0.8913 | best_NDCG@5:0.4824 | best_NDCG@10:0.5175 | best_NDCG@20:0.5635 | 
epoch: 120, training loss: 7.533449782959337, train time: 78.47828531265259
epoch: 121, training loss: 7.985663922754952, train time: 78.20233297348022
epoch: 122, training loss: 9.119855862770876, train time: 77.39171600341797
epoch: 123, training loss: 9.211987255091685, train time: 77.4520857334137
epoch: 124, training loss: 8.365947436163594, train time: 77.89422988891602
epoch: 125, training loss: 8.143745863743902, train time: 77.75446319580078
epoch: 126, training loss: 7.542254917668515, train time: 86.16156649589539
epoch: 127, training loss: 9.123401202302261, train time: 89.36240649223328
epoch: 128, training loss: 7.80034890664632, train time: 89.68047404289246
epoch: 129, training loss: 7.1085317735226, train time: 87.98117661476135
epoch: 130, training loss: 7.795303907404957, train time: 77.6475830078125
epoch: 131, training loss: 9.292287528451311, train time: 78.01251459121704
epoch: 132, training loss: 8.327379754365182, train time: 77.5259702205658
epoch: 133, training loss: 8.200815596156417, train time: 77.65855932235718
epoch: 134, training loss: 7.720095413464435, train time: 79.70280480384827
epoch: 135, training loss: 7.808139816916793, train time: 77.75720500946045
epoch: 136, training loss: 7.926500306891114, train time: 77.81161856651306
epoch: 137, training loss: 7.83394482129961, train time: 72.61176013946533
epoch: 138, training loss: 7.908037984885482, train time: 68.9960687160492
epoch: 139, training loss: 7.310936021035843, train time: 68.73941493034363
epoch: 140, training loss: 7.9122054454726936, train time: 69.0302038192749
epoch: 141, training loss: 7.1850270112348085, train time: 69.05265402793884
epoch: 142, training loss: 7.239452240446269, train time: 69.10312557220459
epoch: 143, training loss: 7.677636208447581, train time: 69.11833190917969
epoch: 144, training loss: 8.083344736757766, train time: 66.4351167678833
epoch: 145, training loss: 7.01435050989727, train time: 55.001976013183594
epoch: 146, training loss: 7.366737151292398, train time: 61.50291037559509
epoch: 147, training loss: 7.427305957866565, train time: 58.62685799598694
epoch: 148, training loss: 8.236972236298726, train time: 55.89610004425049
epoch: 149, training loss: 8.027745641122351, train time: 61.1387825012207
epo:149 | HR@5:0.7403 | HR@10:0.7816 | HR@20:0.8305 | NDCG@5:0.4802 | NDCG@10:0.5168 | NDCG@20:0.5633 | best_HR@5:0.8070 | best_HR@10:0.8497 | best_HR@20:0.8913 | best_NDCG@5:0.4824 | best_NDCG@10:0.5175 | best_NDCG@20:0.5635 | 
training finish
