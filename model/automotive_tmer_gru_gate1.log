nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.5762786865234375e-06
user  100 time:  209.7304813861847
user  200 time:  438.3273332118988
user  300 time:  669.948855638504
user  400 time:  901.215062379837
user  500 time:  1128.1858456134796
user  600 time:  1363.2128591537476
user  700 time:  1627.6632702350616
user  800 time:  1898.9833419322968
user  900 time:  2171.9356775283813
user  1000 time:  2444.72327375412
user  1100 time:  2722.1465129852295
user  1200 time:  2999.3318898677826
user  1300 time:  3278.330088376999
user  1400 time:  3555.028574705124
user  1500 time:  3838.642517566681
user  1600 time:  4118.244471788406
user  1700 time:  4446.737412214279
user  1800 time:  4794.150629043579
user  1900 time:  5148.590355873108
user  2000 time:  5483.552685022354
user  2100 time:  5807.203176021576
user  2200 time:  6146.218168020248
user  2300 time:  6476.425719261169
user  2400 time:  6797.144890546799
user  2500 time:  7123.6849846839905
user  2600 time:  7440.385078430176
user  2700 time:  7766.459820985794
user  2800 time:  8105.429067134857
user  2900 time:  8446.886353254318
user  3000 time:  8778.729846954346
user  3100 time:  9062.00620174408
user  3200 time:  9330.583049058914
user  3300 time:  9600.96167254448
user  3400 time:  9890.15565776825
user  3500 time:  10178.186358213425
user  3600 time:  10439.069709539413
user  3700 time:  10705.112006425858
user  3800 time:  10971.085719585419
user  3900 time:  11254.506160736084
user  4000 time:  11533.67912697792
user  4100 time:  11813.643389463425
user  4200 time:  12070.565353393555
user  4300 time:  12327.732645273209
user  4400 time:  12584.926376581192
user  4500 time:  12840.249875068665
start training item-item instance self attention module...
user  0 time:  6.198883056640625e-06
user  100 time:  195.3647792339325
user  200 time:  392.3208134174347
user  300 time:  585.0454068183899
user  400 time:  777.9792182445526
user  500 time:  964.8088042736053
user  600 time:  1150.9235472679138
user  700 time:  1344.9165165424347
user  800 time:  1535.024075269699
user  900 time:  1725.8785710334778
user  1000 time:  1922.6955139636993
user  1100 time:  2118.148878097534
user  1200 time:  2302.664144039154
user  1300 time:  2492.9982974529266
user  1400 time:  2668.8286986351013
user  1500 time:  2863.578198194504
user  1600 time:  3053.666183233261
user  1700 time:  3236.8833787441254
user  1800 time:  3435.4146332740784
user  1900 time:  3643.0100445747375
user  2000 time:  3838.6145474910736
user  2100 time:  4013.782378911972
user  2200 time:  4180.916086435318
user  2300 time:  4340.260096311569
user  2400 time:  4508.8956825733185
user  2500 time:  4675.659292459488
user  2600 time:  4848.488101482391
user  2700 time:  5026.073501586914
user  2800 time:  5205.895832777023
user  2900 time:  5373.937042951584
user  3000 time:  5542.783946752548
user  3100 time:  5717.464027404785
user  3200 time:  5877.657475709915
user  3300 time:  6046.872413396835
user  3400 time:  6219.700708627701
user  3500 time:  6389.643221139908
user  3600 time:  6571.33874297142
user  3700 time:  6742.028635025024
user  3800 time:  6917.133996486664
user  3900 time:  7073.077447414398
user  4000 time:  7254.456915378571
user  4100 time:  7438.697392702103
user  4200 time:  7614.689195156097
user  4300 time:  7786.261684179306
user  4400 time:  7959.415062665939
user  4500 time:  8123.544594049454
start updating user and item embedding...
user_name:4600
user  0 time:  1.5735626220703125e-05
user  100 time:  65.53449034690857
user  200 time:  130.1232030391693
user  300 time:  193.24781465530396
user  400 time:  258.7707860469818
user  500 time:  323.6967852115631
user  600 time:  387.20832228660583
user  700 time:  451.34647011756897
user  800 time:  515.335565328598
user  900 time:  579.9112079143524
user  1000 time:  643.8953273296356
user  1100 time:  707.7675395011902
user  1200 time:  772.4235799312592
user  1300 time:  836.9481353759766
user  1400 time:  901.6083643436432
user  1500 time:  964.8336324691772
user  1600 time:  1029.9444439411163
user  1700 time:  1094.521203994751
user  1800 time:  1157.686639547348
user  1900 time:  1221.0284943580627
user  2000 time:  1285.2555327415466
user  2100 time:  1349.1844334602356
user  2200 time:  1414.2801973819733
user  2300 time:  1477.6662738323212
user  2400 time:  1542.1037154197693
user  2500 time:  1606.7082810401917
user  2600 time:  1670.0810387134552
user  2700 time:  1734.0208506584167
user  2800 time:  1797.8625082969666
user  2900 time:  1862.115737438202
user  3000 time:  1926.0437505245209
user  3100 time:  1990.0360922813416
user  3200 time:  2053.280297279358
user  3300 time:  2118.3630194664
user  3400 time:  2182.9071514606476
user  3500 time:  2246.73765873909
user  3600 time:  2310.3531370162964
user  3700 time:  2374.101184129715
user  3800 time:  2438.218850374222
user  3900 time:  2502.3875999450684
user  4000 time:  2566.1958513259888
user  4100 time:  2630.3450405597687
user  4200 time:  2693.9586160182953
user  4300 time:  2758.254606962204
user  4400 time:  2821.9823610782623
user  4500 time:  2885.9458289146423
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 290.2855468313355, train time: 328.2358994483948
epoch: 1, training loss: 181.89655227816547, train time: 333.3000147342682
epoch: 2, training loss: 168.57086357165826, train time: 333.84343576431274
epoch: 3, training loss: 160.76299529631797, train time: 334.26786494255066
epoch: 4, training loss: 154.67109663963492, train time: 334.6349141597748
epoch: 5, training loss: 151.0205326143623, train time: 335.0787649154663
epoch: 6, training loss: 146.11740133749845, train time: 334.9926447868347
epoch: 7, training loss: 142.5663792139385, train time: 336.50805473327637
epoch: 8, training loss: 140.20290454693895, train time: 334.6777892112732
epoch: 9, training loss: 134.32571493393334, train time: 334.48552942276
epoch: 10, training loss: 132.19715363283467, train time: 336.7051525115967
epoch: 11, training loss: 128.97919094385725, train time: 334.47566390037537
epoch: 12, training loss: 124.8790400009675, train time: 333.4516131877899
epoch: 13, training loss: 122.19778300945472, train time: 334.22314739227295
epoch: 14, training loss: 120.18515539891087, train time: 333.19589710235596
epoch: 15, training loss: 119.74895783681859, train time: 334.921094417572
epoch: 16, training loss: 117.14534562647896, train time: 334.4221603870392
epoch: 17, training loss: 116.65876494866825, train time: 336.513468503952
epoch: 18, training loss: 113.11142255252344, train time: 335.1706554889679
epoch: 19, training loss: 112.32699660329672, train time: 334.3210039138794
epoch: 20, training loss: 110.37280571230076, train time: 335.63642477989197
epoch: 21, training loss: 110.4979654234412, train time: 336.1613874435425
epoch: 22, training loss: 108.52267654523894, train time: 336.0595796108246
epoch: 23, training loss: 108.36167777986702, train time: 333.10541105270386
epoch: 24, training loss: 109.03918832869385, train time: 334.5840673446655
epoch: 25, training loss: 107.07883719506572, train time: 335.5503103733063
epoch: 26, training loss: 107.24885017432098, train time: 334.84468173980713
epoch: 27, training loss: 107.04464534969884, train time: 334.5544984340668
epoch: 28, training loss: 107.85627217985166, train time: 336.2086365222931
epoch: 29, training loss: 106.03300622007373, train time: 335.1707036495209
epo:29 | HR@5:0.8952 | HR@10:0.9111 | HR@20:0.9319 | NDCG@5:0.4464 | NDCG@10:0.4893 | NDCG@20:0.5416 | recall@5:0.6411 | recall@10:0.7474 | recall@20:0.7699 | precision@5:0.7693 | precision@10:0.4484 | precision@20:0.2310 | best_HR@5:0.8952 | best_HR@10:0.9111 | best_HR@20:0.9319 | best_NDCG@5:0.4464 | best_NDCG@10:0.4893 | best_NDCG@20:0.5416 | best_recall@5:0.6411 | best_recall@10:0.7474 | best_recall@20:0.7699 | best_precision@5:0.7693 | best_precision@10:0.4484 | best_precision@20:0.2310 | 
epoch: 30, training loss: 105.79458924991923, train time: 325.98886823654175
epoch: 31, training loss: 106.59725975838228, train time: 336.5300397872925
epoch: 32, training loss: 105.7559974617543, train time: 335.38674807548523
epoch: 33, training loss: 106.33972187332256, train time: 335.2646610736847
epoch: 34, training loss: 103.77923935204308, train time: 333.58558559417725
epoch: 35, training loss: 105.35678598337108, train time: 335.233430147171
epoch: 36, training loss: 104.23663151057553, train time: 336.71701097488403
epoch: 37, training loss: 105.20245883792813, train time: 335.3095192909241
epoch: 38, training loss: 106.80083541868953, train time: 334.3079299926758
epoch: 39, training loss: 106.10230196668999, train time: 334.91498947143555
epoch: 40, training loss: 105.48355019226437, train time: 333.9953966140747
epoch: 41, training loss: 105.39924326557957, train time: 336.0430340766907
epoch: 42, training loss: 106.32853676605009, train time: 335.11993408203125
epoch: 43, training loss: 107.29335074184928, train time: 334.6781017780304
epoch: 44, training loss: 106.48576421879261, train time: 334.78981137275696
epoch: 45, training loss: 106.40030351367022, train time: 334.55063366889954
epoch: 46, training loss: 106.11578114229633, train time: 335.3371777534485
epoch: 47, training loss: 106.28692607336416, train time: 336.09912943840027
epoch: 48, training loss: 104.2277918332693, train time: 334.8461697101593
epoch: 49, training loss: 107.07649806385598, train time: 335.1055374145508
epoch: 50, training loss: 105.95239124263753, train time: 335.36517548561096
epoch: 51, training loss: 103.52191138707713, train time: 334.5182957649231
epoch: 52, training loss: 104.23690338646702, train time: 335.5436370372772
epoch: 53, training loss: 102.99344478170678, train time: 335.3373363018036
epoch: 54, training loss: 104.24282045041764, train time: 336.29434084892273
epoch: 55, training loss: 103.20144366047316, train time: 335.70991492271423
epoch: 56, training loss: 105.15011390919244, train time: 335.66087007522583
epoch: 57, training loss: 102.97902208089363, train time: 335.97767877578735
epoch: 58, training loss: 105.23231303703506, train time: 335.63029074668884
epoch: 59, training loss: 104.03397726224648, train time: 335.32555079460144
epo:59 | HR@5:0.8690 | HR@10:0.8892 | HR@20:0.9137 | NDCG@5:0.4469 | NDCG@10:0.4884 | NDCG@20:0.5400 | recall@5:0.6320 | recall@10:0.7263 | recall@20:0.7524 | precision@5:0.7584 | precision@10:0.4358 | precision@20:0.2257 | best_HR@5:0.8952 | best_HR@10:0.9111 | best_HR@20:0.9319 | best_NDCG@5:0.4469 | best_NDCG@10:0.4893 | best_NDCG@20:0.5416 | best_recall@5:0.6411 | best_recall@10:0.7474 | best_recall@20:0.7699 | best_precision@5:0.7693 | best_precision@10:0.4484 | best_precision@20:0.2310 | 
epoch: 60, training loss: 103.65265244594775, train time: 338.07150530815125
epoch: 61, training loss: 102.632306784697, train time: 335.0992524623871
epoch: 62, training loss: 103.65536341362167, train time: 335.70688819885254
epoch: 63, training loss: 103.1614612637859, train time: 338.06935596466064
epoch: 64, training loss: 103.4438414327451, train time: 341.6748676300049
epoch: 65, training loss: 105.94383986323373, train time: 334.2263398170471
epoch: 66, training loss: 104.64514926236734, train time: 336.504346370697
epoch: 67, training loss: 103.47970386104134, train time: 337.70013213157654
epoch: 68, training loss: 102.75418773869023, train time: 344.6815097332001
epoch: 69, training loss: 104.47054813189607, train time: 343.7138524055481
epoch: 70, training loss: 102.11344511288917, train time: 341.95812797546387
epoch: 71, training loss: 102.48060212703422, train time: 364.2294065952301
epoch: 72, training loss: 98.12390772945946, train time: 410.08902525901794
epoch: 73, training loss: 97.52063573426858, train time: 436.6762504577637
epoch: 74, training loss: 98.51345424621832, train time: 436.3388788700104
epoch: 75, training loss: 99.38655167788966, train time: 435.94243121147156
epoch: 76, training loss: 97.68449460875127, train time: 437.0097064971924
epoch: 77, training loss: 96.80810258099518, train time: 435.46958684921265
epoch: 78, training loss: 97.40905456637847, train time: 436.02865171432495
epoch: 79, training loss: 95.83187924702361, train time: 436.3862934112549
epoch: 80, training loss: 95.67942403221969, train time: 430.586895942688
epoch: 81, training loss: 98.04621402574412, train time: 429.3779423236847
epoch: 82, training loss: 93.74003399040521, train time: 428.89394998550415
epoch: 83, training loss: 94.60596332024579, train time: 430.9536757469177
epoch: 84, training loss: 93.93125484823395, train time: 429.2491602897644
epoch: 85, training loss: 94.88710523349437, train time: 428.69462299346924
epoch: 86, training loss: 92.98485221784358, train time: 427.90751910209656
epoch: 87, training loss: 93.30562741198082, train time: 428.50024795532227
epoch: 88, training loss: 93.12521893266239, train time: 428.7413008213043
epoch: 89, training loss: 94.00667426097061, train time: 427.75471234321594
epo:89 | HR@5:0.8609 | HR@10:0.8784 | HR@20:0.9038 | NDCG@5:0.4398 | NDCG@10:0.4824 | NDCG@20:0.5348 | recall@5:0.6262 | recall@10:0.7187 | recall@20:0.7444 | precision@5:0.7514 | precision@10:0.4312 | precision@20:0.2233 | best_HR@5:0.8952 | best_HR@10:0.9111 | best_HR@20:0.9319 | best_NDCG@5:0.4469 | best_NDCG@10:0.4893 | best_NDCG@20:0.5416 | best_recall@5:0.6411 | best_recall@10:0.7474 | best_recall@20:0.7699 | best_precision@5:0.7693 | best_precision@10:0.4484 | best_precision@20:0.2310 | 
epoch: 90, training loss: 93.76630969767575, train time: 378.09413957595825
epoch: 91, training loss: 92.37191815926053, train time: 379.52334117889404
epoch: 92, training loss: 94.24757606898493, train time: 380.44796419143677
epoch: 93, training loss: 93.87181928392238, train time: 378.94542694091797
epoch: 94, training loss: 93.22137069884047, train time: 379.0304419994354
epoch: 95, training loss: 94.81342351603962, train time: 378.98833894729614
epoch: 96, training loss: 93.86284820293076, train time: 380.77908635139465
epoch: 97, training loss: 93.55875372538867, train time: 379.5469768047333
Traceback (most recent call last):
  File "recommendation_model2.py", line 620, in <module>
    shuffle=False,  #
  File "recommendation_model2.py", line 221, in rec_net
    running_loss = 0.0
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 352, in __iter__
    return self._get_iterator()
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 294, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 801, in __init__
    w.start()
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/popen_fork.py", line 70, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
