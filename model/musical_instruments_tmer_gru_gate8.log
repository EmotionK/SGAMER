nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.814697265625e-06
user  100 time:  272.74305295944214
user  200 time:  610.8984732627869
user  300 time:  940.1332075595856
user  400 time:  1268.81773853302
user  500 time:  1598.7337436676025
user  600 time:  1914.3079392910004
user  700 time:  2223.3067393302917
user  800 time:  2533.551504135132
user  900 time:  2848.776303768158
user  1000 time:  3162.6633183956146
user  1100 time:  3473.386708498001
user  1200 time:  3787.2921917438507
user  1300 time:  4100.335945606232
user  1400 time:  4414.849534273148
start training item-item instance self attention module...
user  0 time:  6.67572021484375e-06
user  100 time:  276.57823276519775
user  200 time:  552.0905826091766
user  300 time:  833.5301804542542
user  400 time:  1118.664624452591
user  500 time:  1397.4465746879578
user  600 time:  1657.0427627563477
user  700 time:  1943.8013741970062
user  800 time:  2209.0809240341187
user  900 time:  2482.294592857361
user  1000 time:  2737.947292804718
user  1100 time:  3007.2004215717316
user  1200 time:  3286.551793575287
user  1300 time:  3574.050260782242
user  1400 time:  3852.800804376602
start updating user and item embedding...
user_name:1450
user  0 time:  1.9550323486328125e-05
user  100 time:  93.99884152412415
user  200 time:  188.09248518943787
user  300 time:  282.44921493530273
user  400 time:  376.3158838748932
user  500 time:  470.9831326007843
user  600 time:  564.0702652931213
user  700 time:  657.8420321941376
user  800 time:  751.1599409580231
user  900 time:  844.947835445404
user  1000 time:  938.1010444164276
user  1100 time:  1031.5064001083374
user  1200 time:  1125.008568763733
user  1300 time:  1218.533506155014
user  1400 time:  1312.4922082424164
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 150.41985154611757, train time: 60.3325572013855
epoch: 1, training loss: 88.34828245735844, train time: 59.889567613601685
epoch: 2, training loss: 74.29943352221744, train time: 60.343546628952026
epoch: 3, training loss: 67.49643108165765, train time: 60.36941695213318
epoch: 4, training loss: 61.44198800630693, train time: 60.59529638290405
epoch: 5, training loss: 58.00990377517883, train time: 60.719003200531006
epoch: 6, training loss: 54.339058939003735, train time: 61.17114281654358
epoch: 7, training loss: 51.147291439643595, train time: 61.313395738601685
epoch: 8, training loss: 48.75513691636297, train time: 60.26610064506531
epoch: 9, training loss: 46.79836855347821, train time: 60.308587312698364
epoch: 10, training loss: 44.50050804172497, train time: 59.96216058731079
epoch: 11, training loss: 42.8688319303692, train time: 60.388787508010864
epoch: 12, training loss: 40.485555053601274, train time: 59.907658100128174
epoch: 13, training loss: 39.58660073532519, train time: 59.638036012649536
epoch: 14, training loss: 36.914985202416574, train time: 59.983341693878174
epoch: 15, training loss: 35.343976948352065, train time: 60.19787335395813
epoch: 16, training loss: 34.27881204860387, train time: 59.713932037353516
epoch: 17, training loss: 32.74807808313926, train time: 60.187161684036255
epoch: 18, training loss: 31.073194254193368, train time: 60.30138421058655
epoch: 19, training loss: 29.466764254026202, train time: 59.985825300216675
epoch: 20, training loss: 28.25775384101871, train time: 60.01033902168274
epoch: 21, training loss: 26.38671088902811, train time: 60.041224241256714
epoch: 22, training loss: 26.060530621136422, train time: 60.054409980773926
epoch: 23, training loss: 24.26698759604369, train time: 60.347219467163086
epoch: 24, training loss: 23.269107151249045, train time: 60.384527921676636
epoch: 25, training loss: 22.678460174338397, train time: 60.14761710166931
epoch: 26, training loss: 21.196693029889502, train time: 60.170782804489136
epoch: 27, training loss: 21.360100695002984, train time: 60.18430757522583
epoch: 28, training loss: 20.450569178326987, train time: 60.084625005722046
epoch: 29, training loss: 19.56029281872725, train time: 60.1095073223114
epo:29 | HR@5:0.8113 | HR@10:0.8484 | HR@20:0.8897 | NDCG@5:0.4442 | NDCG@10:0.4847 | NDCG@20:0.5354 | recall@5:0.5679 | recall@10:0.6808 | recall@20:0.7272 | precision@5:0.6815 | precision@10:0.4085 | precision@20:0.2182 | best_HR@5:0.8113 | best_HR@10:0.8484 | best_HR@20:0.8897 | best_NDCG@5:0.4442 | best_NDCG@10:0.4847 | best_NDCG@20:0.5354 | best_recall@5:0.5679 | best_recall@10:0.6808 | best_recall@20:0.7272 | best_precision@5:0.6815 | best_precision@10:0.4085 | best_precision@20:0.2182 | 
epoch: 30, training loss: 18.759647759646214, train time: 58.321757793426514
epoch: 31, training loss: 19.395222081051543, train time: 60.34242248535156
epoch: 32, training loss: 18.722925872193628, train time: 60.30975866317749
epoch: 33, training loss: 17.71468741269382, train time: 60.324453353881836
epoch: 34, training loss: 17.064329991090744, train time: 60.2594838142395
epoch: 35, training loss: 17.088282569122384, train time: 60.567633390426636
epoch: 36, training loss: 16.83915122233202, train time: 60.039888858795166
epoch: 37, training loss: 15.287787786701756, train time: 60.29974317550659
epoch: 38, training loss: 16.50076795060886, train time: 60.26171135902405
epoch: 39, training loss: 15.341830183761658, train time: 60.076274394989014
epoch: 40, training loss: 15.210714733181703, train time: 59.804829120635986
epoch: 41, training loss: 15.026682705629355, train time: 60.23264479637146
epoch: 42, training loss: 14.407424228821128, train time: 60.221028089523315
epoch: 43, training loss: 14.102086720465877, train time: 60.03969860076904
epoch: 44, training loss: 14.171680840893941, train time: 60.1986768245697
epoch: 45, training loss: 14.11718202054817, train time: 60.08771824836731
epoch: 46, training loss: 13.681895630033068, train time: 59.66741466522217
epoch: 47, training loss: 14.194629331654141, train time: 59.67842888832092
epoch: 48, training loss: 12.95887361575501, train time: 59.55282115936279
epoch: 49, training loss: 13.790292744722592, train time: 59.39377784729004
epoch: 50, training loss: 12.68132586801903, train time: 59.208528995513916
epoch: 51, training loss: 13.083601196875861, train time: 58.4684476852417
epoch: 52, training loss: 13.518941686516655, train time: 59.13579821586609
epoch: 53, training loss: 12.380628243833712, train time: 59.35114789009094
epoch: 54, training loss: 13.103906486242522, train time: 59.19264888763428
epoch: 55, training loss: 12.406671076134444, train time: 59.15058398246765
epoch: 56, training loss: 12.886387094865881, train time: 59.17182636260986
epoch: 57, training loss: 12.371981125304501, train time: 59.0460479259491
epoch: 58, training loss: 12.447223853171295, train time: 58.69648027420044
epoch: 59, training loss: 12.847319998772491, train time: 58.708943367004395
epo:59 | HR@5:0.7718 | HR@10:0.8107 | HR@20:0.8623 | NDCG@5:0.4683 | NDCG@10:0.5062 | NDCG@20:0.5542 | recall@5:0.5482 | recall@10:0.6501 | recall@20:0.7000 | precision@5:0.6578 | precision@10:0.3901 | precision@20:0.2100 | best_HR@5:0.8113 | best_HR@10:0.8484 | best_HR@20:0.8897 | best_NDCG@5:0.4683 | best_NDCG@10:0.5062 | best_NDCG@20:0.5542 | best_recall@5:0.5679 | best_recall@10:0.6808 | best_recall@20:0.7272 | best_precision@5:0.6815 | best_precision@10:0.4085 | best_precision@20:0.2182 | 
epoch: 60, training loss: 11.737195248175283, train time: 57.87585401535034
epoch: 61, training loss: 12.059702111124125, train time: 59.179848194122314
epoch: 62, training loss: 12.287369897356712, train time: 59.27910757064819
epoch: 63, training loss: 11.93684839264256, train time: 59.06903338432312
epoch: 64, training loss: 11.227750616896628, train time: 59.33959889411926
epoch: 65, training loss: 11.73891680564634, train time: 59.21240830421448
epoch: 66, training loss: 11.368289596598288, train time: 59.58365726470947
epoch: 67, training loss: 11.636595356089629, train time: 58.918909788131714
epoch: 68, training loss: 10.436306198511375, train time: 59.447927474975586
epoch: 69, training loss: 11.954663067630236, train time: 59.60693311691284
epoch: 70, training loss: 10.432249049906318, train time: 58.9845073223114
epoch: 71, training loss: 11.54272987342199, train time: 58.8275625705719
epoch: 72, training loss: 11.132175104849125, train time: 59.447362422943115
epoch: 73, training loss: 10.525232860790766, train time: 59.22240400314331
epoch: 74, training loss: 10.612347056734507, train time: 59.200429916381836
epoch: 75, training loss: 10.301798739786989, train time: 59.451571226119995
epoch: 76, training loss: 11.400457257448352, train time: 59.416762828826904
epoch: 77, training loss: 9.99911362644616, train time: 59.6111695766449
epoch: 78, training loss: 10.149496593784875, train time: 59.53521966934204
epoch: 79, training loss: 10.924828655785404, train time: 59.113383769989014
epoch: 80, training loss: 9.955645393548139, train time: 59.11950874328613
epoch: 81, training loss: 10.472159341611132, train time: 59.3461971282959
epoch: 82, training loss: 10.158935318086037, train time: 59.01225805282593
epoch: 83, training loss: 9.599166016507752, train time: 59.35254192352295
epoch: 84, training loss: 10.822189617790627, train time: 59.19508171081543
epoch: 85, training loss: 10.515181644585937, train time: 58.875985860824585
epoch: 86, training loss: 10.46739843668172, train time: 59.273149728775024
epoch: 87, training loss: 10.090797519269984, train time: 59.48448896408081
epoch: 88, training loss: 10.88810514340264, train time: 59.48996353149414
epoch: 89, training loss: 9.895788895176679, train time: 59.61007881164551
epo:89 | HR@5:0.7615 | HR@10:0.8023 | HR@20:0.8448 | NDCG@5:0.4688 | NDCG@10:0.5072 | NDCG@20:0.5554 | recall@5:0.5452 | recall@10:0.6400 | recall@20:0.6895 | precision@5:0.6542 | precision@10:0.3840 | precision@20:0.2069 | best_HR@5:0.8113 | best_HR@10:0.8484 | best_HR@20:0.8897 | best_NDCG@5:0.4688 | best_NDCG@10:0.5072 | best_NDCG@20:0.5554 | best_recall@5:0.5679 | best_recall@10:0.6808 | best_recall@20:0.7272 | best_precision@5:0.6815 | best_precision@10:0.4085 | best_precision@20:0.2182 | 
epoch: 90, training loss: 10.01406237631602, train time: 57.91648602485657
epoch: 91, training loss: 10.113671151303379, train time: 58.84402298927307
epoch: 92, training loss: 9.68964009929482, train time: 59.07869362831116
epoch: 93, training loss: 9.963238382055636, train time: 58.953967332839966
epoch: 94, training loss: 10.251498952331303, train time: 59.30953311920166
epoch: 95, training loss: 9.476617396670804, train time: 59.110841274261475
epoch: 96, training loss: 10.13337711758129, train time: 59.48761558532715
epoch: 97, training loss: 9.536394241273456, train time: 58.998215198516846
epoch: 98, training loss: 9.502316166269907, train time: 59.28331899642944
epoch: 99, training loss: 9.920963751266072, train time: 59.79065680503845
epoch: 100, training loss: 9.79942309187561, train time: 59.29289484024048
epoch: 101, training loss: 9.665332838804233, train time: 58.940568923950195
epoch: 102, training loss: 9.789142559211541, train time: 59.34886360168457
epoch: 103, training loss: 9.265004296675016, train time: 59.18773913383484
epoch: 104, training loss: 9.055848784049772, train time: 59.303524017333984
epoch: 105, training loss: 9.248698393994005, train time: 58.96842885017395
epoch: 106, training loss: 9.830152823254139, train time: 59.305052042007446
epoch: 107, training loss: 8.57970057206603, train time: 59.21179270744324
epoch: 108, training loss: 9.68570776858246, train time: 58.894827365875244
epoch: 109, training loss: 8.446451369547276, train time: 59.00610136985779
epoch: 110, training loss: 9.132784853478995, train time: 58.712007999420166
epoch: 111, training loss: 8.196884888482032, train time: 59.020087003707886
epoch: 112, training loss: 9.71737251886941, train time: 59.21843123435974
epoch: 113, training loss: 7.8364113638346, train time: 59.16656231880188
epoch: 114, training loss: 9.341209939036162, train time: 58.97211956977844
epoch: 115, training loss: 9.440470977960274, train time: 58.81609916687012
epoch: 116, training loss: 8.92477696297533, train time: 59.21738576889038
epoch: 117, training loss: 8.031425966882466, train time: 59.61640477180481
epoch: 118, training loss: 8.713443063433317, train time: 59.32672381401062
epoch: 119, training loss: 8.568560963440405, train time: 59.11428737640381
epo:119 | HR@5:0.7501 | HR@10:0.7916 | HR@20:0.8430 | NDCG@5:0.4724 | NDCG@10:0.5096 | NDCG@20:0.5568 | recall@5:0.5366 | recall@10:0.6331 | recall@20:0.6839 | precision@5:0.6439 | precision@10:0.3799 | precision@20:0.2052 | best_HR@5:0.8113 | best_HR@10:0.8484 | best_HR@20:0.8897 | best_NDCG@5:0.4724 | best_NDCG@10:0.5096 | best_NDCG@20:0.5568 | best_recall@5:0.5679 | best_recall@10:0.6808 | best_recall@20:0.7272 | best_precision@5:0.6815 | best_precision@10:0.4085 | best_precision@20:0.2182 | 
epoch: 120, training loss: 9.254821117817357, train time: 58.796610832214355
epoch: 121, training loss: 8.054221702548546, train time: 59.31101632118225
epoch: 122, training loss: 8.745413818624115, train time: 59.60161542892456
epoch: 123, training loss: 8.5485492551461, train time: 59.7162549495697
epoch: 124, training loss: 9.635495823643822, train time: 59.4632134437561
epoch: 125, training loss: 8.590286807257826, train time: 59.580265283584595
epoch: 126, training loss: 8.812394750821, train time: 59.393683671951294
epoch: 127, training loss: 7.807664560624687, train time: 59.715585231781006
epoch: 128, training loss: 8.01349535340188, train time: 59.4947509765625
epoch: 129, training loss: 9.000463218549811, train time: 59.78002071380615
epoch: 130, training loss: 8.939241043508531, train time: 59.096410274505615
epoch: 131, training loss: 8.469567535606785, train time: 59.65242099761963
epoch: 132, training loss: 8.807787007657168, train time: 59.86496019363403
epoch: 133, training loss: 8.444442905848916, train time: 59.27697396278381
epoch: 134, training loss: 8.980860209439811, train time: 60.03037786483765
epoch: 135, training loss: 7.661097186420335, train time: 60.010592222213745
epoch: 136, training loss: 7.605074477185553, train time: 60.138978481292725
epoch: 137, training loss: 8.698781223140486, train time: 59.75544571876526
epoch: 138, training loss: 8.842748849955342, train time: 59.908040285110474
epoch: 139, training loss: 8.162562288976744, train time: 59.75432801246643
epoch: 140, training loss: 8.726821160113388, train time: 59.51040840148926
epoch: 141, training loss: 7.927276844334301, train time: 59.97418975830078
epoch: 142, training loss: 6.969796603017471, train time: 60.129634857177734
epoch: 143, training loss: 8.869877343114922, train time: 59.42637062072754
epoch: 144, training loss: 7.895411429652711, train time: 60.04847073554993
epoch: 145, training loss: 8.6583964197753, train time: 59.511571168899536
epoch: 146, training loss: 7.7945854475611895, train time: 59.931856632232666
epoch: 147, training loss: 8.273321672388533, train time: 59.67684197425842
epoch: 148, training loss: 8.255769064515846, train time: 59.78127121925354
epoch: 149, training loss: 8.008665355280584, train time: 60.01853656768799
epo:149 | HR@5:0.7626 | HR@10:0.8001 | HR@20:0.8447 | NDCG@5:0.4687 | NDCG@10:0.5062 | NDCG@20:0.5544 | recall@5:0.5466 | recall@10:0.6424 | recall@20:0.6870 | precision@5:0.6559 | precision@10:0.3854 | precision@20:0.2061 | best_HR@5:0.8113 | best_HR@10:0.8484 | best_HR@20:0.8897 | best_NDCG@5:0.4724 | best_NDCG@10:0.5096 | best_NDCG@20:0.5568 | best_recall@5:0.5679 | best_recall@10:0.6808 | best_recall@20:0.7272 | best_precision@5:0.6815 | best_precision@10:0.4085 | best_precision@20:0.2182 | 
training finish
