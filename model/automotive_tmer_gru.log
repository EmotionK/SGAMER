nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Automotive......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([1858400])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  4.291534423828125e-06
user  100 time:  284.68086886405945
user  200 time:  607.7845239639282
user  300 time:  933.0177066326141
user  400 time:  1262.0496077537537
user  500 time:  1586.0935842990875
user  600 time:  1916.3940341472626
user  700 time:  2245.3028070926666
user  800 time:  2571.1650516986847
user  900 time:  2896.8375182151794
user  1000 time:  3220.356472015381
user  1100 time:  3548.161339044571
user  1200 time:  3872.6358375549316
user  1300 time:  4201.48232960701
user  1400 time:  4521.055144309998
user  1500 time:  4841.62818813324
user  1600 time:  5159.876985549927
user  1700 time:  5480.362962722778
user  1800 time:  5802.853791713715
user  1900 time:  6125.734796762466
user  2000 time:  6447.732693195343
user  2100 time:  6765.531046867371
user  2200 time:  7087.227397203445
user  2300 time:  7412.85583114624
user  2400 time:  7732.245179176331
user  2500 time:  8053.711626291275
user  2600 time:  8377.076848506927
user  2700 time:  8698.46395611763
user  2800 time:  9025.270918369293
user  2900 time:  9347.576242923737
user  3000 time:  9668.353889226913
user  3100 time:  10016.711739778519
user  3200 time:  10363.489170074463
user  3300 time:  10715.950261592865
user  3400 time:  11068.192774772644
user  3500 time:  11416.460373401642
user  3600 time:  11746.517304420471
user  3700 time:  12057.696193695068
user  3800 time:  12367.350810527802
user  3900 time:  12680.493937015533
user  4000 time:  12993.36651301384
user  4100 time:  13308.600697755814
user  4200 time:  13619.88492178917
user  4300 time:  13974.613505125046
user  4400 time:  14396.090567350388
user  4500 time:  14815.147637605667
start training item-item instance self attention module...
user  0 time:  1.0251998901367188e-05
user  100 time:  319.3932330608368
user  200 time:  642.5917754173279
user  300 time:  949.3240854740143
user  400 time:  1246.5759205818176
user  500 time:  1538.6254703998566
user  600 time:  1831.5343322753906
user  700 time:  2134.9673368930817
user  800 time:  2435.3003330230713
user  900 time:  2735.0402822494507
user  1000 time:  3045.91650390625
user  1100 time:  3354.525449991226
user  1200 time:  3664.306697368622
user  1300 time:  3985.2974860668182
user  1400 time:  4281.512596130371
user  1500 time:  4601.511124372482
user  1600 time:  4914.540618181229
user  1700 time:  5215.9756100177765
user  1800 time:  5529.00584769249
user  1900 time:  5787.036692380905
user  2000 time:  6029.370276451111
user  2100 time:  6248.381428718567
user  2200 time:  6469.511596441269
user  2300 time:  6689.538044214249
user  2400 time:  6919.8008897304535
user  2500 time:  7149.562515974045
user  2600 time:  7388.067286729813
user  2700 time:  7634.261162519455
user  2800 time:  7909.72453212738
user  2900 time:  8168.444545984268
user  3000 time:  8426.28877234459
user  3100 time:  8691.459734678268
user  3200 time:  8936.05213356018
user  3300 time:  9194.527714014053
user  3400 time:  9458.0852227211
user  3500 time:  9697.527983665466
user  3600 time:  9951.313292264938
user  3700 time:  10186.068725824356
user  3800 time:  10427.439372062683
user  3900 time:  10642.113613843918
user  4000 time:  10894.66298532486
user  4100 time:  11147.711098909378
user  4200 time:  11386.558109998703
user  4300 time:  11625.040328979492
user  4400 time:  11892.758932352066
user  4500 time:  12145.083505868912
start updating user and item embedding...
user_name:4600
user  0 time:  1.52587890625e-05
user  100 time:  58.0666127204895
user  200 time:  115.75327038764954
user  300 time:  173.07396984100342
user  400 time:  230.55801033973694
user  500 time:  288.1631829738617
user  600 time:  345.6938593387604
user  700 time:  403.5246841907501
user  800 time:  461.47759914398193
user  900 time:  518.4632306098938
user  1000 time:  576.5409393310547
user  1100 time:  634.5773227214813
user  1200 time:  692.0776541233063
user  1300 time:  750.0116281509399
user  1400 time:  807.2814280986786
user  1500 time:  865.3226039409637
user  1600 time:  922.8553190231323
user  1700 time:  980.4340970516205
user  1800 time:  1036.8649973869324
user  1900 time:  1092.245435476303
user  2000 time:  1144.5183556079865
user  2100 time:  1195.7215094566345
user  2200 time:  1247.5946609973907
user  2300 time:  1298.7883224487305
user  2400 time:  1350.202761888504
user  2500 time:  1401.574770450592
user  2600 time:  1453.2107410430908
user  2700 time:  1505.3865580558777
user  2800 time:  1557.2495024204254
user  2900 time:  1608.5273189544678
user  3000 time:  1659.9016921520233
user  3100 time:  1711.4273827075958
user  3200 time:  1763.1327736377716
user  3300 time:  1814.2977077960968
user  3400 time:  1866.0586166381836
user  3500 time:  1916.9740045070648
user  3600 time:  1968.828731060028
user  3700 time:  2020.7427518367767
user  3800 time:  2072.375340461731
user  3900 time:  2123.4847807884216
user  4000 time:  2175.183402776718
user  4100 time:  2226.731959104538
user  4200 time:  2278.6114468574524
user  4300 time:  2330.3869953155518
user  4400 time:  2381.7767927646637
user  4500 time:  2432.9203045368195
start training recommendation module...
recommendation_model2.py:59: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 288.11855229160574, train time: 443.80348205566406
epoch: 1, training loss: 181.84395536415104, train time: 366.77083015441895
epoch: 2, training loss: 168.51653623043967, train time: 322.27461528778076
epoch: 3, training loss: 161.45358242436487, train time: 315.57358288764954
epoch: 4, training loss: 154.55167937368242, train time: 331.0777657032013
epoch: 5, training loss: 150.65296685325302, train time: 333.4691779613495
epoch: 6, training loss: 146.84123500708665, train time: 333.071786403656
epoch: 7, training loss: 142.2276347589068, train time: 332.3131639957428
epoch: 8, training loss: 138.16501268342836, train time: 333.62691354751587
epoch: 9, training loss: 134.356057977202, train time: 333.1409194469452
epoch: 10, training loss: 131.01389854893932, train time: 333.15290212631226
epoch: 11, training loss: 128.7610149546017, train time: 334.16400504112244
epoch: 12, training loss: 126.51146789664199, train time: 332.410769701004
epoch: 13, training loss: 122.5636607580891, train time: 332.385107755661
epoch: 14, training loss: 120.80007112464227, train time: 333.53746223449707
epoch: 15, training loss: 119.63022913294844, train time: 332.12050557136536
epoch: 16, training loss: 119.16235892078839, train time: 331.80286502838135
epoch: 17, training loss: 117.88855158794468, train time: 333.0864374637604
epoch: 18, training loss: 114.56669388614682, train time: 333.04465198516846
epoch: 19, training loss: 113.67635007594072, train time: 332.88031244277954
epoch: 20, training loss: 113.72439914919232, train time: 332.8502051830292
epoch: 21, training loss: 110.59835320948332, train time: 333.2515170574188
epoch: 22, training loss: 111.61011142702773, train time: 332.6021366119385
epoch: 23, training loss: 111.32673058242653, train time: 333.3922698497772
epoch: 24, training loss: 111.27597738878103, train time: 333.45355248451233
epoch: 25, training loss: 108.30366071430763, train time: 333.60615062713623
epoch: 26, training loss: 110.1830317071217, train time: 331.48324394226074
epoch: 27, training loss: 107.22120720134262, train time: 333.423299074173
epoch: 28, training loss: 106.83718413717725, train time: 333.83992099761963
epoch: 29, training loss: 108.16613436274929, train time: 333.4154722690582
epo:29 | HR@5:0.8995 | HR@10:0.9155 | HR@20:0.9352 | NDCG@5:0.4256 | NDCG@10:0.4698 | NDCG@20:0.5241 | recall@5:0.6413 | recall@10:0.7491 | recall@20:0.7719 | precision@5:0.7696 | precision@10:0.4495 | precision@20:0.2316 | best_HR@5:0.8995 | best_HR@10:0.9155 | best_HR@20:0.9352 | best_NDCG@5:0.4256 | best_NDCG@10:0.4698 | best_NDCG@20:0.5241 | best_recall@5:0.6413 | best_recall@10:0.7491 | best_recall@20:0.7719 | best_precision@5:0.7696 | best_precision@10:0.4495 | best_precision@20:0.2316 | 
epoch: 30, training loss: 106.85283059971698, train time: 319.79527282714844
epoch: 31, training loss: 105.2683340932781, train time: 329.1014873981476
epoch: 32, training loss: 104.60722785002872, train time: 317.871253490448
epoch: 33, training loss: 104.1109890377702, train time: 304.5021188259125
epoch: 34, training loss: 103.89375645614928, train time: 317.7837178707123
epoch: 35, training loss: 103.20498521723857, train time: 319.5235061645508
epoch: 36, training loss: 104.57505912462511, train time: 330.838098526001
epoch: 37, training loss: 101.69564825927955, train time: 334.15729999542236
epoch: 38, training loss: 100.97471159663837, train time: 332.2670600414276
epoch: 39, training loss: 99.88301898079226, train time: 332.9344027042389
epoch: 40, training loss: 101.38597921000473, train time: 333.8339276313782
epoch: 41, training loss: 101.7948257944081, train time: 334.724730014801
epoch: 42, training loss: 102.38675584663724, train time: 333.28379678726196
epoch: 43, training loss: 100.24610483528522, train time: 334.01749777793884
epoch: 44, training loss: 102.09420567264169, train time: 333.97883892059326
epoch: 45, training loss: 102.18876718825777, train time: 333.17979288101196
epoch: 46, training loss: 100.65513192321669, train time: 333.94606041908264
epoch: 47, training loss: 101.33318209282152, train time: 333.29068875312805
epoch: 48, training loss: 102.4018076210341, train time: 332.5776195526123
epoch: 49, training loss: 101.77338793802483, train time: 333.6616985797882
epoch: 50, training loss: 100.46835620606726, train time: 333.25708770751953
epoch: 51, training loss: 102.7515817442254, train time: 333.06005358695984
epoch: 52, training loss: 101.96987016619096, train time: 334.3685231208801
epoch: 53, training loss: 100.64738231201773, train time: 332.1631772518158
epoch: 54, training loss: 99.35472782164288, train time: 335.58610463142395
epoch: 55, training loss: 99.02699475396366, train time: 333.8569059371948
epoch: 56, training loss: 98.11416888213716, train time: 332.32480335235596
epoch: 57, training loss: 97.9742100825315, train time: 332.5091757774353
epoch: 58, training loss: 96.73495545840706, train time: 332.3201491832733
epoch: 59, training loss: 96.42813171243324, train time: 332.8796455860138
epo:59 | HR@5:0.8805 | HR@10:0.8970 | HR@20:0.9199 | NDCG@5:0.4451 | NDCG@10:0.4872 | NDCG@20:0.5391 | recall@5:0.6336 | recall@10:0.7344 | recall@20:0.7581 | precision@5:0.7603 | precision@10:0.4407 | precision@20:0.2274 | best_HR@5:0.8995 | best_HR@10:0.9155 | best_HR@20:0.9352 | best_NDCG@5:0.4451 | best_NDCG@10:0.4872 | best_NDCG@20:0.5391 | best_recall@5:0.6413 | best_recall@10:0.7491 | best_recall@20:0.7719 | best_precision@5:0.7696 | best_precision@10:0.4495 | best_precision@20:0.2316 | 
epoch: 60, training loss: 95.59960427341139, train time: 329.2953853607178
epoch: 61, training loss: 94.30866664556379, train time: 321.4344780445099
epoch: 62, training loss: 94.84888938000222, train time: 328.04238963127136
epoch: 63, training loss: 95.1899306986088, train time: 327.60398411750793
epoch: 64, training loss: 94.0424781612819, train time: 329.00646138191223
epoch: 65, training loss: 96.18027303306008, train time: 328.6064896583557
epoch: 66, training loss: 94.53259405681456, train time: 339.30174684524536
epoch: 67, training loss: 94.24677929605969, train time: 324.5010633468628
epoch: 68, training loss: 93.32812902938895, train time: 335.6910984516144
epoch: 69, training loss: 92.76784269393829, train time: 321.49338603019714
epoch: 70, training loss: 92.14128431972495, train time: 333.97302317619324
epoch: 71, training loss: 92.08966879170475, train time: 332.3024582862854
epoch: 72, training loss: 91.72885618553119, train time: 331.83337783813477
epoch: 73, training loss: 93.54676147031569, train time: 334.0916488170624
epoch: 74, training loss: 92.78875316224003, train time: 333.7484567165375
epoch: 75, training loss: 93.36493866041565, train time: 334.1235086917877
epoch: 76, training loss: 95.22854305884539, train time: 332.57347440719604
epoch: 77, training loss: 93.96726962985667, train time: 333.37414932250977
epoch: 78, training loss: 92.96584589930717, train time: 332.01177740097046
epoch: 79, training loss: 94.38291699090041, train time: 333.530277967453
epoch: 80, training loss: 92.77291800161765, train time: 332.7085657119751
epoch: 81, training loss: 93.24323910673411, train time: 332.83314204216003
epoch: 82, training loss: 92.92730155580648, train time: 332.3496389389038
epoch: 83, training loss: 92.97746092618763, train time: 334.2233798503876
epoch: 84, training loss: 94.67172932981339, train time: 333.7984309196472
epoch: 85, training loss: 91.82564708815335, train time: 333.6429581642151
epoch: 86, training loss: 92.91374425870163, train time: 331.90774607658386
epoch: 87, training loss: 93.63824205120181, train time: 331.99598145484924
epoch: 88, training loss: 94.22648216871312, train time: 331.73082756996155
epoch: 89, training loss: 92.99348928789186, train time: 332.6871974468231
epo:89 | HR@5:0.8668 | HR@10:0.8834 | HR@20:0.9061 | NDCG@5:0.4455 | NDCG@10:0.4881 | NDCG@20:0.5406 | recall@5:0.6285 | recall@10:0.7232 | recall@20:0.7475 | precision@5:0.7542 | precision@10:0.4339 | precision@20:0.2243 | best_HR@5:0.8995 | best_HR@10:0.9155 | best_HR@20:0.9352 | best_NDCG@5:0.4455 | best_NDCG@10:0.4881 | best_NDCG@20:0.5406 | best_recall@5:0.6413 | best_recall@10:0.7491 | best_recall@20:0.7719 | best_precision@5:0.7696 | best_precision@10:0.4495 | best_precision@20:0.2316 | 
epoch: 90, training loss: 92.24095955184748, train time: 419.57342529296875
Traceback (most recent call last):
  File "recommendation_model2.py", line 621, in <module>
  File "recommendation_model2.py", line 222, in rec_net
    batch_item_emb = node_emb[batch[:, 1]].reshape((batch.shape[0], 1, embedding_size)).to(device)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 352, in __iter__
    return self._get_iterator()
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 294, in _get_iterator
    return _MultiProcessingDataLoaderIter(self)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/site-packages/torch/utils/data/dataloader.py", line 801, in __init__
    w.start()
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/process.py", line 121, in start
    self._popen = self._Popen(self)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/context.py", line 224, in _Popen
    return _default_context.get_context().Process._Popen(process_obj)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/context.py", line 277, in _Popen
    return Popen(process_obj)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/popen_fork.py", line 19, in __init__
    self._launch(process_obj)
  File "/home/ubuntu/.conda/envs/PaperModel/lib/python3.8/multiprocessing/popen_fork.py", line 70, in _launch
    self.pid = os.fork()
OSError: [Errno 12] Cannot allocate memory
