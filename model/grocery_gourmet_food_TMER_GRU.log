nohup: ignoring input
run.py device: cuda
Amazon_Grocery_Gourmet_Food......
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([2020000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.337860107421875e-06
user  100 time:  263.39307618141174
user  200 time:  534.7658932209015
user  300 time:  803.2149279117584
user  400 time:  1076.1845536231995
user  500 time:  1342.8066952228546
user  600 time:  1616.0585551261902
user  700 time:  1892.7901101112366
user  800 time:  2169.9552812576294
user  900 time:  2446.694890975952
user  1000 time:  2718.3241057395935
user  1100 time:  2990.8655610084534
user  1200 time:  3262.316240787506
user  1300 time:  3537.6998534202576
user  1400 time:  3809.4162027835846
user  1500 time:  4082.396171092987
user  1600 time:  4362.979749917984
user  1700 time:  4638.935193061829
user  1800 time:  4914.753020524979
user  1900 time:  5194.234160423279
user  2000 time:  5465.411892414093
user  2100 time:  5748.313874959946
user  2200 time:  6027.363236904144
user  2300 time:  6301.2760491371155
user  2400 time:  6581.539703607559
user  2500 time:  6861.7597732543945
user  2600 time:  7141.969080448151
user  2700 time:  7416.555804014206
user  2800 time:  7701.723732471466
user  2900 time:  7978.291206598282
user  3000 time:  8262.624845981598
user  3100 time:  8538.759318590164
user  3200 time:  8814.459821224213
user  3300 time:  9094.668927431107
user  3400 time:  9372.865912437439
user  3500 time:  9657.562846422195
user  3600 time:  9934.998703956604
user  3700 time:  10214.645589351654
user  3800 time:  10494.32010436058
user  3900 time:  10768.863349437714
user  4000 time:  11042.915749311447
user  4100 time:  11320.041295289993
user  4200 time:  11603.694204092026
user  4300 time:  11882.629581928253
user  4400 time:  12157.738333463669
user  4500 time:  12442.383476734161
user  4600 time:  12721.03588938713
user  4700 time:  12997.59991812706
user  4800 time:  13279.725216150284
user  4900 time:  13560.54017496109
start training item-item instance self attention module...
user  0 time:  5.4836273193359375e-06
user  100 time:  130.94806790351868
user  200 time:  242.1847219467163
user  300 time:  358.08031725883484
user  400 time:  475.95021200180054
user  500 time:  589.4610011577606
user  600 time:  712.5456228256226
user  700 time:  812.4142823219299
user  800 time:  923.531861782074
user  900 time:  1019.4087021350861
user  1000 time:  1136.8995096683502
user  1100 time:  1256.3350100517273
user  1200 time:  1364.8467154502869
user  1300 time:  1470.882197856903
user  1400 time:  1583.002295255661
user  1500 time:  1699.0129079818726
user  1600 time:  1809.2131659984589
user  1700 time:  1916.542922258377
user  1800 time:  2027.704538822174
user  1900 time:  2135.5106432437897
user  2000 time:  2240.0551342964172
user  2100 time:  2358.4746387004852
user  2200 time:  2458.212902069092
user  2300 time:  2570.4852764606476
user  2400 time:  2680.7562866210938
user  2500 time:  2781.308012485504
user  2600 time:  2888.5915756225586
user  2700 time:  2998.943524837494
user  2800 time:  3113.469395160675
user  2900 time:  3214.945672273636
user  3000 time:  3319.670312166214
user  3100 time:  3422.473009109497
user  3200 time:  3532.829587459564
user  3300 time:  3646.196834087372
user  3400 time:  3755.047162294388
user  3500 time:  3857.713049173355
user  3600 time:  3966.6707379817963
user  3700 time:  4069.685343503952
user  3800 time:  4182.470164775848
user  3900 time:  4288.642150402069
user  4000 time:  4396.976985692978
user  4100 time:  4505.228844642639
user  4200 time:  4607.792906284332
user  4300 time:  4721.428486824036
user  4400 time:  4823.0585696697235
user  4500 time:  4933.756416559219
user  4600 time:  5051.909326791763
user  4700 time:  5146.294832468033
user  4800 time:  5269.331506252289
user  4900 time:  5382.811025381088
start updating user and item embedding...
user_name:5000
user  0 time:  1.0013580322265625e-05
user  100 time:  16.779290437698364
user  200 time:  33.621530532836914
user  300 time:  50.497084617614746
user  400 time:  67.35816502571106
user  500 time:  83.89451599121094
user  600 time:  100.50752711296082
user  700 time:  117.10204744338989
user  800 time:  134.05692267417908
user  900 time:  150.75064635276794
user  1000 time:  167.27798008918762
user  1100 time:  183.82106709480286
user  1200 time:  200.5792202949524
user  1300 time:  217.2929425239563
user  1400 time:  234.11073756217957
user  1500 time:  250.75395011901855
user  1600 time:  267.31437730789185
user  1700 time:  283.77897119522095
user  1800 time:  300.66982078552246
user  1900 time:  317.3055272102356
user  2000 time:  333.66198539733887
user  2100 time:  350.1754627227783
user  2200 time:  366.6543803215027
user  2300 time:  383.5621964931488
user  2400 time:  400.44536232948303
user  2500 time:  416.96565318107605
user  2600 time:  433.6052014827728
user  2700 time:  450.10682106018066
user  2800 time:  466.84361577033997
user  2900 time:  483.5953447818756
user  3000 time:  500.11062479019165
user  3100 time:  516.3844571113586
user  3200 time:  532.8216545581818
user  3300 time:  549.6882195472717
user  3400 time:  566.5011630058289
user  3500 time:  583.0301291942596
user  3600 time:  599.4699161052704
user  3700 time:  615.7347333431244
user  3800 time:  632.5809857845306
user  3900 time:  649.2185688018799
user  4000 time:  665.8171441555023
user  4100 time:  682.020854473114
user  4200 time:  698.3506510257721
user  4300 time:  715.1119856834412
user  4400 time:  731.8882269859314
user  4500 time:  748.5695631504059
user  4600 time:  765.0116469860077
user  4700 time:  781.2674276828766
user  4800 time:  798.1042673587799
user  4900 time:  814.9597525596619
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 580.6699824063107, train time: 255.26060581207275
epoch: 1, training loss: 453.50903652922716, train time: 245.5992410182953
epoch: 2, training loss: 434.8898610419419, train time: 254.43012809753418
epoch: 3, training loss: 424.325289506698, train time: 254.54763793945312
epoch: 4, training loss: 416.2748636835313, train time: 254.71200370788574
epoch: 5, training loss: 411.79991509747924, train time: 254.9915750026703
epoch: 6, training loss: 405.30438721290557, train time: 254.76487064361572
epoch: 7, training loss: 400.1550152939453, train time: 255.0108916759491
epoch: 8, training loss: 395.89993910703924, train time: 255.10563206672668
epoch: 9, training loss: 391.2810435149004, train time: 254.83057975769043
epoch: 10, training loss: 387.2199738706695, train time: 254.90997910499573
epoch: 11, training loss: 384.8503371826955, train time: 254.6467912197113
epoch: 12, training loss: 381.10351912278566, train time: 254.21435165405273
epoch: 13, training loss: 379.24525067716604, train time: 254.326726436615
epoch: 14, training loss: 377.00829223092296, train time: 254.1477403640747
epoch: 15, training loss: 374.54987901623826, train time: 253.48784136772156
epoch: 16, training loss: 371.99506159723387, train time: 253.26555132865906
epoch: 17, training loss: 370.66655482692295, train time: 253.4515483379364
epoch: 18, training loss: 369.49123663204955, train time: 254.27965331077576
epoch: 19, training loss: 369.07299171428895, train time: 253.50560855865479
epoch: 20, training loss: 368.0991589812329, train time: 254.12742733955383
epoch: 21, training loss: 366.6624160669744, train time: 254.97409105300903
epoch: 22, training loss: 366.22598928640946, train time: 253.72285175323486
epoch: 23, training loss: 365.5569074629748, train time: 253.42326045036316
epoch: 24, training loss: 364.47450422574184, train time: 254.05756974220276
epoch: 25, training loss: 362.26899422489805, train time: 253.73384165763855
epoch: 26, training loss: 361.7751345493016, train time: 253.8346357345581
epoch: 27, training loss: 361.5280399400217, train time: 253.7470154762268
epoch: 28, training loss: 362.4847184808168, train time: 254.109703540802
epoch: 29, training loss: 360.40101377203246, train time: 253.84393620491028
epoch: 30, training loss: 361.0207385410031, train time: 254.14866518974304
epoch: 31, training loss: 359.47644212609157, train time: 254.00083017349243
epoch: 32, training loss: 358.91109658573987, train time: 253.62649869918823
epoch: 33, training loss: 360.09514890116407, train time: 253.82358074188232
epoch: 34, training loss: 357.1879859676992, train time: 253.0574345588684
epoch: 35, training loss: 356.99138255702564, train time: 253.78352880477905
epoch: 36, training loss: 356.9848703748721, train time: 253.43489623069763
epoch: 37, training loss: 355.80494893871946, train time: 253.5643448829651
epoch: 38, training loss: 354.7989816070185, train time: 253.63039255142212
epoch: 39, training loss: 354.61583191226237, train time: 253.51521587371826
epoch: 40, training loss: 352.4894904116227, train time: 253.10854721069336
epoch: 41, training loss: 351.6588647098106, train time: 253.5664734840393
epoch: 42, training loss: 350.0278055521776, train time: 253.97759985923767
epoch: 43, training loss: 351.69059564798954, train time: 254.1770839691162
epoch: 44, training loss: 350.2462070376496, train time: 253.61365580558777
epoch: 45, training loss: 348.9933252297924, train time: 254.03120255470276
epoch: 46, training loss: 348.40157019067556, train time: 254.57674169540405
epoch: 47, training loss: 348.01030664902646, train time: 254.4363145828247
epoch: 48, training loss: 350.500785318407, train time: 253.98619866371155
epoch: 49, training loss: 351.4696010396583, train time: 254.31124591827393
epo:49 | HR@5:0.7984 | HR@10:0.8393 | HR@20:0.8840 | NDCG@5:0.4233 | NDCG@10:0.4656 | NDCG@20:0.5183 | recall@5:0.5536 | recall@10:0.6728 | recall@20:0.7220 | precision@5:0.6643 | precision@10:0.4037 | precision@20:0.2166 | best_HR@5:0.7984 | best_HR@10:0.8393 | best_HR@20:0.8840 | best_NDCG@5:0.4233 | best_NDCG@10:0.4656 | best_NDCG@20:0.5183 | best_recall@5:0.5536 | best_recall@10:0.6728 | best_recall@20:0.7220 | best_precision@5:0.6643 | best_precision@10:0.4037 | best_precision@20:0.2166 | 
epoch: 50, training loss: 351.1445350115828, train time: 254.88121247291565
epoch: 51, training loss: 349.9167375709221, train time: 254.2845869064331
epoch: 52, training loss: 347.1478765787615, train time: 254.33405089378357
epoch: 53, training loss: 349.1519076587574, train time: 254.41674947738647
epoch: 54, training loss: 350.80499205336673, train time: 254.78717279434204
epoch: 55, training loss: 348.53429797803983, train time: 254.45580220222473
epoch: 56, training loss: 350.63172204687726, train time: 255.30868768692017
epoch: 57, training loss: 349.5720695166092, train time: 254.38939833641052
epoch: 58, training loss: 348.34249262488447, train time: 254.74111557006836
epoch: 59, training loss: 350.32634090349893, train time: 254.4347162246704
epoch: 60, training loss: 349.2967727812065, train time: 254.74288249015808
epoch: 61, training loss: 348.4151075842674, train time: 254.71496391296387
epoch: 62, training loss: 347.48565372798475, train time: 255.12604594230652
epoch: 63, training loss: 348.39244814621634, train time: 254.75753664970398
epoch: 64, training loss: 347.1083978655515, train time: 224.77009534835815
epoch: 65, training loss: 347.848332863563, train time: 203.2725212574005
epoch: 66, training loss: 347.5969465293165, train time: 203.21670508384705
epoch: 67, training loss: 348.3788397740864, train time: 202.91862297058105
epoch: 68, training loss: 347.9357215491764, train time: 203.41103959083557
epoch: 69, training loss: 349.7923575115856, train time: 203.40855240821838
epoch: 70, training loss: 351.25724752320093, train time: 204.01913475990295
epoch: 71, training loss: 352.024220852938, train time: 203.46248030662537
epoch: 72, training loss: 349.88990225063753, train time: 203.34205746650696
epoch: 73, training loss: 349.76528623577906, train time: 203.27844166755676
epoch: 74, training loss: 350.4030922430393, train time: 203.36745882034302
epoch: 75, training loss: 352.5034111991117, train time: 203.21770524978638
epoch: 76, training loss: 350.1195732981723, train time: 203.4300241470337
epoch: 77, training loss: 351.7104717157199, train time: 203.28161644935608
epoch: 78, training loss: 352.66970425850013, train time: 204.04719877243042
epoch: 79, training loss: 349.2952119198744, train time: 203.55295991897583
epoch: 80, training loss: 347.95832710532704, train time: 203.53073859214783
epoch: 81, training loss: 351.26888118256466, train time: 203.19534516334534
epoch: 82, training loss: 350.9305979422934, train time: 203.64977264404297
epoch: 83, training loss: 349.96003681074944, train time: 204.06841039657593
epoch: 84, training loss: 350.8398382886371, train time: 203.61304020881653
epoch: 85, training loss: 349.76421592812403, train time: 204.03368496894836
epoch: 86, training loss: 349.112909069343, train time: 203.7117531299591
epoch: 87, training loss: 348.4623638536723, train time: 203.73015308380127
epoch: 88, training loss: 348.1304344087839, train time: 204.83171486854553
epoch: 89, training loss: 350.3806438962056, train time: 203.91393899917603
epoch: 90, training loss: 346.72524570079986, train time: 203.90639972686768
epoch: 91, training loss: 348.2119889453752, train time: 203.8633680343628
epoch: 92, training loss: 348.55558715673396, train time: 203.83860445022583
epoch: 93, training loss: 351.3561808959639, train time: 203.90714287757874
epoch: 94, training loss: 349.6944299121096, train time: 203.58596992492676
epoch: 95, training loss: 350.7637821623648, train time: 203.34484577178955
epoch: 96, training loss: 349.4486280544661, train time: 203.64417028427124
epoch: 97, training loss: 349.49675242745434, train time: 203.77825736999512
epoch: 98, training loss: 349.11767956934636, train time: 203.44319653511047
epoch: 99, training loss: 349.0958455791697, train time: 203.60240983963013
epo:99 | HR@5:0.7677 | HR@10:0.8083 | HR@20:0.8579 | NDCG@5:0.4100 | NDCG@10:0.4528 | NDCG@20:0.5057 | recall@5:0.5419 | recall@10:0.6466 | recall@20:0.6985 | precision@5:0.6503 | precision@10:0.3879 | precision@20:0.2096 | best_HR@5:0.7984 | best_HR@10:0.8393 | best_HR@20:0.8840 | best_NDCG@5:0.4233 | best_NDCG@10:0.4656 | best_NDCG@20:0.5183 | best_recall@5:0.5536 | best_recall@10:0.6728 | best_recall@20:0.7220 | best_precision@5:0.6643 | best_precision@10:0.4037 | best_precision@20:0.2166 | 
training finish
