nohup: ignoring input
run.py device: cuda
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
/home/ubuntu/miniconda3/envs/PaperModel/lib/python3.9/site-packages/torch/cuda/__init__.py:497: UserWarning: Can't initialize NVML
  warnings.warn("Can't initialize NVML")
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.5762786865234375e-06
user  100 time:  3.460604190826416
user  200 time:  6.057662725448608
user  300 time:  10.359157085418701
user  400 time:  10.734724760055542
user  500 time:  18.91010594367981
user  600 time:  21.264176607131958
user  700 time:  22.010197401046753
user  800 time:  23.888672828674316
user  900 time:  24.723528146743774
user  1000 time:  26.226261138916016
user  1100 time:  27.74313521385193
user  1200 time:  31.304099798202515
user  1300 time:  32.850849628448486
user  1400 time:  42.341675758361816
start training item-item instance self attention module...
user  0 time:  8.58306884765625e-06
user  100 time:  84.15794014930725
user  200 time:  154.53394556045532
user  300 time:  227.65925431251526
user  400 time:  294.0760750770569
user  500 time:  375.612845659256
user  600 time:  451.1250669956207
user  700 time:  535.6083898544312
user  800 time:  609.6062235832214
user  900 time:  677.4435517787933
user  1000 time:  752.767270565033
user  1100 time:  831.3899540901184
user  1200 time:  908.8297169208527
user  1300 time:  989.2968604564667
user  1400 time:  1063.59122300148
start updating user and item embedding...
user_name:1450
user  0 time:  1.1682510375976562e-05
user  100 time:  15.179639101028442
user  200 time:  31.568536043167114
user  300 time:  47.02177548408508
user  400 time:  62.611412048339844
user  500 time:  78.52137017250061
user  600 time:  94.4222207069397
user  700 time:  110.44060182571411
user  800 time:  126.18110942840576
user  900 time:  140.29728150367737
user  1000 time:  153.84599256515503
user  1100 time:  167.6838459968567
user  1200 time:  181.46177768707275
user  1300 time:  195.2998526096344
user  1400 time:  208.88527369499207
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model1.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 311.6998443440534, train time: 22.94322681427002
epoch: 1, training loss: 211.91994294826873, train time: 22.923421621322632
epoch: 2, training loss: 188.2551911951159, train time: 22.817717790603638
epoch: 3, training loss: 178.9665420819656, train time: 22.806191444396973
epoch: 4, training loss: 171.3842771635973, train time: 22.72995662689209
epoch: 5, training loss: 165.1935439053923, train time: 23.139175415039062
epoch: 6, training loss: 160.9211627689656, train time: 22.978968858718872
epoch: 7, training loss: 156.19804663214018, train time: 22.91506600379944
epoch: 8, training loss: 152.74796297511784, train time: 22.398687601089478
epoch: 9, training loss: 149.30592172316392, train time: 23.067160606384277
epoch: 10, training loss: 144.76423081022222, train time: 23.156278371810913
epoch: 11, training loss: 143.38913024621434, train time: 22.783662796020508
epoch: 12, training loss: 140.11510938199353, train time: 23.12966513633728
epoch: 13, training loss: 138.27177768954425, train time: 22.727123975753784
epoch: 14, training loss: 136.244920175086, train time: 22.798625707626343
epoch: 15, training loss: 134.19709869337385, train time: 23.01350164413452
epoch: 16, training loss: 132.02970006811665, train time: 22.67397928237915
epoch: 17, training loss: 130.73172964506375, train time: 22.604302167892456
epoch: 18, training loss: 129.8723262095009, train time: 22.532531261444092
epoch: 19, training loss: 128.433335692127, train time: 22.993436336517334
epoch: 20, training loss: 127.93485909358424, train time: 22.829258680343628
epoch: 21, training loss: 127.0203322362213, train time: 22.494264364242554
epoch: 22, training loss: 124.38296944223111, train time: 22.707001209259033
epoch: 23, training loss: 123.52685040578945, train time: 22.430575370788574
epoch: 24, training loss: 123.21663569044904, train time: 23.178388833999634
epoch: 25, training loss: 122.15286257093248, train time: 23.14120888710022
epoch: 26, training loss: 121.55660808974062, train time: 22.76511025428772
epoch: 27, training loss: 119.4501819399884, train time: 23.069315433502197
epoch: 28, training loss: 118.88813746636151, train time: 23.054619550704956
epoch: 29, training loss: 119.30605439863575, train time: 22.963589906692505
epoch: 30, training loss: 118.12683929808554, train time: 22.906371116638184
epoch: 31, training loss: 117.50001003607758, train time: 23.156470775604248
epoch: 32, training loss: 115.74942869837105, train time: 22.69693613052368
epoch: 33, training loss: 116.32402915089915, train time: 23.00203514099121
epoch: 34, training loss: 115.668890107685, train time: 22.915672540664673
epoch: 35, training loss: 114.75322998933552, train time: 22.77115249633789
epoch: 36, training loss: 113.01499064502423, train time: 22.45198345184326
epoch: 37, training loss: 113.85114212452754, train time: 22.8600754737854
epoch: 38, training loss: 112.99794182063488, train time: 22.738551139831543
epoch: 39, training loss: 111.83245045845979, train time: 22.62194299697876
epoch: 40, training loss: 111.87395614415436, train time: 23.266702890396118
epoch: 41, training loss: 110.95105620063987, train time: 23.070762634277344
epoch: 42, training loss: 110.84515510134224, train time: 22.58367657661438
epoch: 43, training loss: 110.36953185530001, train time: 22.68830370903015
epoch: 44, training loss: 109.5132137217588, train time: 22.629859685897827
epoch: 45, training loss: 109.73364030740049, train time: 22.697794914245605
epoch: 46, training loss: 108.63344773424615, train time: 22.693521976470947
epoch: 47, training loss: 108.86461923297611, train time: 23.09493637084961
epoch: 48, training loss: 108.37106675920222, train time: 23.134597063064575
epoch: 49, training loss: 107.51473491077195, train time: 22.973123788833618
epo:49|HR@1:0.4825 | HR@5:0.8298 | HR@10:0.9137 | HR@20:0.9670 | HR@50:0.9941 | NDCG@1:0.3536 | NDCG@5:0.3970 | NDCG@10:0.4298| NDCG@20:0.4798| NDCG@50:0.5913| best_HR@1:0.4825 | best_HR@5:0.8298 | best_HR@10:0.9137 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3536 | best_NDCG@5:0.3970 | best_NDCG@10:0.4298 | best_NDCG@20:0.4798 | best_NDCG@50:0.5913 | train_time:22.97 | test_time:381.83
epoch: 50, training loss: 107.89827079385577, train time: 23.365536212921143
epoch: 51, training loss: 107.286565087692, train time: 22.87472105026245
epoch: 52, training loss: 107.6762206547719, train time: 22.829843282699585
epoch: 53, training loss: 107.22637468526227, train time: 22.95132803916931
epoch: 54, training loss: 107.69920695247129, train time: 22.767510890960693
epoch: 55, training loss: 108.02320173628686, train time: 22.417876720428467
epoch: 56, training loss: 106.48760710153147, train time: 23.16445755958557
epoch: 57, training loss: 105.60211535789131, train time: 22.91132640838623
epoch: 58, training loss: 105.45198126646574, train time: 22.81965208053589
epoch: 59, training loss: 104.38967381698603, train time: 22.66261863708496
epoch: 60, training loss: 104.04949128252338, train time: 22.923913955688477
epoch: 61, training loss: 104.10237459959171, train time: 23.071301221847534
epoch: 62, training loss: 103.47541378435562, train time: 22.864015340805054
epoch: 63, training loss: 104.37360783938493, train time: 22.918693780899048
epoch: 64, training loss: 104.88249445615656, train time: 22.801326036453247
epoch: 65, training loss: 102.94620687589486, train time: 23.09875726699829
epoch: 66, training loss: 104.36002293085039, train time: 22.447783708572388
epoch: 67, training loss: 102.51331008890702, train time: 23.13834571838379
epoch: 68, training loss: 101.8037833493072, train time: 22.51831889152527
epoch: 69, training loss: 102.59994534414727, train time: 23.025912284851074
epoch: 70, training loss: 102.90156125000794, train time: 23.082249879837036
epoch: 71, training loss: 103.0475014406693, train time: 23.030741214752197
epoch: 72, training loss: 102.27404224780912, train time: 22.55594277381897
epoch: 73, training loss: 102.1648423981751, train time: 23.003234386444092
epoch: 74, training loss: 101.8437783379195, train time: 23.02274441719055
epoch: 75, training loss: 101.55459197116579, train time: 22.933101177215576
epoch: 76, training loss: 101.42313556773297, train time: 23.133634090423584
epoch: 77, training loss: 100.7263791555888, train time: 22.751035690307617
epoch: 78, training loss: 100.17315318430701, train time: 23.11350440979004
epoch: 79, training loss: 99.95521035048296, train time: 22.776808977127075
epoch: 80, training loss: 100.87865568609413, train time: 23.12774896621704
epoch: 81, training loss: 101.0914965463162, train time: 22.871955633163452
epoch: 82, training loss: 101.28542674998607, train time: 22.81017780303955
epoch: 83, training loss: 99.46104777878281, train time: 22.82940912246704
epoch: 84, training loss: 98.69378540626349, train time: 22.17701029777527
epoch: 85, training loss: 99.79819891336228, train time: 22.843642711639404
epoch: 86, training loss: 99.23955043096794, train time: 22.619534492492676
epoch: 87, training loss: 99.5264390979246, train time: 23.155892610549927
epoch: 88, training loss: 99.65400116459205, train time: 23.15461254119873
epoch: 89, training loss: 99.0858959879697, train time: 22.699248790740967
epoch: 90, training loss: 99.41527972264157, train time: 22.67697024345398
epoch: 91, training loss: 98.90292612149642, train time: 22.90066957473755
epoch: 92, training loss: 96.44720723921637, train time: 22.655967235565186
epoch: 93, training loss: 97.9258009915502, train time: 22.615465879440308
epoch: 94, training loss: 96.91388049061061, train time: 23.079118728637695
epoch: 95, training loss: 97.17252339494007, train time: 22.992608785629272
epoch: 96, training loss: 96.59352361605488, train time: 22.06169581413269
epoch: 97, training loss: 97.77172364726721, train time: 23.011203050613403
epoch: 98, training loss: 96.9090022027085, train time: 22.98091459274292
epoch: 99, training loss: 96.83714487133693, train time: 22.695388555526733
epo:99|HR@1:0.4906 | HR@5:0.8436 | HR@10:0.9239 | HR@20:0.9646 | HR@50:0.9941 | NDCG@1:0.3563 | NDCG@5:0.3995 | NDCG@10:0.4340| NDCG@20:0.4855| NDCG@50:0.5976| best_HR@1:0.4906 | best_HR@5:0.8436 | best_HR@10:0.9239 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4340 | best_NDCG@20:0.4855 | best_NDCG@50:0.5976 | train_time:22.70 | test_time:380.76
epoch: 100, training loss: 96.50222668887363, train time: 22.91246271133423
epoch: 101, training loss: 97.00357833589806, train time: 22.590306282043457
epoch: 102, training loss: 96.41587855796388, train time: 23.21754789352417
epoch: 103, training loss: 96.61321091940772, train time: 22.97163987159729
epoch: 104, training loss: 96.07169642679219, train time: 22.767783164978027
epoch: 105, training loss: 95.9196349222766, train time: 22.629574298858643
epoch: 106, training loss: 95.61501640698407, train time: 22.929889917373657
epoch: 107, training loss: 95.23921116580459, train time: 22.247672080993652
epoch: 108, training loss: 94.85331004641193, train time: 22.5604887008667
epoch: 109, training loss: 95.2278497346233, train time: 23.291337728500366
epoch: 110, training loss: 95.0990959957926, train time: 23.00828528404236
epoch: 111, training loss: 94.98573023372956, train time: 22.70950961112976
epoch: 112, training loss: 94.91122571928281, train time: 22.806986093521118
epoch: 113, training loss: 93.79950213182747, train time: 22.669654846191406
epoch: 114, training loss: 93.70217695104657, train time: 22.3884756565094
epoch: 115, training loss: 94.45902551959443, train time: 22.736371755599976
epoch: 116, training loss: 94.2541851110509, train time: 23.248973608016968
epoch: 117, training loss: 95.79783787605265, train time: 23.09128499031067
epoch: 118, training loss: 93.41169151810755, train time: 22.943543672561646
epoch: 119, training loss: 95.00739033649734, train time: 22.862064123153687
epoch: 120, training loss: 93.96086846087564, train time: 23.212496280670166
epoch: 121, training loss: 94.10656938128523, train time: 22.736618995666504
epoch: 122, training loss: 93.61252364857137, train time: 22.767135620117188
epoch: 123, training loss: 93.21263968380663, train time: 22.980799436569214
epoch: 124, training loss: 94.13368347512733, train time: 23.017281532287598
epoch: 125, training loss: 94.22928674219293, train time: 22.597740173339844
epoch: 126, training loss: 94.35649384203862, train time: 22.85074543952942
epoch: 127, training loss: 93.52297659612304, train time: 22.685095071792603
epoch: 128, training loss: 93.18839528344688, train time: 23.292503356933594
epoch: 129, training loss: 93.2952643755707, train time: 23.24079465866089
epoch: 130, training loss: 93.62936743727914, train time: 22.64563751220703
epoch: 131, training loss: 93.34080148245266, train time: 23.05877995491028
epoch: 132, training loss: 93.59063397202408, train time: 22.78952670097351
epoch: 133, training loss: 93.97687983487413, train time: 22.886202335357666
epoch: 134, training loss: 94.25619703440316, train time: 23.150368452072144
epoch: 135, training loss: 93.9425240366254, train time: 22.785436630249023
epoch: 136, training loss: 93.8585664432685, train time: 22.80223035812378
epoch: 137, training loss: 92.7065374542799, train time: 22.98300290107727
epoch: 138, training loss: 92.38038226596836, train time: 22.287164449691772
epoch: 139, training loss: 92.55193142935968, train time: 22.85688614845276
epoch: 140, training loss: 92.81142722552613, train time: 23.213926792144775
epoch: 141, training loss: 92.58958024028107, train time: 22.790918827056885
epoch: 142, training loss: 92.96148962841835, train time: 22.873605012893677
epoch: 143, training loss: 92.29921973997261, train time: 22.41869616508484
epoch: 144, training loss: 91.51170172714774, train time: 22.695777654647827
epoch: 145, training loss: 91.9019841086847, train time: 22.85927963256836
epoch: 146, training loss: 92.76301375623734, train time: 22.931259393692017
epoch: 147, training loss: 92.78422850256902, train time: 23.421226263046265
epoch: 148, training loss: 93.20741543584154, train time: 23.01110529899597
epoch: 149, training loss: 92.01091573312442, train time: 22.44915795326233
epo:149|HR@1:0.5095 | HR@5:0.8433 | HR@10:0.9233 | HR@20:0.9647 | HR@50:0.9936 | NDCG@1:0.3509 | NDCG@5:0.3970 | NDCG@10:0.4324| NDCG@20:0.4849| NDCG@50:0.5968| best_HR@1:0.5095 | best_HR@5:0.8436 | best_HR@10:0.9239 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4340 | best_NDCG@20:0.4855 | best_NDCG@50:0.5976 | train_time:22.45 | test_time:383.26
epoch: 150, training loss: 90.50249678130785, train time: 23.33272433280945
epoch: 151, training loss: 92.35771711764028, train time: 22.538723945617676
epoch: 152, training loss: 91.25445818232402, train time: 22.917174100875854
epoch: 153, training loss: 90.98265599009028, train time: 22.817888975143433
epoch: 154, training loss: 91.49839885759502, train time: 22.25860619544983
epoch: 155, training loss: 92.12338227333748, train time: 22.70894193649292
epoch: 156, training loss: 91.53652172634611, train time: 23.16129755973816
epoch: 157, training loss: 90.81252715125447, train time: 22.735902309417725
epoch: 158, training loss: 91.62165898588864, train time: 23.07507824897766
epoch: 159, training loss: 91.49165609717602, train time: 22.6715726852417
epoch: 160, training loss: 91.1222993243573, train time: 22.87931990623474
epoch: 161, training loss: 90.97039878076976, train time: 23.04598903656006
epoch: 162, training loss: 90.46826229495491, train time: 22.9628803730011
epoch: 163, training loss: 90.63349606819247, train time: 22.93310260772705
epoch: 164, training loss: 90.17309547928744, train time: 23.216172695159912
epoch: 165, training loss: 90.2480713607838, train time: 22.959528923034668
epoch: 166, training loss: 91.20348230662785, train time: 23.068599462509155
epoch: 167, training loss: 91.76782050387556, train time: 22.689820051193237
epoch: 168, training loss: 90.16870970873788, train time: 22.590473175048828
epoch: 169, training loss: 90.66913718810974, train time: 22.833653211593628
epoch: 170, training loss: 90.61035489757342, train time: 22.894676208496094
epoch: 171, training loss: 91.30308885773411, train time: 22.925579071044922
epoch: 172, training loss: 90.75477227671945, train time: 22.679147958755493
epoch: 173, training loss: 93.03418164683535, train time: 22.655411958694458
epoch: 174, training loss: 92.90232673401624, train time: 22.84980010986328
epoch: 175, training loss: 91.39311738377, train time: 23.061097145080566
epoch: 176, training loss: 91.26888262142893, train time: 23.024157524108887
epoch: 177, training loss: 90.52088894870394, train time: 22.788006067276
epoch: 178, training loss: 89.56412096362328, train time: 23.101604223251343
epoch: 179, training loss: 91.41152534441062, train time: 22.830255031585693
epoch: 180, training loss: 89.36331813879224, train time: 22.577362537384033
epoch: 181, training loss: 91.38781234824273, train time: 23.14776372909546
epoch: 182, training loss: 89.73504466447775, train time: 23.39834475517273
epoch: 183, training loss: 89.36534983279307, train time: 22.64959216117859
epoch: 184, training loss: 90.08532582152839, train time: 22.46335005760193
epoch: 185, training loss: 90.32762362019275, train time: 22.854823112487793
epoch: 186, training loss: 89.99053154726062, train time: 22.635443449020386
epoch: 187, training loss: 90.15421541550313, train time: 22.793675899505615
epoch: 188, training loss: 90.01443574298901, train time: 23.19547939300537
epoch: 189, training loss: 91.18096259669255, train time: 22.351255178451538
epoch: 190, training loss: 91.05847747230655, train time: 22.83951687812805
epoch: 191, training loss: 90.3994732493702, train time: 22.693888425827026
epoch: 192, training loss: 90.02291822790721, train time: 22.85822558403015
epoch: 193, training loss: 89.97544828839455, train time: 22.497634887695312
epoch: 194, training loss: 89.28439851505573, train time: 22.810802698135376
epoch: 195, training loss: 89.85243030176207, train time: 22.60920286178589
epoch: 196, training loss: 89.5629245768032, train time: 22.996971368789673
epoch: 197, training loss: 90.19350572138501, train time: 22.441550254821777
epoch: 198, training loss: 88.70620940437948, train time: 23.100467443466187
epoch: 199, training loss: 88.79914144580835, train time: 22.822587251663208
epo:199|HR@1:0.5076 | HR@5:0.8346 | HR@10:0.9128 | HR@20:0.9625 | HR@50:0.9920 | NDCG@1:0.3473 | NDCG@5:0.3994 | NDCG@10:0.4360| NDCG@20:0.4888| NDCG@50:0.6000| best_HR@1:0.5095 | best_HR@5:0.8436 | best_HR@10:0.9239 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:22.82 | test_time:383.86
epoch: 200, training loss: 88.99731914257427, train time: 22.935672283172607
epoch: 201, training loss: 87.88439983499848, train time: 22.40111541748047
epoch: 202, training loss: 88.32337335915145, train time: 22.940391063690186
epoch: 203, training loss: 87.98537579797267, train time: 22.565083980560303
epoch: 204, training loss: 89.94746223750553, train time: 22.653608322143555
epoch: 205, training loss: 88.90730811488902, train time: 23.500638008117676
epoch: 206, training loss: 89.9202528754613, train time: 22.578679084777832
epoch: 207, training loss: 89.459282183896, train time: 22.935356616973877
epoch: 208, training loss: 89.0052044693075, train time: 22.87907910346985
epoch: 209, training loss: 87.47372680221815, train time: 22.054675340652466
epoch: 210, training loss: 87.92504396747609, train time: 22.878058433532715
epoch: 211, training loss: 88.80370823523845, train time: 23.281367301940918
epoch: 212, training loss: 88.81107639916445, train time: 22.955259799957275
epoch: 213, training loss: 90.3644916376652, train time: 22.981528282165527
epoch: 214, training loss: 89.4572794922824, train time: 22.936604499816895
epoch: 215, training loss: 89.11855577242386, train time: 22.835577249526978
epoch: 216, training loss: 89.42269778264017, train time: 22.887892246246338
epoch: 217, training loss: 89.12199234170839, train time: 22.48240876197815
epoch: 218, training loss: 89.36962082991522, train time: 22.638913869857788
epoch: 219, training loss: 88.85146123800587, train time: 23.27632784843445
epoch: 220, training loss: 89.532304794222, train time: 22.781078100204468
epoch: 221, training loss: 89.12175304376797, train time: 22.894967794418335
epoch: 222, training loss: 89.70703336509177, train time: 22.925595998764038
epoch: 223, training loss: 87.504105209322, train time: 23.015939474105835
epoch: 224, training loss: 87.51640606451838, train time: 22.73888611793518
epoch: 225, training loss: 88.86923359033244, train time: 22.88497567176819
epoch: 226, training loss: 88.98356523688562, train time: 22.709238290786743
epoch: 227, training loss: 88.53176278121828, train time: 22.836845874786377
epoch: 228, training loss: 88.49274792337019, train time: 22.652955055236816
epoch: 229, training loss: 88.84187454901985, train time: 22.80063486099243
epoch: 230, training loss: 87.30149564673229, train time: 22.86686134338379
epoch: 231, training loss: 86.77935498351508, train time: 23.012845993041992
epoch: 232, training loss: 87.22282323042964, train time: 22.870831966400146
epoch: 233, training loss: 87.88890999494834, train time: 22.81121063232422
epoch: 234, training loss: 88.7312995563334, train time: 23.367960929870605
epoch: 235, training loss: 87.05440807762716, train time: 22.998062133789062
epoch: 236, training loss: 88.74291627921048, train time: 22.917402267456055
epoch: 237, training loss: 88.43778971994107, train time: 22.94651770591736
epoch: 238, training loss: 88.90439501722358, train time: 22.849176168441772
epoch: 239, training loss: 87.61756966071334, train time: 23.10770010948181
epoch: 240, training loss: 88.12755088843187, train time: 22.8986177444458
epoch: 241, training loss: 87.44341247929333, train time: 22.91689896583557
epoch: 242, training loss: 87.91716684419225, train time: 23.41420841217041
epoch: 243, training loss: 88.7421172486429, train time: 22.734911680221558
epoch: 244, training loss: 88.28406754917887, train time: 22.673927068710327
epoch: 245, training loss: 88.21234657619789, train time: 22.967331409454346
epoch: 246, training loss: 88.4933338642004, train time: 23.08790612220764
epoch: 247, training loss: 89.03590595069181, train time: 23.10478448867798
epoch: 248, training loss: 89.30571334827982, train time: 22.633792400360107
epoch: 249, training loss: 88.9926403342688, train time: 23.209741830825806
epo:249|HR@1:0.5144 | HR@5:0.8416 | HR@10:0.9182 | HR@20:0.9651 | HR@50:0.9940 | NDCG@1:0.3458 | NDCG@5:0.3938 | NDCG@10:0.4303| NDCG@20:0.4832| NDCG@50:0.5958| best_HR@1:0.5144 | best_HR@5:0.8436 | best_HR@10:0.9239 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:23.21 | test_time:378.12
epoch: 250, training loss: 88.93753456892591, train time: 22.960374116897583
epoch: 251, training loss: 88.89239161160367, train time: 22.89676332473755
epoch: 252, training loss: 88.23210371342793, train time: 23.062944889068604
epoch: 253, training loss: 89.03860427729887, train time: 22.98853826522827
epoch: 254, training loss: 88.26970929110212, train time: 23.178964376449585
epoch: 255, training loss: 87.21226893267885, train time: 23.089334726333618
epoch: 256, training loss: 88.53215668061057, train time: 22.450345277786255
epoch: 257, training loss: 88.23201716905896, train time: 22.92207956314087
epoch: 258, training loss: 88.1458235802711, train time: 22.456802129745483
epoch: 259, training loss: 88.14319461930427, train time: 22.780277729034424
epoch: 260, training loss: 87.14295860061975, train time: 22.879968881607056
epoch: 261, training loss: 88.01694700097141, train time: 22.902132987976074
epoch: 262, training loss: 87.79427934932392, train time: 22.515467166900635
epoch: 263, training loss: 87.59729197049819, train time: 23.055488109588623
epoch: 264, training loss: 87.8503403468203, train time: 22.885091304779053
epoch: 265, training loss: 87.88240613311427, train time: 23.134521007537842
epoch: 266, training loss: 86.30448536084441, train time: 23.501866340637207
epoch: 267, training loss: 88.37385576225279, train time: 23.099565744400024
epoch: 268, training loss: 87.83133994936361, train time: 22.78365468978882
epoch: 269, training loss: 87.29390443708326, train time: 22.99748706817627
epoch: 270, training loss: 86.9082087208335, train time: 23.099286794662476
epoch: 271, training loss: 85.95912308589322, train time: 23.15334963798523
epoch: 272, training loss: 86.47680069840499, train time: 23.228137969970703
epoch: 273, training loss: 86.18947340878003, train time: 22.343148708343506
epoch: 274, training loss: 85.98795523621084, train time: 23.005378484725952
epoch: 275, training loss: 86.34341359420796, train time: 22.941489458084106
epoch: 276, training loss: 86.94098921184195, train time: 22.884929180145264
epoch: 277, training loss: 87.90742092667642, train time: 22.336220741271973
epoch: 278, training loss: 87.51141489556176, train time: 23.072904109954834
epoch: 279, training loss: 87.52031314432315, train time: 23.09641718864441
epoch: 280, training loss: 87.2258818942064, train time: 22.735081911087036
epoch: 281, training loss: 86.36758904264207, train time: 23.033069372177124
epoch: 282, training loss: 87.81897453530291, train time: 22.68990182876587
epoch: 283, training loss: 87.16030335451433, train time: 23.104822158813477
epoch: 284, training loss: 88.02684939398387, train time: 22.81271505355835
epoch: 285, training loss: 86.39181234709395, train time: 23.2367045879364
epoch: 286, training loss: 86.1032760096823, train time: 22.825714111328125
epoch: 287, training loss: 86.70084917878921, train time: 23.250071048736572
epoch: 288, training loss: 86.41990701633767, train time: 23.285579442977905
epoch: 289, training loss: 87.88866424329353, train time: 22.47596049308777
epoch: 290, training loss: 87.60037922960692, train time: 22.86788296699524
epoch: 291, training loss: 87.78786961267542, train time: 23.119664192199707
epoch: 292, training loss: 87.34253826124768, train time: 23.303123712539673
epoch: 293, training loss: 86.51382817139165, train time: 22.54563593864441
epoch: 294, training loss: 87.37344972784194, train time: 23.04443359375
epoch: 295, training loss: 86.67034186260571, train time: 22.552483797073364
epoch: 296, training loss: 87.64101091169141, train time: 22.9853835105896
epoch: 297, training loss: 87.76359659725495, train time: 22.67722797393799
epoch: 298, training loss: 87.43344590959896, train time: 22.932854890823364
epoch: 299, training loss: 87.37980064881776, train time: 22.887399911880493
epo:299|HR@1:0.5118 | HR@5:0.8484 | HR@10:0.9205 | HR@20:0.9629 | HR@50:0.9928 | NDCG@1:0.3375 | NDCG@5:0.3826 | NDCG@10:0.4188| NDCG@20:0.4726| NDCG@50:0.5870| best_HR@1:0.5144 | best_HR@5:0.8484 | best_HR@10:0.9239 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:22.89 | test_time:382.24
epoch: 300, training loss: 87.89677785888489, train time: 22.999157905578613
epoch: 301, training loss: 87.9133474855771, train time: 22.993433713912964
epoch: 302, training loss: 87.47151368013874, train time: 22.797305583953857
epoch: 303, training loss: 87.2793649224659, train time: 23.101155042648315
epoch: 304, training loss: 88.22982468645932, train time: 23.094676733016968
epoch: 305, training loss: 87.40972716773831, train time: 22.932445287704468
epoch: 306, training loss: 87.24154365264985, train time: 22.857513427734375
epoch: 307, training loss: 87.71683287252381, train time: 22.573350429534912
epoch: 308, training loss: 87.62059544927615, train time: 22.965885639190674
epoch: 309, training loss: 87.86366362725312, train time: 23.118611574172974
epoch: 310, training loss: 87.81603577054193, train time: 22.837244272232056
epoch: 311, training loss: 88.28066313755699, train time: 22.65931463241577
epoch: 312, training loss: 87.59162313003617, train time: 22.42069673538208
epoch: 313, training loss: 87.79080025779695, train time: 22.98368263244629
epoch: 314, training loss: 87.63766310883875, train time: 22.480764150619507
epoch: 315, training loss: 88.85202034719259, train time: 22.81434178352356
epoch: 316, training loss: 88.33898455167946, train time: 23.02781391143799
epoch: 317, training loss: 86.74552000049516, train time: 22.70307731628418
epoch: 318, training loss: 86.75829897564108, train time: 22.851324796676636
epoch: 319, training loss: 86.87437145780859, train time: 22.52916979789734
epoch: 320, training loss: 87.23868138355465, train time: 22.941833972930908
epoch: 321, training loss: 87.52291749880897, train time: 23.13282585144043
epoch: 322, training loss: 87.69559044203925, train time: 22.566977977752686
epoch: 323, training loss: 87.56041231777272, train time: 22.388056993484497
epoch: 324, training loss: 88.59706642661331, train time: 22.96157217025757
epoch: 325, training loss: 87.71503455735728, train time: 22.201467752456665
epoch: 326, training loss: 85.79584204957428, train time: 23.201488971710205
epoch: 327, training loss: 86.133981589639, train time: 22.749695301055908
epoch: 328, training loss: 86.03602753677114, train time: 22.902356386184692
epoch: 329, training loss: 86.22961659792054, train time: 22.317442893981934
epoch: 330, training loss: 86.11834877676483, train time: 22.896252870559692
epoch: 331, training loss: 87.16704952408327, train time: 22.700246572494507
epoch: 332, training loss: 87.39924807563511, train time: 22.949649572372437
epoch: 333, training loss: 86.55815679853913, train time: 23.03985285758972
epoch: 334, training loss: 85.95115383864322, train time: 23.20691418647766
epoch: 335, training loss: 86.1876158164232, train time: 23.355870246887207
epoch: 336, training loss: 86.56377601556596, train time: 22.97094702720642
epoch: 337, training loss: 86.13024946293444, train time: 23.110742330551147
epoch: 338, training loss: 86.48781896655782, train time: 22.453612327575684
epoch: 339, training loss: 87.81166941118136, train time: 23.252981901168823
epoch: 340, training loss: 87.87028518696025, train time: 22.776668548583984
epoch: 341, training loss: 86.81234820592363, train time: 22.925464868545532
epoch: 342, training loss: 86.92034258147578, train time: 22.949204444885254
epoch: 343, training loss: 86.66060423798626, train time: 22.926305055618286
epoch: 344, training loss: 86.6591620655854, train time: 22.772279024124146
epoch: 345, training loss: 86.66880787222908, train time: 23.01258897781372
epoch: 346, training loss: 86.28274121513459, train time: 23.15427589416504
epoch: 347, training loss: 86.3516407444913, train time: 22.898045539855957
epoch: 348, training loss: 86.6411451445274, train time: 23.07634997367859
epoch: 349, training loss: 87.52194839900767, train time: 22.98204255104065
epo:349|HR@1:0.5151 | HR@5:0.8487 | HR@10:0.9214 | HR@20:0.9648 | HR@50:0.9924 | NDCG@1:0.3248 | NDCG@5:0.3719 | NDCG@10:0.4091| NDCG@20:0.4640| NDCG@50:0.5798| best_HR@1:0.5151 | best_HR@5:0.8487 | best_HR@10:0.9239 | best_HR@20:0.9670 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:22.98 | test_time:378.20
epoch: 350, training loss: 86.9761760575966, train time: 22.96639633178711
epoch: 351, training loss: 86.42829110348248, train time: 23.032279014587402
epoch: 352, training loss: 85.55061238605776, train time: 23.224098443984985
epoch: 353, training loss: 86.40771130838039, train time: 22.414518117904663
epoch: 354, training loss: 85.30235525730859, train time: 22.812500715255737
epoch: 355, training loss: 85.65174582487816, train time: 22.73660635948181
epoch: 356, training loss: 86.02519304745147, train time: 22.37531805038452
epoch: 357, training loss: 85.29569487697154, train time: 22.783209800720215
epoch: 358, training loss: 86.1378477156868, train time: 23.198968410491943
epoch: 359, training loss: 86.18044778762851, train time: 23.1231951713562
epoch: 360, training loss: 85.10646117025681, train time: 23.0661563873291
epoch: 361, training loss: 85.79310418151363, train time: 22.275323629379272
epoch: 362, training loss: 85.95692385426446, train time: 22.755595207214355
epoch: 363, training loss: 86.4068003598295, train time: 22.82218360900879
epoch: 364, training loss: 86.97013883908949, train time: 22.93562412261963
epoch: 365, training loss: 86.40315015894885, train time: 22.929337739944458
epoch: 366, training loss: 85.60973308862958, train time: 22.522967100143433
epoch: 367, training loss: 86.86383890554134, train time: 22.731303691864014
epoch: 368, training loss: 85.59902513269662, train time: 22.857208490371704
epoch: 369, training loss: 86.23795753777995, train time: 22.951735496520996
epoch: 370, training loss: 85.44673223309655, train time: 22.497235536575317
epoch: 371, training loss: 86.60085403853736, train time: 22.866854667663574
epoch: 372, training loss: 86.02524728879871, train time: 22.97168278694153
epoch: 373, training loss: 86.73266085697469, train time: 23.215715646743774
epoch: 374, training loss: 87.94082733197865, train time: 22.799644947052002
epoch: 375, training loss: 86.91004187852377, train time: 22.890665769577026
epoch: 376, training loss: 85.45522706894371, train time: 22.641592502593994
epoch: 377, training loss: 86.31392200008486, train time: 23.176107168197632
epoch: 378, training loss: 86.63907075604584, train time: 22.75727415084839
epoch: 379, training loss: 85.15590127688483, train time: 22.718542337417603
epoch: 380, training loss: 85.40925156753292, train time: 22.80072546005249
epoch: 381, training loss: 86.55664334341782, train time: 22.460357189178467
epoch: 382, training loss: 86.25410668920813, train time: 22.42041802406311
epoch: 383, training loss: 86.1466769462495, train time: 22.62984013557434
epoch: 384, training loss: 85.42876271277783, train time: 22.459758281707764
epoch: 385, training loss: 86.10718414267103, train time: 22.80713176727295
epoch: 386, training loss: 84.89298724632317, train time: 22.06607937812805
epoch: 387, training loss: 84.30951336765793, train time: 22.658268451690674
epoch: 388, training loss: 85.0944857742179, train time: 22.745795726776123
epoch: 389, training loss: 86.77008136100994, train time: 22.95195198059082
epoch: 390, training loss: 84.82240390344668, train time: 22.951402187347412
epoch: 391, training loss: 85.10164256001008, train time: 22.86799144744873
epoch: 392, training loss: 85.06206643758196, train time: 22.290487051010132
epoch: 393, training loss: 85.80659609215218, train time: 22.9157497882843
epoch: 394, training loss: 85.01250205747783, train time: 22.93835711479187
epoch: 395, training loss: 85.15513324692802, train time: 22.8102867603302
epoch: 396, training loss: 85.0275259040609, train time: 22.999564170837402
epoch: 397, training loss: 86.3922944821752, train time: 22.696269035339355
epoch: 398, training loss: 85.88883739532321, train time: 23.34745168685913
epoch: 399, training loss: 85.52523887817006, train time: 23.03430485725403
epo:399|HR@1:0.5138 | HR@5:0.8590 | HR@10:0.9306 | HR@20:0.9691 | HR@50:0.9929 | NDCG@1:0.3092 | NDCG@5:0.3563 | NDCG@10:0.3945| NDCG@20:0.4514| NDCG@50:0.5700| best_HR@1:0.5151 | best_HR@5:0.8590 | best_HR@10:0.9306 | best_HR@20:0.9691 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:23.03 | test_time:384.12
epoch: 400, training loss: 85.60718425422965, train time: 23.11503767967224
epoch: 401, training loss: 85.71501317008551, train time: 22.788326501846313
epoch: 402, training loss: 85.09635520157099, train time: 22.789714574813843
epoch: 403, training loss: 84.7237522142641, train time: 23.07473349571228
epoch: 404, training loss: 84.40078406460088, train time: 22.974581956863403
epoch: 405, training loss: 84.58876675737338, train time: 22.771209716796875
epoch: 406, training loss: 85.65262385969254, train time: 22.890533924102783
epoch: 407, training loss: 85.96734579564327, train time: 22.9028582572937
epoch: 408, training loss: 85.68992613079172, train time: 22.99774670600891
epoch: 409, training loss: 85.15366377146711, train time: 23.04013681411743
epoch: 410, training loss: 85.60223043849874, train time: 23.061370849609375
epoch: 411, training loss: 85.46648995329815, train time: 22.495821475982666
epoch: 412, training loss: 84.59845119324018, train time: 22.59400248527527
epoch: 413, training loss: 85.33439134751097, train time: 23.231838941574097
epoch: 414, training loss: 85.2117244258734, train time: 23.318328619003296
epoch: 415, training loss: 85.20545691012012, train time: 22.460747718811035
epoch: 416, training loss: 85.49139774006107, train time: 22.8451669216156
epoch: 417, training loss: 85.64747584607721, train time: 22.627542734146118
epoch: 418, training loss: 84.8118407811744, train time: 22.793206691741943
epoch: 419, training loss: 85.07613413868603, train time: 23.147398471832275
epoch: 420, training loss: 84.25057256125729, train time: 23.257531881332397
epoch: 421, training loss: 83.85635766165069, train time: 22.644208431243896
epoch: 422, training loss: 85.46493587784425, train time: 22.660202264785767
epoch: 423, training loss: 84.06812133922722, train time: 23.050193548202515
epoch: 424, training loss: 85.5523048894247, train time: 22.9269061088562
epoch: 425, training loss: 87.2075743684727, train time: 23.20056653022766
epoch: 426, training loss: 86.25998317928133, train time: 22.853660821914673
epoch: 427, training loss: 85.3114254260945, train time: 23.053972005844116
epoch: 428, training loss: 86.16487101210441, train time: 22.54033088684082
epoch: 429, training loss: 85.58290896125982, train time: 22.56475019454956
epoch: 430, training loss: 85.76510854355365, train time: 23.264421463012695
epoch: 431, training loss: 84.65775432828741, train time: 22.611027717590332
epoch: 432, training loss: 85.17909312741904, train time: 22.743775367736816
epoch: 433, training loss: 85.24028613941482, train time: 22.745394945144653
epoch: 434, training loss: 85.15725561414001, train time: 23.014591217041016
epoch: 435, training loss: 84.86061901465291, train time: 23.277432918548584
epoch: 436, training loss: 85.8142099763827, train time: 22.753180265426636
epoch: 437, training loss: 84.69819837780415, train time: 22.814837217330933
epoch: 438, training loss: 85.56812132867162, train time: 22.764723777770996
epoch: 439, training loss: 84.90366690976225, train time: 22.498531341552734
epoch: 440, training loss: 85.05746703758268, train time: 23.428354263305664
epoch: 441, training loss: 84.27590335551577, train time: 22.678510665893555
epoch: 442, training loss: 85.04862427260741, train time: 22.560877561569214
epoch: 443, training loss: 85.76755151664656, train time: 23.16856598854065
epoch: 444, training loss: 85.5750918166159, train time: 22.8277804851532
epoch: 445, training loss: 85.02422466442295, train time: 22.53026533126831
epoch: 446, training loss: 85.84789185301634, train time: 22.71410822868347
epoch: 447, training loss: 85.58129871349956, train time: 22.67493724822998
epoch: 448, training loss: 85.20952767731069, train time: 22.801300764083862
epoch: 449, training loss: 84.13057085108085, train time: 22.768958568572998
epo:449|HR@1:0.5233 | HR@5:0.8493 | HR@10:0.9179 | HR@20:0.9601 | HR@50:0.9893 | NDCG@1:0.3036 | NDCG@5:0.3539 | NDCG@10:0.3935| NDCG@20:0.4509| NDCG@50:0.5695| best_HR@1:0.5233 | best_HR@5:0.8590 | best_HR@10:0.9306 | best_HR@20:0.9691 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:22.77 | test_time:381.05
epoch: 450, training loss: 85.01535001868797, train time: 22.943009614944458
epoch: 451, training loss: 85.15383280852257, train time: 22.751250743865967
epoch: 452, training loss: 85.62762795707386, train time: 23.1001615524292
epoch: 453, training loss: 86.11161455231922, train time: 23.32844829559326
epoch: 454, training loss: 85.44287741035441, train time: 22.661309480667114
epoch: 455, training loss: 85.2539961338407, train time: 22.744569063186646
epoch: 456, training loss: 85.56985208562037, train time: 22.3478741645813
epoch: 457, training loss: 86.43552317151625, train time: 22.99463438987732
epoch: 458, training loss: 85.67076301108682, train time: 22.735472440719604
epoch: 459, training loss: 85.88772528900881, train time: 22.457951307296753
epoch: 460, training loss: 84.73878073310334, train time: 22.82164239883423
epoch: 461, training loss: 85.80205269491853, train time: 22.965314149856567
epoch: 462, training loss: 86.11552888874394, train time: 22.801413774490356
epoch: 463, training loss: 86.27057089235677, train time: 23.016538858413696
epoch: 464, training loss: 85.76317526538332, train time: 22.80721354484558
epoch: 465, training loss: 86.03187635663926, train time: 22.741777658462524
epoch: 466, training loss: 86.79040848269142, train time: 22.57258892059326
epoch: 467, training loss: 86.83989236462367, train time: 22.65418291091919
epoch: 468, training loss: 87.20336401413442, train time: 22.956888914108276
epoch: 469, training loss: 85.78753503872213, train time: 23.135303258895874
epoch: 470, training loss: 85.87767313711629, train time: 22.588390827178955
epoch: 471, training loss: 86.3038562727379, train time: 22.678416967391968
epoch: 472, training loss: 86.01347751392677, train time: 22.497608423233032
epoch: 473, training loss: 86.40237585638715, train time: 22.805800199508667
epoch: 474, training loss: 87.28871159132905, train time: 22.942870378494263
epoch: 475, training loss: 85.30196531051115, train time: 22.80769443511963
epoch: 476, training loss: 86.60309943003813, train time: 22.976269721984863
epoch: 477, training loss: 85.81073440589171, train time: 23.00172185897827
epoch: 478, training loss: 86.18129438372853, train time: 23.173349142074585
epoch: 479, training loss: 85.52325906014448, train time: 22.690423011779785
epoch: 480, training loss: 85.86831040229299, train time: 22.609444856643677
epoch: 481, training loss: 86.13011431533232, train time: 22.55148720741272
epoch: 482, training loss: 87.61914784526016, train time: 22.81608533859253
epoch: 483, training loss: 86.64643503205298, train time: 22.83464765548706
epoch: 484, training loss: 87.1033261739467, train time: 22.56308627128601
epoch: 485, training loss: 86.51601522082638, train time: 22.95128297805786
epoch: 486, training loss: 86.19919926898365, train time: 22.697582244873047
epoch: 487, training loss: 85.58563044610128, train time: 23.149760246276855
epoch: 488, training loss: 86.364406210374, train time: 22.791993379592896
epoch: 489, training loss: 86.08428590138647, train time: 22.791435718536377
epoch: 490, training loss: 86.43635382231878, train time: 22.945609092712402
epoch: 491, training loss: 85.95067456690958, train time: 22.663111209869385
epoch: 492, training loss: 86.75082467958055, train time: 22.999200344085693
epoch: 493, training loss: 86.35379085022578, train time: 22.70416283607483
epoch: 494, training loss: 86.17599491743385, train time: 22.987001419067383
epoch: 495, training loss: 85.7504239009213, train time: 23.102115631103516
epoch: 496, training loss: 85.17950273092356, train time: 22.79174304008484
epoch: 497, training loss: 85.2083290912924, train time: 22.69558548927307
epoch: 498, training loss: 84.72440301673487, train time: 22.73794436454773
epoch: 499, training loss: 85.2225911811438, train time: 22.57318139076233
epo:499|HR@1:0.5145 | HR@5:0.8544 | HR@10:0.9260 | HR@20:0.9657 | HR@50:0.9911 | NDCG@1:0.2755 | NDCG@5:0.3257 | NDCG@10:0.3653| NDCG@20:0.4235| NDCG@50:0.5454| best_HR@1:0.5233 | best_HR@5:0.8590 | best_HR@10:0.9306 | best_HR@20:0.9691 | best_HR@50:0.9941 | best_NDCG@1:0.3563 | best_NDCG@5:0.3995 | best_NDCG@10:0.4360 | best_NDCG@20:0.4888 | best_NDCG@50:0.6000 | train_time:22.57 | test_time:382.85
training finish
