nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_CDs_Vinyl......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  9.5367431640625e-06
user  100 time:  300.04398107528687
user  200 time:  597.2433345317841
user  300 time:  905.3926846981049
user  400 time:  1214.3523547649384
user  500 time:  1530.284868478775
user  600 time:  1851.346072435379
user  700 time:  2166.538577079773
user  800 time:  2481.5840713977814
user  900 time:  2800.996645450592
user  1000 time:  3117.507388830185
user  1100 time:  3432.0037801265717
user  1200 time:  3745.858952999115
user  1300 time:  4057.8281650543213
user  1400 time:  4315.149293899536
user  1500 time:  4578.817582368851
user  1600 time:  4836.0923528671265
user  1700 time:  5096.331690073013
user  1800 time:  5356.612099885941
user  1900 time:  5615.221972227097
start training item-item instance self attention module...
user  0 time:  5.9604644775390625e-06
user  100 time:  46.62867069244385
user  200 time:  96.09386587142944
user  300 time:  140.84134006500244
user  400 time:  185.02881932258606
user  500 time:  227.2481553554535
user  600 time:  269.9402594566345
user  700 time:  311.3550236225128
user  800 time:  354.9575996398926
user  900 time:  404.5770583152771
user  1000 time:  449.73265528678894
user  1100 time:  493.83193922042847
user  1200 time:  535.9522812366486
user  1300 time:  578.7158787250519
user  1400 time:  618.0667905807495
user  1500 time:  663.4798691272736
user  1600 time:  704.0299737453461
user  1700 time:  749.2532007694244
user  1800 time:  795.643547296524
user  1900 time:  843.2297322750092
start updating user and item embedding...
user_name:2000
user  0 time:  7.62939453125e-06
user  100 time:  18.749956369400024
user  200 time:  37.3293673992157
user  300 time:  55.65021252632141
user  400 time:  74.21264362335205
user  500 time:  91.63482975959778
user  600 time:  111.69841599464417
user  700 time:  131.61985778808594
user  800 time:  151.38829946517944
user  900 time:  171.71957635879517
user  1000 time:  192.00980353355408
user  1100 time:  211.87932753562927
user  1200 time:  231.84209895133972
user  1300 time:  251.87323260307312
user  1400 time:  272.06387186050415
user  1500 time:  292.1848075389862
user  1600 time:  312.15046215057373
user  1700 time:  332.0985643863678
user  1800 time:  352.50299620628357
user  1900 time:  372.733829498291
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 137.58039512491086, train time: 126.02036213874817
epoch: 1, training loss: 62.40372982077679, train time: 126.16572737693787
epoch: 2, training loss: 48.49692910482918, train time: 126.46423149108887
epoch: 3, training loss: 41.79220948006514, train time: 126.08855104446411
epoch: 4, training loss: 36.39490043085607, train time: 125.83924579620361
epoch: 5, training loss: 32.51678864693895, train time: 126.35453343391418
epoch: 6, training loss: 30.430392391839632, train time: 126.4299898147583
epoch: 7, training loss: 27.246321316140893, train time: 126.12966847419739
epoch: 8, training loss: 25.22078979127582, train time: 126.4019730091095
epoch: 9, training loss: 23.9376892984792, train time: 126.42898225784302
epoch: 10, training loss: 22.317581931572022, train time: 126.19897866249084
epoch: 11, training loss: 22.679425977575193, train time: 126.6297390460968
epoch: 12, training loss: 19.97150277759465, train time: 126.4829032421112
epoch: 13, training loss: 20.00145445413, train time: 125.86288690567017
epoch: 14, training loss: 19.001243755493306, train time: 126.09852910041809
epoch: 15, training loss: 18.870835553236248, train time: 125.7676260471344
epoch: 16, training loss: 17.16271751635395, train time: 125.72217059135437
epoch: 17, training loss: 16.55526016968497, train time: 126.50786566734314
epoch: 18, training loss: 16.861033144657995, train time: 121.73328948020935
epoch: 19, training loss: 16.95586320597249, train time: 115.60710453987122
epoch: 20, training loss: 14.97186831109093, train time: 118.16729974746704
epoch: 21, training loss: 15.237742279688973, train time: 121.69547128677368
epoch: 22, training loss: 13.910419247189566, train time: 120.63192176818848
epoch: 23, training loss: 14.80108717110943, train time: 121.09168815612793
epoch: 24, training loss: 13.996436655162142, train time: 125.58656692504883
epoch: 25, training loss: 13.383907219440516, train time: 126.15424394607544
epoch: 26, training loss: 13.548705097671245, train time: 126.79726839065552
epoch: 27, training loss: 13.538660483020294, train time: 127.0796627998352
epoch: 28, training loss: 11.77407490914311, train time: 126.41515231132507
epoch: 29, training loss: 12.7583330737084, train time: 126.19499731063843
epo:29 | HR@5:0.8989 | HR@10:0.9150 | HR@20:0.9335 | NDCG@5:0.4073 | NDCG@10:0.4533 | NDCG@20:0.5088 | recall@5:0.6424 | recall@10:0.7521 | recall@20:0.7727 | precision@5:0.7709 | precision@10:0.4512 | precision@20:0.2318 | best_HR@5:0.8989 | best_HR@10:0.9150 | best_HR@20:0.9335 | best_NDCG@5:0.4073 | best_NDCG@10:0.4533 | best_NDCG@20:0.5088 | best_recall@5:0.6424 | best_recall@10:0.7521 | best_recall@20:0.7727 | best_precision@5:0.7709 | best_precision@10:0.4512 | best_precision@20:0.2318 | 
epoch: 30, training loss: 12.816853244648655, train time: 126.26579689979553
epoch: 31, training loss: 13.077634873603074, train time: 126.1357421875
epoch: 32, training loss: 11.717568434793748, train time: 126.48098969459534
epoch: 33, training loss: 12.46651231551914, train time: 130.99311709403992
epoch: 34, training loss: 11.935380510019058, train time: 128.214861869812
epoch: 35, training loss: 11.292305651477363, train time: 129.6836588382721
epoch: 36, training loss: 10.083382188207224, train time: 131.1734321117401
epoch: 37, training loss: 10.87364856907493, train time: 125.02197098731995
epoch: 38, training loss: 10.776253881976572, train time: 109.64211654663086
epoch: 39, training loss: 10.978456421941928, train time: 110.07737946510315
epoch: 40, training loss: 10.862705044658014, train time: 106.27248430252075
epoch: 41, training loss: 10.829322435754875, train time: 99.96266341209412
epoch: 42, training loss: 9.9462390135036, train time: 100.32959246635437
epoch: 43, training loss: 10.708104155960882, train time: 99.93225836753845
epoch: 44, training loss: 8.940410803342957, train time: 100.14565110206604
epoch: 45, training loss: 10.892080763138893, train time: 99.8094596862793
epoch: 46, training loss: 10.748598770782792, train time: 107.96093797683716
epoch: 47, training loss: 10.456343472800313, train time: 110.11560463905334
epoch: 48, training loss: 10.56790579067092, train time: 109.7354953289032
epoch: 49, training loss: 10.786689394642963, train time: 109.9981050491333
epoch: 50, training loss: 11.017853186352113, train time: 109.79044103622437
epoch: 51, training loss: 10.137591251115737, train time: 109.81323003768921
epoch: 52, training loss: 9.333139412914534, train time: 110.01006031036377
epoch: 53, training loss: 9.73378772297292, train time: 111.1449646949768
epoch: 54, training loss: 9.184874199957676, train time: 109.80072259902954
epoch: 55, training loss: 9.086397842465885, train time: 109.92855334281921
epoch: 56, training loss: 8.797997262206763, train time: 109.73842406272888
epoch: 57, training loss: 10.065248612933374, train time: 109.84142875671387
epoch: 58, training loss: 8.989887206296316, train time: 109.69046258926392
epoch: 59, training loss: 9.84743498178932, train time: 109.85580396652222
epo:59 | HR@5:0.8620 | HR@10:0.8762 | HR@20:0.9008 | NDCG@5:0.4390 | NDCG@10:0.4836 | NDCG@20:0.5367 | recall@5:0.6303 | recall@10:0.7187 | recall@20:0.7413 | precision@5:0.7564 | precision@10:0.4312 | precision@20:0.2224 | best_HR@5:0.8989 | best_HR@10:0.9150 | best_HR@20:0.9335 | best_NDCG@5:0.4390 | best_NDCG@10:0.4836 | best_NDCG@20:0.5367 | best_recall@5:0.6424 | best_recall@10:0.7521 | best_recall@20:0.7727 | best_precision@5:0.7709 | best_precision@10:0.4512 | best_precision@20:0.2318 | 
epoch: 60, training loss: 9.807359655292203, train time: 109.35428309440613
epoch: 61, training loss: 9.30822597981296, train time: 107.29652857780457
epoch: 62, training loss: 9.341235495509864, train time: 99.69186496734619
epoch: 63, training loss: 8.951741022403212, train time: 99.78712129592896
epoch: 64, training loss: 8.970363393060325, train time: 99.81371545791626
epoch: 65, training loss: 8.17205931473245, train time: 102.50031995773315
epoch: 66, training loss: 8.862293359581372, train time: 100.13217997550964
epoch: 67, training loss: 9.06450035568264, train time: 105.09815049171448
epoch: 68, training loss: 9.839892202591273, train time: 109.83434963226318
epoch: 69, training loss: 9.03109459614575, train time: 109.86756587028503
epoch: 70, training loss: 9.2252476675651, train time: 109.70939898490906
epoch: 71, training loss: 8.881045934667782, train time: 109.63209080696106
epoch: 72, training loss: 9.642250845741955, train time: 110.0088746547699
epoch: 73, training loss: 9.472149253622774, train time: 109.94410634040833
epoch: 74, training loss: 8.48653746449213, train time: 109.79778122901917
epoch: 75, training loss: 9.079235421532303, train time: 109.28713154792786
epoch: 76, training loss: 9.878386889734884, train time: 109.85603904724121
epoch: 77, training loss: 9.319504343680364, train time: 109.92376947402954
epoch: 78, training loss: 8.748891656919113, train time: 109.79680681228638
epoch: 79, training loss: 8.753981597723396, train time: 109.68633794784546
epoch: 80, training loss: 9.655878381007824, train time: 109.68504929542542
epoch: 81, training loss: 8.909251297848641, train time: 109.6349503993988
epoch: 82, training loss: 8.38192723303382, train time: 109.58540391921997
epoch: 83, training loss: 9.275182986302752, train time: 109.59154987335205
epoch: 84, training loss: 7.949946025310055, train time: 109.89423942565918
epoch: 85, training loss: 9.116951518851238, train time: 109.93792986869812
epoch: 86, training loss: 9.436474073856289, train time: 109.7061562538147
epoch: 87, training loss: 7.931298559745301, train time: 109.5577642917633
epoch: 88, training loss: 9.709145621380117, train time: 108.87009263038635
epoch: 89, training loss: 8.774631314124235, train time: 99.63276886940002
epo:89 | HR@5:0.8619 | HR@10:0.8783 | HR@20:0.9018 | NDCG@5:0.3615 | NDCG@10:0.4105 | NDCG@20:0.4698 | recall@5:0.6269 | recall@10:0.7197 | recall@20:0.7427 | precision@5:0.7523 | precision@10:0.4318 | precision@20:0.2228 | best_HR@5:0.8989 | best_HR@10:0.9150 | best_HR@20:0.9335 | best_NDCG@5:0.4390 | best_NDCG@10:0.4836 | best_NDCG@20:0.5367 | best_recall@5:0.6424 | best_recall@10:0.7521 | best_recall@20:0.7727 | best_precision@5:0.7709 | best_precision@10:0.4512 | best_precision@20:0.2318 | 
epoch: 90, training loss: 8.701253366758237, train time: 109.47409510612488
epoch: 91, training loss: 8.357359732820669, train time: 109.93256115913391
epoch: 92, training loss: 8.862142919306734, train time: 109.85680150985718
epoch: 93, training loss: 8.900255355636546, train time: 109.73266983032227
epoch: 94, training loss: 9.02047317783854, train time: 109.49854469299316
epoch: 95, training loss: 8.108868649769647, train time: 109.51977324485779
epoch: 96, training loss: 8.844742135628167, train time: 109.46100211143494
epoch: 97, training loss: 8.850139108648818, train time: 109.50093030929565
epoch: 98, training loss: 9.46261362802511, train time: 109.70729470252991
epoch: 99, training loss: 8.590786034857956, train time: 109.51943039894104
epoch: 100, training loss: 8.44718536446851, train time: 109.65211510658264
epoch: 101, training loss: 8.856901751747955, train time: 109.70499897003174
epoch: 102, training loss: 8.256061372006116, train time: 109.41747975349426
epoch: 103, training loss: 7.831480100691579, train time: 109.55712890625
epoch: 104, training loss: 7.223545975523166, train time: 109.92064785957336
epoch: 105, training loss: 8.183456664483401, train time: 109.50731468200684
epoch: 106, training loss: 8.4236154564951, train time: 109.83198475837708
epoch: 107, training loss: 8.479856274916983, train time: 109.77255725860596
epoch: 108, training loss: 7.251457501318441, train time: 110.15598630905151
epoch: 109, training loss: 8.656872730936357, train time: 109.3809072971344
epoch: 110, training loss: 7.617845099512351, train time: 101.37667107582092
epoch: 111, training loss: 8.407594523771763, train time: 100.39794087409973
epoch: 112, training loss: 9.446207864009466, train time: 99.81830859184265
epoch: 113, training loss: 8.18313481971245, train time: 99.60024094581604
epoch: 114, training loss: 9.026457793355348, train time: 101.63127493858337
epoch: 115, training loss: 8.533566434162822, train time: 98.934485912323
epoch: 116, training loss: 8.471557962831639, train time: 86.01143264770508
epoch: 117, training loss: 8.141805331632895, train time: 86.01181364059448
epoch: 118, training loss: 9.174783192877612, train time: 86.0035412311554
epoch: 119, training loss: 8.70657196304137, train time: 85.70333909988403
epo:119 | HR@5:0.8448 | HR@10:0.8598 | HR@20:0.8831 | NDCG@5:0.3454 | NDCG@10:0.3936 | NDCG@20:0.4528 | recall@5:0.6216 | recall@10:0.7057 | recall@20:0.7272 | precision@5:0.7459 | precision@10:0.4234 | precision@20:0.2182 | best_HR@5:0.8989 | best_HR@10:0.9150 | best_HR@20:0.9335 | best_NDCG@5:0.4390 | best_NDCG@10:0.4836 | best_NDCG@20:0.5367 | best_recall@5:0.6424 | best_recall@10:0.7521 | best_recall@20:0.7727 | best_precision@5:0.7709 | best_precision@10:0.4512 | best_precision@20:0.2318 | 
epoch: 120, training loss: 8.499557144506525, train time: 85.94429206848145
epoch: 121, training loss: 7.954640373468976, train time: 85.86146450042725
epoch: 122, training loss: 9.290293604785575, train time: 85.8119592666626
epoch: 123, training loss: 7.924929921544958, train time: 85.79977583885193
epoch: 124, training loss: 8.277581967284277, train time: 85.96244359016418
epoch: 125, training loss: 8.080560900879846, train time: 85.91798663139343
epoch: 126, training loss: 8.349188465993848, train time: 85.804696559906
epoch: 127, training loss: 8.357067962743258, train time: 86.15074396133423
epoch: 128, training loss: 7.890672040188747, train time: 85.87803244590759
epoch: 129, training loss: 8.771672068928638, train time: 85.79449367523193
epoch: 130, training loss: 9.080797478995919, train time: 85.76229977607727
epoch: 131, training loss: 10.059705811811, train time: 85.989821434021
epoch: 132, training loss: 8.687262347350497, train time: 86.22946000099182
epoch: 133, training loss: 8.708018193097587, train time: 85.80348348617554
epoch: 134, training loss: 8.51869711235895, train time: 86.03725576400757
epoch: 135, training loss: 7.156725489249311, train time: 86.02896332740784
epoch: 136, training loss: 9.34132027986334, train time: 86.0141282081604
epoch: 137, training loss: 8.826896389902231, train time: 85.43639612197876
epoch: 138, training loss: 9.027328896410836, train time: 86.11685132980347
epoch: 139, training loss: 8.043262720558687, train time: 85.82887697219849
epoch: 140, training loss: 8.377356151990625, train time: 85.85054779052734
epoch: 141, training loss: 9.359887574007985, train time: 85.842702627182
epoch: 142, training loss: 9.582123473550382, train time: 85.6209077835083
epoch: 143, training loss: 8.906513042413735, train time: 86.02352547645569
epoch: 144, training loss: 8.91092480576026, train time: 85.81673097610474
epoch: 145, training loss: 9.143198599140078, train time: 86.18513035774231
epoch: 146, training loss: 9.40498370253772, train time: 85.93910384178162
epoch: 147, training loss: 9.1793477213414, train time: 85.88210821151733
epoch: 148, training loss: 8.78825085090807, train time: 86.00450110435486
epoch: 149, training loss: 8.579253799494836, train time: 85.97243022918701
epo:149 | HR@5:0.8370 | HR@10:0.8541 | HR@20:0.8816 | NDCG@5:0.3288 | NDCG@10:0.3788 | NDCG@20:0.4403 | recall@5:0.6193 | recall@10:0.7012 | recall@20:0.7242 | precision@5:0.7432 | precision@10:0.4207 | precision@20:0.2173 | best_HR@5:0.8989 | best_HR@10:0.9150 | best_HR@20:0.9335 | best_NDCG@5:0.4390 | best_NDCG@10:0.4836 | best_NDCG@20:0.5367 | best_recall@5:0.6424 | best_recall@10:0.7521 | best_recall@20:0.7727 | best_precision@5:0.7709 | best_precision@10:0.4512 | best_precision@20:0.2318 | 
training finish
