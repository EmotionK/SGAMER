nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_Musical_Instruments......
----------------------------------------------------------------------------------------------------
labels.shape: torch.Size([585800])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  8.106231689453125e-06
user  100 time:  387.4644832611084
user  200 time:  775.841682434082
user  300 time:  1169.872797012329
user  400 time:  1568.3413932323456
user  500 time:  1971.7961497306824
user  600 time:  2366.9799966812134
user  700 time:  2763.159928560257
user  800 time:  3164.5025629997253
user  900 time:  3568.7656950950623
user  1000 time:  3969.797353744507
user  1100 time:  4372.399467945099
user  1200 time:  4790.514227628708
user  1300 time:  5208.34582567215
user  1400 time:  5617.347429275513
start training item-item instance self attention module...
user  0 time:  6.4373016357421875e-06
user  100 time:  274.8805351257324
user  200 time:  557.7369589805603
user  300 time:  853.9244141578674
user  400 time:  1144.3446543216705
user  500 time:  1425.4487965106964
user  600 time:  1663.9102647304535
user  700 time:  1927.102243900299
user  800 time:  2172.5143523216248
user  900 time:  2440.388415336609
user  1000 time:  2702.6472911834717
user  1100 time:  2974.4900484085083
user  1200 time:  3258.3263194561005
user  1300 time:  3535.4774239063263
user  1400 time:  3809.2633826732635
start updating user and item embedding...
user_name:1450
user  0 time:  9.059906005859375e-06
user  100 time:  19.423537969589233
user  200 time:  39.08141469955444
user  300 time:  58.25470018386841
user  400 time:  78.17511773109436
user  500 time:  97.61220407485962
user  600 time:  117.70371150970459
user  700 time:  137.87691974639893
user  800 time:  157.77523684501648
user  900 time:  177.9043459892273
user  1000 time:  197.72079491615295
user  1100 time:  217.31828999519348
user  1200 time:  237.64019584655762
user  1300 time:  257.6670892238617
user  1400 time:  277.64740467071533
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:58: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 149.21313342169742, train time: 33.88705658912659
epoch: 1, training loss: 87.49506328557618, train time: 33.79841470718384
epoch: 2, training loss: 74.32274530953146, train time: 33.57731056213379
epoch: 3, training loss: 66.73483286773262, train time: 33.92759728431702
epoch: 4, training loss: 61.833815494392184, train time: 33.73826241493225
epoch: 5, training loss: 57.485604183639225, train time: 33.548511028289795
epoch: 6, training loss: 54.38652534661378, train time: 33.62834620475769
epoch: 7, training loss: 50.9987494762463, train time: 33.60973262786865
epoch: 8, training loss: 48.41262581323099, train time: 33.770835399627686
epoch: 9, training loss: 46.50125594557176, train time: 33.70752310752869
epoch: 10, training loss: 43.42496858933009, train time: 33.77704191207886
epoch: 11, training loss: 42.10313902469352, train time: 33.56757187843323
epoch: 12, training loss: 40.23092731144425, train time: 33.60494112968445
epoch: 13, training loss: 38.36977166896395, train time: 33.8377251625061
epoch: 14, training loss: 36.28672114082292, train time: 33.743401765823364
epoch: 15, training loss: 35.603886754452105, train time: 33.55028939247131
epoch: 16, training loss: 33.69334301796698, train time: 33.612205505371094
epoch: 17, training loss: 32.5248390639681, train time: 33.477729082107544
epoch: 18, training loss: 30.448882612039597, train time: 33.456326961517334
epoch: 19, training loss: 29.529856484150514, train time: 33.72865891456604
epoch: 20, training loss: 28.286800887546633, train time: 33.53241968154907
epoch: 21, training loss: 25.92653433511441, train time: 33.69352436065674
epoch: 22, training loss: 26.128801802591624, train time: 33.62155103683472
epoch: 23, training loss: 24.26434589445853, train time: 33.664435625076294
epoch: 24, training loss: 23.314599087725583, train time: 33.49646496772766
epoch: 25, training loss: 22.69428602424523, train time: 33.43454694747925
epoch: 26, training loss: 21.632311594396015, train time: 33.486286878585815
epoch: 27, training loss: 21.11667636846414, train time: 33.441734313964844
epoch: 28, training loss: 19.68038260576577, train time: 33.367043018341064
epoch: 29, training loss: 20.497837343250467, train time: 33.14488482475281
epo:29 | HR@5:0.7920 | HR@10:0.8355 | HR@20:0.8831 | NDCG@5:0.4507 | NDCG@10:0.4892 | NDCG@20:0.5385 | recall@5:0.5590 | recall@10:0.6655 | recall@20:0.7186 | precision@5:0.6708 | precision@10:0.3993 | precision@20:0.2156 | best_HR@5:0.7920 | best_HR@10:0.8355 | best_HR@20:0.8831 | best_NDCG@5:0.4507 | best_NDCG@10:0.4892 | best_NDCG@20:0.5385 | best_recall@5:0.5590 | best_recall@10:0.6655 | best_recall@20:0.7186 | best_precision@5:0.6708 | best_precision@10:0.3993 | best_precision@20:0.2156 | 
epoch: 30, training loss: 18.910477571091178, train time: 33.84724140167236
epoch: 31, training loss: 18.56832396021855, train time: 33.55360507965088
epoch: 32, training loss: 18.247271051651296, train time: 33.658053159713745
epoch: 33, training loss: 17.371801519878318, train time: 33.719000577926636
epoch: 34, training loss: 17.383337119754287, train time: 33.70702028274536
epoch: 35, training loss: 16.256972701408813, train time: 33.66905331611633
epoch: 36, training loss: 16.923607876034566, train time: 33.74150347709656
epoch: 37, training loss: 15.323310256525929, train time: 33.65960121154785
epoch: 38, training loss: 15.518527209635522, train time: 33.63727617263794
epoch: 39, training loss: 16.094352436603913, train time: 33.48169231414795
epoch: 40, training loss: 14.724091348384945, train time: 33.73701620101929
epoch: 41, training loss: 13.96463681532532, train time: 33.61695170402527
epoch: 42, training loss: 14.882033273772777, train time: 33.85428500175476
epoch: 43, training loss: 14.692014955370723, train time: 33.53252983093262
epoch: 44, training loss: 16.382511250867992, train time: 33.80890917778015
epoch: 45, training loss: 14.000983571217148, train time: 33.78008484840393
epoch: 46, training loss: 14.029447548302869, train time: 33.63899874687195
epoch: 47, training loss: 13.69977257600317, train time: 33.9007625579834
epoch: 48, training loss: 13.452083860055609, train time: 33.78024363517761
epoch: 49, training loss: 13.25418578191966, train time: 33.84872627258301
epoch: 50, training loss: 12.207792201168786, train time: 33.76088333129883
epoch: 51, training loss: 14.02923671175131, train time: 33.93484354019165
epoch: 52, training loss: 13.043009131741883, train time: 33.724087715148926
epoch: 53, training loss: 12.365462754468581, train time: 33.88912391662598
epoch: 54, training loss: 13.607674393922707, train time: 33.600892543792725
epoch: 55, training loss: 13.089759456871093, train time: 33.14832663536072
epoch: 56, training loss: 12.593989842046994, train time: 33.69456148147583
epoch: 57, training loss: 12.637786968102546, train time: 33.42330741882324
epoch: 58, training loss: 12.431044151883043, train time: 33.362024784088135
epoch: 59, training loss: 12.42216989145004, train time: 33.198814392089844
epo:59 | HR@5:0.7734 | HR@10:0.8175 | HR@20:0.8623 | NDCG@5:0.4634 | NDCG@10:0.5018 | NDCG@20:0.5501 | recall@5:0.5513 | recall@10:0.6524 | recall@20:0.7036 | precision@5:0.6615 | precision@10:0.3914 | precision@20:0.2111 | best_HR@5:0.7920 | best_HR@10:0.8355 | best_HR@20:0.8831 | best_NDCG@5:0.4634 | best_NDCG@10:0.5018 | best_NDCG@20:0.5501 | best_recall@5:0.5590 | best_recall@10:0.6655 | best_recall@20:0.7186 | best_precision@5:0.6708 | best_precision@10:0.3993 | best_precision@20:0.2156 | 
epoch: 60, training loss: 12.186874320620973, train time: 30.900546312332153
epoch: 61, training loss: 11.26967739453903, train time: 31.108546018600464
epoch: 62, training loss: 12.630540325664242, train time: 30.888664722442627
epoch: 63, training loss: 12.168641167101669, train time: 30.7258563041687
epoch: 64, training loss: 11.889373844286638, train time: 31.30704617500305
epoch: 65, training loss: 12.558118894932136, train time: 31.232088565826416
epoch: 66, training loss: 11.760424725905523, train time: 31.039040565490723
epoch: 67, training loss: 11.023706593747647, train time: 31.043914079666138
epoch: 68, training loss: 10.819933441398689, train time: 31.224592447280884
epoch: 69, training loss: 11.372784133223377, train time: 30.717868089675903
epoch: 70, training loss: 11.0141430058776, train time: 30.77077841758728
epoch: 71, training loss: 11.12311366595884, train time: 31.113247871398926
epoch: 72, training loss: 11.61860107653365, train time: 30.77115488052368
epoch: 73, training loss: 11.202659058860945, train time: 30.927056550979614
epoch: 74, training loss: 11.994317535770506, train time: 31.37568712234497
epoch: 75, training loss: 9.794884118156347, train time: 30.93850564956665
epoch: 76, training loss: 10.665447795736554, train time: 31.058537483215332
epoch: 77, training loss: 11.12144099474699, train time: 30.947447299957275
epoch: 78, training loss: 11.019518065719922, train time: 30.97442865371704
epoch: 79, training loss: 10.348971712854677, train time: 30.99278688430786
epoch: 80, training loss: 10.2559448938473, train time: 30.930914878845215
epoch: 81, training loss: 10.212584240065098, train time: 30.939786672592163
epoch: 82, training loss: 10.274436071669015, train time: 31.20939064025879
epoch: 83, training loss: 10.43739926476269, train time: 31.157559871673584
epoch: 84, training loss: 10.675342910463144, train time: 31.04911184310913
epoch: 85, training loss: 10.528278554406825, train time: 31.078110218048096
epoch: 86, training loss: 9.768364403182545, train time: 30.881107568740845
epoch: 87, training loss: 9.634543603956615, train time: 30.752094984054565
epoch: 88, training loss: 9.89486011652258, train time: 31.112885236740112
epoch: 89, training loss: 9.451621051309417, train time: 31.03368854522705
epo:89 | HR@5:0.7792 | HR@10:0.8152 | HR@20:0.8586 | NDCG@5:0.4615 | NDCG@10:0.5008 | NDCG@20:0.5498 | recall@5:0.5528 | recall@10:0.6533 | recall@20:0.6994 | precision@5:0.6633 | precision@10:0.3920 | precision@20:0.2098 | best_HR@5:0.7920 | best_HR@10:0.8355 | best_HR@20:0.8831 | best_NDCG@5:0.4634 | best_NDCG@10:0.5018 | best_NDCG@20:0.5501 | best_recall@5:0.5590 | best_recall@10:0.6655 | best_recall@20:0.7186 | best_precision@5:0.6708 | best_precision@10:0.3993 | best_precision@20:0.2156 | 
epoch: 90, training loss: 9.464491697817834, train time: 30.727010488510132
epoch: 91, training loss: 10.072578562361514, train time: 30.856361627578735
epoch: 92, training loss: 9.6670257036252, train time: 31.126996994018555
epoch: 93, training loss: 11.464384455056233, train time: 30.805891275405884
epoch: 94, training loss: 9.197409768313662, train time: 31.09278964996338
epoch: 95, training loss: 10.224218169909136, train time: 31.09323811531067
epoch: 96, training loss: 8.949976080603506, train time: 30.864564180374146
epoch: 97, training loss: 10.64213058079281, train time: 30.982309818267822
epoch: 98, training loss: 9.365959283652273, train time: 30.958486557006836
epoch: 99, training loss: 9.69100004304039, train time: 31.086052656173706
epoch: 100, training loss: 9.559259512060976, train time: 31.145103216171265
epoch: 101, training loss: 8.975364378881864, train time: 30.923391342163086
epoch: 102, training loss: 8.912307970246388, train time: 31.076332807540894
epoch: 103, training loss: 10.731971139497091, train time: 30.997368335723877
epoch: 104, training loss: 8.871664039484244, train time: 30.68134307861328
epoch: 105, training loss: 9.271310538065109, train time: 31.0571768283844
epoch: 106, training loss: 8.611604289424633, train time: 31.328571319580078
epoch: 107, training loss: 9.442243713352354, train time: 31.088492393493652
epoch: 108, training loss: 9.782842025707225, train time: 31.08860492706299
epoch: 109, training loss: 8.626624172509082, train time: 30.91551637649536
epoch: 110, training loss: 8.931936902038728, train time: 30.97058868408203
epoch: 111, training loss: 7.752721024040966, train time: 30.622587203979492
epoch: 112, training loss: 8.981364785457004, train time: 30.810351133346558
epoch: 113, training loss: 9.342164547249808, train time: 31.082472562789917
epoch: 114, training loss: 9.203992323184366, train time: 30.76312828063965
epoch: 115, training loss: 9.116524363311328, train time: 31.04698085784912
epoch: 116, training loss: 9.832720290736233, train time: 30.879537105560303
epoch: 117, training loss: 8.788882982965447, train time: 31.101600170135498
epoch: 118, training loss: 9.492286602803688, train time: 30.741759061813354
epoch: 119, training loss: 8.422198084279444, train time: 30.928939819335938
epo:119 | HR@5:0.7516 | HR@10:0.7945 | HR@20:0.8433 | NDCG@5:0.4789 | NDCG@10:0.5157 | NDCG@20:0.5623 | recall@5:0.5407 | recall@10:0.6329 | recall@20:0.6831 | precision@5:0.6488 | precision@10:0.3797 | precision@20:0.2049 | best_HR@5:0.7920 | best_HR@10:0.8355 | best_HR@20:0.8831 | best_NDCG@5:0.4789 | best_NDCG@10:0.5157 | best_NDCG@20:0.5623 | best_recall@5:0.5590 | best_recall@10:0.6655 | best_recall@20:0.7186 | best_precision@5:0.6708 | best_precision@10:0.3993 | best_precision@20:0.2156 | 
epoch: 120, training loss: 8.689677110049047, train time: 30.743733406066895
epoch: 121, training loss: 9.561924883180154, train time: 31.102901458740234
epoch: 122, training loss: 8.786745057248254, train time: 31.150823831558228
epoch: 123, training loss: 7.673072233383834, train time: 31.121835947036743
epoch: 124, training loss: 8.668735842571834, train time: 31.140494346618652
epoch: 125, training loss: 7.919669018675336, train time: 31.056257247924805
epoch: 126, training loss: 8.499385130160817, train time: 30.939566612243652
epoch: 127, training loss: 8.905126535516843, train time: 30.865684270858765
epoch: 128, training loss: 9.476085350455492, train time: 30.873140811920166
epoch: 129, training loss: 7.662961440104425, train time: 32.47204780578613
epoch: 130, training loss: 8.655055031269285, train time: 34.254228353500366
epoch: 131, training loss: 8.87544562140829, train time: 34.40183401107788
epoch: 132, training loss: 7.605276842335996, train time: 34.4198157787323
epoch: 133, training loss: 8.113117906980278, train time: 34.37959361076355
epoch: 134, training loss: 9.18554159111875, train time: 34.07386636734009
epoch: 135, training loss: 8.191163481027559, train time: 34.1042537689209
epoch: 136, training loss: 8.38377901315954, train time: 34.105722427368164
epoch: 137, training loss: 8.454600569922235, train time: 34.52861738204956
epoch: 138, training loss: 8.746130256595336, train time: 34.28334164619446
epoch: 139, training loss: 7.896397111861461, train time: 34.37009024620056
epoch: 140, training loss: 8.713557164040424, train time: 34.585636138916016
epoch: 141, training loss: 8.656709893559537, train time: 34.590702533721924
epoch: 142, training loss: 7.610466085519505, train time: 34.44754123687744
epoch: 143, training loss: 7.620862896855954, train time: 34.453837633132935
epoch: 144, training loss: 8.503748751382204, train time: 34.37982964515686
epoch: 145, training loss: 9.193541198514964, train time: 34.42344522476196
epoch: 146, training loss: 8.191293083589585, train time: 34.37004613876343
epoch: 147, training loss: 8.377306630647155, train time: 34.136107206344604
epoch: 148, training loss: 8.487353626963255, train time: 34.358206272125244
epoch: 149, training loss: 7.560232668494848, train time: 34.030195236206055
epo:149 | HR@5:0.7492 | HR@10:0.7903 | HR@20:0.8357 | NDCG@5:0.4745 | NDCG@10:0.5116 | NDCG@20:0.5588 | recall@5:0.5400 | recall@10:0.6287 | recall@20:0.6798 | precision@5:0.6480 | precision@10:0.3772 | precision@20:0.2039 | best_HR@5:0.7920 | best_HR@10:0.8355 | best_HR@20:0.8831 | best_NDCG@5:0.4789 | best_NDCG@10:0.5157 | best_NDCG@20:0.5623 | best_recall@5:0.5590 | best_recall@10:0.6655 | best_recall@20:0.7186 | best_precision@5:0.6708 | best_precision@10:0.3993 | best_precision@20:0.2156 | 
training finish
