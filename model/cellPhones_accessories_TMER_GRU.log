nohup: ignoring input
run.py device: cuda
----------------------------------------------------------------------------------------------------
Amazon_CellPhones_Accessories......
----------------------------------------------------------------------------------------------------
/home/ubuntu/model/PaperModel/model/../model/util/data_utils.py:13: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)
  nodewv_tensor = torch.Tensor(nodewv_tensor)
labels.shape: torch.Size([808000])
loading node embedding, all user-item and item-item paths embedding...finished
start training user-item instance self attention module...
user  0 time:  3.814697265625e-06
user  100 time:  233.60641980171204
user  200 time:  468.7756209373474
user  300 time:  708.7782881259918
user  400 time:  947.8862707614899
user  500 time:  1184.3472301959991
user  600 time:  1422.9502964019775
user  700 time:  1665.0233066082
user  800 time:  1905.6405222415924
user  900 time:  2144.8067145347595
user  1000 time:  2387.0319373607635
user  1100 time:  2630.881061077118
user  1200 time:  2871.3007185459137
user  1300 time:  3112.7477588653564
user  1400 time:  3351.0000207424164
user  1500 time:  3590.1758184432983
user  1600 time:  3833.8760993480682
user  1700 time:  4078.7645630836487
user  1800 time:  4322.149037361145
user  1900 time:  4570.258438825607
start training item-item instance self attention module...
user  0 time:  6.198883056640625e-06
user  100 time:  41.318254709243774
user  200 time:  76.13902997970581
user  300 time:  117.64079070091248
user  400 time:  156.71527433395386
user  500 time:  199.27133655548096
user  600 time:  244.45704579353333
user  700 time:  283.2428915500641
user  800 time:  328.56378865242004
user  900 time:  369.69405341148376
user  1000 time:  407.7814738750458
user  1100 time:  454.1211392879486
user  1200 time:  495.4972722530365
user  1300 time:  531.4727132320404
user  1400 time:  564.8863265514374
user  1500 time:  603.2738289833069
user  1600 time:  648.5388062000275
user  1700 time:  688.6879334449768
user  1800 time:  725.2937972545624
user  1900 time:  764.7422678470612
start updating user and item embedding...
user_name:2000
user  0 time:  1.0013580322265625e-05
user  100 time:  15.266724824905396
user  200 time:  30.395052909851074
user  300 time:  45.82031059265137
user  400 time:  60.97957229614258
user  500 time:  76.50750303268433
user  600 time:  92.0058662891388
user  700 time:  107.55601906776428
user  800 time:  123.20537376403809
user  900 time:  138.33229327201843
user  1000 time:  153.51730108261108
user  1100 time:  168.94296383857727
user  1200 time:  184.34701108932495
user  1300 time:  199.83179903030396
user  1400 time:  215.17817902565002
user  1500 time:  230.6882381439209
user  1600 time:  246.0922133922577
user  1700 time:  261.4210259914398
user  1800 time:  276.77256536483765
user  1900 time:  292.08647418022156
start training recommendation module...
/home/ubuntu/model/PaperModel/model/recommendation_model.py:54: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.
  fe = F.log_softmax(output)
epoch: 0, training loss: 170.08741865711636, train time: 97.98697280883789
epoch: 1, training loss: 94.99478462849947, train time: 98.23886060714722
epoch: 2, training loss: 84.64945212585008, train time: 98.20194387435913
epoch: 3, training loss: 76.8166034056776, train time: 97.99509072303772
epoch: 4, training loss: 71.73134654515525, train time: 97.80854153633118
epoch: 5, training loss: 68.50005417186912, train time: 97.92548847198486
epoch: 6, training loss: 65.73509464007657, train time: 98.25669503211975
epoch: 7, training loss: 63.503554450573574, train time: 98.16084742546082
epoch: 8, training loss: 62.426642603750224, train time: 98.28460597991943
epoch: 9, training loss: 62.33437112941101, train time: 98.37533736228943
epoch: 10, training loss: 60.480422481603455, train time: 98.12295055389404
epoch: 11, training loss: 61.20915551298458, train time: 98.2261414527893
epoch: 12, training loss: 59.68376906602498, train time: 98.35577487945557
epoch: 13, training loss: 58.2354406256527, train time: 98.21055459976196
epoch: 14, training loss: 57.39641313013999, train time: 98.34389424324036
epoch: 15, training loss: 56.97856895714358, train time: 98.31939458847046
epoch: 16, training loss: 58.056197341786174, train time: 97.88817858695984
epoch: 17, training loss: 59.002908374877734, train time: 98.52688980102539
epoch: 18, training loss: 54.28579258508398, train time: 98.54857802391052
epoch: 19, training loss: 53.41774304585124, train time: 98.23235249519348
epoch: 20, training loss: 56.89140702274017, train time: 98.5649139881134
epoch: 21, training loss: 55.359497872941574, train time: 98.16754269599915
epoch: 22, training loss: 54.714675877774425, train time: 98.52415680885315
epoch: 23, training loss: 52.635232018135866, train time: 98.31080055236816
epoch: 24, training loss: 54.263090610606014, train time: 98.52174162864685
epoch: 25, training loss: 53.61892355431701, train time: 98.43423056602478
epoch: 26, training loss: 54.541414713727136, train time: 98.82976293563843
epoch: 27, training loss: 51.91608631923373, train time: 98.21206641197205
epoch: 28, training loss: 51.63798548526938, train time: 98.40099549293518
epoch: 29, training loss: 53.75445488668993, train time: 98.62582755088806
epo:29 | HR@5:0.8625 | HR@10:0.8812 | HR@20:0.9088 | NDCG@5:0.2743 | NDCG@10:0.3179 | NDCG@20:0.3741 | recall@5:0.6296 | recall@10:0.7194 | recall@20:0.7470 | precision@5:0.7555 | precision@10:0.4316 | precision@20:0.2241 | best_HR@5:0.8625 | best_HR@10:0.8812 | best_HR@20:0.9088 | best_NDCG@5:0.2743 | best_NDCG@10:0.3179 | best_NDCG@20:0.3741 | best_recall@5:0.6296 | best_recall@10:0.7194 | best_recall@20:0.7470 | best_precision@5:0.7555 | best_precision@10:0.4316 | best_precision@20:0.2241 | 
epoch: 30, training loss: 52.523440255878086, train time: 98.19162321090698
epoch: 31, training loss: 54.559697363245505, train time: 98.31788849830627
epoch: 32, training loss: 52.81551598934493, train time: 92.66360020637512
epoch: 33, training loss: 54.473709399971995, train time: 95.40558433532715
epoch: 34, training loss: 52.65862157880474, train time: 98.10144114494324
epoch: 35, training loss: 53.62794590438716, train time: 97.66162538528442
epoch: 36, training loss: 53.31810907070394, train time: 98.25556373596191
epoch: 37, training loss: 51.92939509147618, train time: 97.75935125350952
epoch: 38, training loss: 51.46439194440245, train time: 98.39908170700073
epoch: 39, training loss: 53.39044573990759, train time: 98.42257928848267
epoch: 40, training loss: 52.841345116245066, train time: 98.41211032867432
epoch: 41, training loss: 53.61491100048261, train time: 98.44426965713501
epoch: 42, training loss: 56.60660092222679, train time: 98.38817811012268
epoch: 43, training loss: 56.12913891563221, train time: 98.36343359947205
epoch: 44, training loss: 54.16528375045527, train time: 98.06148219108582
epoch: 45, training loss: 55.082634287520705, train time: 98.29018235206604
epoch: 46, training loss: 53.75880419310852, train time: 98.68488359451294
epoch: 47, training loss: 51.98666833761308, train time: 98.10288691520691
epoch: 48, training loss: 52.00762365374976, train time: 98.38115072250366
epoch: 49, training loss: 53.71747316305118, train time: 98.24539470672607
epoch: 50, training loss: 53.476395832385606, train time: 98.70647144317627
epoch: 51, training loss: 54.22192112207267, train time: 98.81698751449585
epoch: 52, training loss: 52.8379282805945, train time: 98.38638305664062
epoch: 53, training loss: 51.66337217352702, train time: 98.305490732193
epoch: 54, training loss: 52.99851521184246, train time: 98.12918066978455
epoch: 55, training loss: 53.09157754286389, train time: 97.97905898094177
epoch: 56, training loss: 51.13813804736128, train time: 99.77899360656738
epoch: 57, training loss: 52.46538911841981, train time: 98.81593012809753
epoch: 58, training loss: 50.24831258631093, train time: 98.58285427093506
epoch: 59, training loss: 50.68382193831894, train time: 98.32438588142395
epo:59 | HR@5:0.8456 | HR@10:0.8689 | HR@20:0.9040 | NDCG@5:0.2298 | NDCG@10:0.2690 | NDCG@20:0.3220 | recall@5:0.6220 | recall@10:0.7052 | recall@20:0.7412 | precision@5:0.7464 | precision@10:0.4231 | precision@20:0.2223 | best_HR@5:0.8625 | best_HR@10:0.8812 | best_HR@20:0.9088 | best_NDCG@5:0.2743 | best_NDCG@10:0.3179 | best_NDCG@20:0.3741 | best_recall@5:0.6296 | best_recall@10:0.7194 | best_recall@20:0.7470 | best_precision@5:0.7555 | best_precision@10:0.4316 | best_precision@20:0.2241 | 
epoch: 60, training loss: 51.06544684718574, train time: 98.21941423416138
epoch: 61, training loss: 52.60493161787963, train time: 98.48960089683533
epoch: 62, training loss: 52.36215489958158, train time: 98.26927018165588
epoch: 63, training loss: 54.072044203814585, train time: 98.35057377815247
epoch: 64, training loss: 51.99316375646595, train time: 98.11948800086975
epoch: 65, training loss: 50.323740199828535, train time: 98.3002119064331
epoch: 66, training loss: 53.2543276443248, train time: 97.9307222366333
epoch: 67, training loss: 54.2619005652341, train time: 98.24622559547424
epoch: 68, training loss: 52.541459471951384, train time: 98.15632152557373
epoch: 69, training loss: 53.14908100467801, train time: 98.20684242248535
epoch: 70, training loss: 52.701384343672544, train time: 98.05517673492432
epoch: 71, training loss: 53.19645596292321, train time: 98.30059790611267
epoch: 72, training loss: 52.761167981378094, train time: 98.49570679664612
epoch: 73, training loss: 54.36244231300225, train time: 98.34216594696045
epoch: 74, training loss: 51.726316139688606, train time: 98.19226670265198
epoch: 75, training loss: 53.13607065725955, train time: 98.15232300758362
epoch: 76, training loss: 55.77232160212225, train time: 98.10548615455627
epoch: 77, training loss: 53.08709379310312, train time: 97.7567811012268
epoch: 78, training loss: 52.019729393026864, train time: 98.1804690361023
epoch: 79, training loss: 56.72893185973953, train time: 98.1142361164093
epoch: 80, training loss: 55.22801148061262, train time: 98.2557008266449
epoch: 81, training loss: 53.863221855277516, train time: 98.19681167602539
epoch: 82, training loss: 54.62476452820556, train time: 98.02308106422424
epoch: 83, training loss: 53.24319697013743, train time: 98.13839340209961
epoch: 84, training loss: 55.95526726432581, train time: 98.16957998275757
epoch: 85, training loss: 55.75206608087319, train time: 98.20757865905762
epoch: 86, training loss: 53.49880257198129, train time: 98.31915283203125
epoch: 87, training loss: 56.4853134001205, train time: 98.00277733802795
epoch: 88, training loss: 55.61621912827468, train time: 98.77244853973389
epoch: 89, training loss: 55.305436895170715, train time: 98.44533133506775
epo:89 | HR@5:0.8602 | HR@10:0.8825 | HR@20:0.9054 | NDCG@5:0.2040 | NDCG@10:0.2419 | NDCG@20:0.2964 | recall@5:0.6272 | recall@10:0.7204 | recall@20:0.7472 | precision@5:0.7526 | precision@10:0.4322 | precision@20:0.2242 | best_HR@5:0.8625 | best_HR@10:0.8825 | best_HR@20:0.9088 | best_NDCG@5:0.2743 | best_NDCG@10:0.3179 | best_NDCG@20:0.3741 | best_recall@5:0.6296 | best_recall@10:0.7204 | best_recall@20:0.7472 | best_precision@5:0.7555 | best_precision@10:0.4322 | best_precision@20:0.2242 | 
epoch: 90, training loss: 54.46227712193877, train time: 98.89137840270996
epoch: 91, training loss: 56.271074567859614, train time: 98.84315466880798
epoch: 92, training loss: 55.79485770794781, train time: 98.201003074646
epoch: 93, training loss: 56.116764189560854, train time: 98.33705806732178
epoch: 94, training loss: 54.465627234365456, train time: 98.3316707611084
epoch: 95, training loss: 53.87420868575646, train time: 98.59748554229736
epoch: 96, training loss: 56.89671847004138, train time: 98.0910382270813
epoch: 97, training loss: 57.51810241417479, train time: 98.22758150100708
epoch: 98, training loss: 56.92710214341605, train time: 98.186763048172
epoch: 99, training loss: 54.052996706115664, train time: 98.128653049469
epoch: 100, training loss: 54.363397275928946, train time: 98.27958798408508
epoch: 101, training loss: 54.583225448894154, train time: 98.29852151870728
epoch: 102, training loss: 57.44070293307959, train time: 98.37937259674072
epoch: 103, training loss: 56.45117916892195, train time: 98.0841293334961
epoch: 104, training loss: 56.22012781322337, train time: 98.00675630569458
epoch: 105, training loss: 59.044525844907184, train time: 98.84192371368408
epoch: 106, training loss: 61.45005568877605, train time: 98.28950071334839
epoch: 107, training loss: 56.468199938002726, train time: 98.24559187889099
epoch: 108, training loss: 55.213309958180616, train time: 98.00133466720581
epoch: 109, training loss: 53.63367764094801, train time: 97.98298382759094
epoch: 110, training loss: 58.090708653762704, train time: 98.11372089385986
epoch: 111, training loss: 55.270784729615116, train time: 98.26466131210327
epoch: 112, training loss: 55.68612581596972, train time: 97.99867463111877
epoch: 113, training loss: 56.107294819521485, train time: 98.05400443077087
epoch: 114, training loss: 57.354771922025975, train time: 98.1424171924591
epoch: 115, training loss: 60.91670512416749, train time: 97.92413783073425
epoch: 116, training loss: 58.637259197630556, train time: 97.98962664604187
epoch: 117, training loss: 60.34775146457105, train time: 97.92036581039429
epoch: 118, training loss: 58.14851501172234, train time: 97.99977016448975
epoch: 119, training loss: 57.19334099511798, train time: 98.26619029045105
epo:119 | HR@5:0.8683 | HR@10:0.8858 | HR@20:0.9019 | NDCG@5:0.1312 | NDCG@10:0.1635 | NDCG@20:0.2110 | recall@5:0.6294 | recall@10:0.7249 | recall@20:0.7445 | precision@5:0.7553 | precision@10:0.4349 | precision@20:0.2233 | best_HR@5:0.8683 | best_HR@10:0.8858 | best_HR@20:0.9088 | best_NDCG@5:0.2743 | best_NDCG@10:0.3179 | best_NDCG@20:0.3741 | best_recall@5:0.6296 | best_recall@10:0.7249 | best_recall@20:0.7472 | best_precision@5:0.7555 | best_precision@10:0.4349 | best_precision@20:0.2242 | 
epoch: 120, training loss: 56.979266481992454, train time: 97.74109888076782
epoch: 121, training loss: 55.56238915760332, train time: 98.10443377494812
epoch: 122, training loss: 60.03265969129825, train time: 97.71164035797119
epoch: 123, training loss: 59.481355119798536, train time: 97.99638438224792
epoch: 124, training loss: 60.313041190132935, train time: 98.18868684768677
epoch: 125, training loss: 60.376857287152234, train time: 97.97477269172668
epoch: 126, training loss: 59.199538325337926, train time: 97.88455104827881
epoch: 127, training loss: 59.263329207034985, train time: 98.02632403373718
epoch: 128, training loss: 60.319947204003256, train time: 98.13210105895996
epoch: 129, training loss: 59.79274197018822, train time: 98.07825827598572
epoch: 130, training loss: 59.438407778241526, train time: 98.59474992752075
epoch: 131, training loss: 60.23205874921405, train time: 98.51744437217712
epoch: 132, training loss: 58.33573949414131, train time: 98.39633393287659
epoch: 133, training loss: 59.23088978044689, train time: 97.887371301651
epoch: 134, training loss: 59.34396184268553, train time: 98.53650116920471
epoch: 135, training loss: 60.551292861327966, train time: 98.10335040092468
epoch: 136, training loss: 60.44146251514212, train time: 98.05717968940735
epoch: 137, training loss: 57.781873640462436, train time: 97.88763880729675
epoch: 138, training loss: 57.452954552056326, train time: 97.7800886631012
epoch: 139, training loss: 58.33065282904499, train time: 98.13070774078369
epoch: 140, training loss: 60.28596162307986, train time: 97.73631310462952
epoch: 141, training loss: 56.86808847732027, train time: 97.76702928543091
epoch: 142, training loss: 55.11698111656733, train time: 98.08526992797852
epoch: 143, training loss: 57.11677469031929, train time: 97.9026050567627
epoch: 144, training loss: 58.126462596221245, train time: 97.36607193946838
epoch: 145, training loss: 59.80506681665429, train time: 97.3076844215393
